---
title: "How to handle class-unbalanced data?"
description: |
  In class-imbalanced datasets, the majority class dominates while the minority class is underrepresented, leading models to bias their predictions toward the majority class.
author: "Kundan Kumar"
date: "2025-05-06"
categories: [Data science, Machine Learning]
sidebar: false
code-block-bg: true
code-block-border-left: true
format: 
  html:
    toc: true
    toc-depth: 2
    code-fold: false
    code-tools: false
---

There are three systematic different ways to handle class imbalanced data.

## Data-Level Approaches
Oversampling the minority class

Random Oversampling: duplicate minority class examples until balance is reached.

SMOTE (Synthetic Minority Over-sampling Technique): generates synthetic data points for the minority class by interpolating between nearest neighbors.

ADASYN: similar to SMOTE but focuses on generating harder-to-learn examples.

Undersampling the majority class

Random Undersampling: randomly remove majority class examples.

Cluster Centroids / Tomek Links / NearMiss: more informed undersampling to preserve useful structure.

Hybrid methods

Combine oversampling and undersampling to avoid overfitting or losing too much information.

## Algorithm-Level Approaches

Class Weights / Cost-Sensitive Learning

Assign higher misclassification cost to minority class (many libraries like scikit-learn allow class_weight='balanced').

Anomaly Detection / One-Class Models

Treat minority class as "rare events" and use anomaly detection approaches.

Ensemble Techniques

Use Bagging/Boosting with imbalance-aware modifications (e.g., Balanced Random Forest, EasyEnsemble, RUSBoost).

## Evaluation-Level Approaches

- Avoid accuracy as the metric (it will be misleading).
- Prefer metrics that account for imbalance:

Precision, Recall, F1-score

ROC-AUC, PR-AUC (especially useful with rare positives)

Matthews Correlation Coefficient (MCC)

Balanced Accuracy

> If I face class imbalance, I’d start by understanding how severe it is. At the data level, I could resample — oversample the minority with SMOTE or undersample the majority. At the algorithm level, I’d use class weights or imbalance-aware ensembles like Balanced Random Forest. I’d also focus on the right metrics — like precision, recall, F1, or PR-AUC — since accuracy can be misleading. In deep learning, I might use focal loss. For example, in a fraud detection project, I combined SMOTE with class-weighted XGBoost, which improved recall significantly while controlling false positives.

[The Book of OHDSI](https://ohdsi.github.io/TheBookOfOhdsi/) written by the OHDSI community.

What is required to go from origin (source data) to destination (evidence):

* understanding of health informatics, patient and provider interaction through administrative and clinical systems into final depository
* appreciation of the biases that can arise in the data curation processes
* mastery of epidemiological principles and statistical methods to translate clinical questions into an observational study design properly
* technical ability to implement and execute computationally-efficient data science algorithms
* clinical knowledge to synthesize knowledge learned, and determine how the new knowledge should impact health policy and clinical practice.



OMOP: Observational Medical Outcomes Partnership, aims to identify true drug safety association.

OMOP CDM: common data model, a mechanism to standardize the structure, content and semantics to make it possible to write statistical code that can be reused at every data site.

OHDSI community (2014) has created libraries of open-source analytics tools atop OMOP CDM to support: 

* clinical characterisation for disease natural history, treatment utilisation, quality improvement
* population level effect estimation to apply causal inference for medical product safety surveillance and comparative effectiveness
* patient level prediction to apply machine learning for precision medicine and disease interception


# Chapter 7 Data analytics use cases

Three major categories: characterization, population-level estimation, patient-level prediction


## Characterization

> What happened to the patients.

[Chapter 11 Characterization](https://ohdsi.github.io/TheBookOfOhdsi/Characterization.html#cohort-characterization)

Typical characterization questions: 

* How many patients...? 
* How often does...? What proportion of patients ...?
* What is the distribution of values for ...?
* What is the median length of exposure for patients on ...?
* Other drugs the patient is using?

Desired output:

* count, percentage
* averages and other descriptive statistics
* prevalence, incidence rate
* rule-based phenotype
* drug utilization, adherence, treatment pathways, line of therapy
* disease natural history, co-morbidity profile


## Population-level estimation

> What are the causal effects

[Chapter 12 Population-level Estimation](https://ohdsi.github.io/TheBookOfOhdsi/PopulationLevelEstimation.html)

Typical questions:

* What is the effect of ...?
* Which treatment works better?
* What is the risk of X on Y?
* What is the time-to-event of ...?

Desired output:

* RR, HR, OR
* Association, correlation
* ATE, causal effect

## Patient-level prediction

> What will happen to A?

[Chapter 13 Patient-level Prediction](https://ohdsi.github.io/TheBookOfOhdsi/PatientLevelPrediction.html)

Typical questions:

* What is the chance that this patient will...?
* Who are the candidate for...?

Desired output:

* probability for an individual
* prediction model
* high/low risk groups
* probabilistic phenotype


https://imbalanced-learn.org/stable/user_guide.html#user-guide


https://imbalanced-learn.org/stable/over_sampling.html




