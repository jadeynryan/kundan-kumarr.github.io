[
  {
    "objectID": "dev/short_bio.html",
    "href": "dev/short_bio.html",
    "title": "Hello, I'm Kundan",
    "section": "",
    "text": "Chi Zhang is a statistician and R developer at the Faculty of Medicine, University of Oslo in Norway. Since 2016, She has been working with Real World Data such as Electronic Health Records (EHR) and large public health registries. In 2020-2022 she worked as part of the COVID emergency response team at Norwegian Institute of Public Health to develop an open source real-time analysis and public health surveillance system. At the moment Chi is collaborating with Oslo University Hospital and Akershus University Hospital on understanding antibiotics prescription and usage recorded in the EHR system. She develops R packages to facilitate exploration of messy EHR data. Outside research and R package development, She teaches statistics courses using R and Quarto. Chi is a Co-lead of the CAMIS project (Comparing Analysis Method Implementations in Software) and has contributed to various chapters to RWD Guideline for Programming and Analysis Processes."
  },
  {
    "objectID": "teaching/coms3620/index.html",
    "href": "teaching/coms3620/index.html",
    "title": "COMS 3620",
    "section": "",
    "text": "COMS 3620 is a project-based course focused on object-oriented requirements analysis and system design. Students learn to apply Unified Modeling Language (UML) and design patterns to build robust and evolvable software systems. The course emphasizes teamwork, iterative development, and the communication of design decisions through technical documentation and presentations.\n\n\n\n\nAs a Teaching Assistant for COMS 3620, I provided critical support to students in both design and implementation phases of large-scale object-oriented projects. My contributions included:\n\nUML & Design Mentorship: Assisted students in modeling system requirements using UML (use case diagrams, class diagrams, sequence diagrams).\nProject Guidance: Advised student teams during the full development cycle—from requirement analysis and domain modeling to implementation and testing.\nDesign Critique & Feedback: Evaluated and gave feedback on design homeworks, helping students strengthen their reasoning with design principles and patterns.\nTechnical Assistance: Supported Java implementation of OO designs, clarifying concepts like inheritance, polymorphism, abstraction, and class hierarchies.\nPresentation Coaching: Helped teams prepare and deliver clear, well-structured final project presentations and reports.\n\nThis role enhanced my skills in software architecture, peer instruction, and coaching students on both technical correctness and design clarity.\n\n\n\n\nUpon completing the course, students are expected to:\n\nAnalyze software system requirements and model problem domains\n\nDesign robust object-oriented systems using design principles and heuristics\n\nApply and justify the use of design patterns\n\nProduce UML-based design documentation\n\nImplement object-oriented solutions in Java using abstraction, inheritance, and polymorphism\n\nPresent and explain team design decisions in written and oral formats\n\nCollaborate effectively in team-based development projects\n\n\n\n\n\n\nProcedural & Data Abstraction\n\nModularity, Objects, State\n\nUnified Modeling Language (UML)\n\nDesign Principles & Patterns\n\nMetalinguistic Abstraction\n\nProject Presentations & Evaluation\n\n\n💡 Course Catalog | Email"
  },
  {
    "objectID": "teaching/coms3620/index.html#coms-3620-object-oriented-analysis-and-design",
    "href": "teaching/coms3620/index.html#coms-3620-object-oriented-analysis-and-design",
    "title": "COMS 3620",
    "section": "",
    "text": "COMS 3620 is a project-based course focused on object-oriented requirements analysis and system design. Students learn to apply Unified Modeling Language (UML) and design patterns to build robust and evolvable software systems. The course emphasizes teamwork, iterative development, and the communication of design decisions through technical documentation and presentations.\n\n\n\n\nAs a Teaching Assistant for COMS 3620, I provided critical support to students in both design and implementation phases of large-scale object-oriented projects. My contributions included:\n\nUML & Design Mentorship: Assisted students in modeling system requirements using UML (use case diagrams, class diagrams, sequence diagrams).\nProject Guidance: Advised student teams during the full development cycle—from requirement analysis and domain modeling to implementation and testing.\nDesign Critique & Feedback: Evaluated and gave feedback on design homeworks, helping students strengthen their reasoning with design principles and patterns.\nTechnical Assistance: Supported Java implementation of OO designs, clarifying concepts like inheritance, polymorphism, abstraction, and class hierarchies.\nPresentation Coaching: Helped teams prepare and deliver clear, well-structured final project presentations and reports.\n\nThis role enhanced my skills in software architecture, peer instruction, and coaching students on both technical correctness and design clarity.\n\n\n\n\nUpon completing the course, students are expected to:\n\nAnalyze software system requirements and model problem domains\n\nDesign robust object-oriented systems using design principles and heuristics\n\nApply and justify the use of design patterns\n\nProduce UML-based design documentation\n\nImplement object-oriented solutions in Java using abstraction, inheritance, and polymorphism\n\nPresent and explain team design decisions in written and oral formats\n\nCollaborate effectively in team-based development projects\n\n\n\n\n\n\nProcedural & Data Abstraction\n\nModularity, Objects, State\n\nUnified Modeling Language (UML)\n\nDesign Principles & Patterns\n\nMetalinguistic Abstraction\n\nProject Presentations & Evaluation\n\n\n💡 Course Catalog | Email"
  },
  {
    "objectID": "teaching/coms3090/index.html",
    "href": "teaching/coms3090/index.html",
    "title": "COMS 3090",
    "section": "",
    "text": "This course provides a practical introduction to software engineering principles including process models, requirements engineering, system design, testing, and project management. It emphasizes collaborative software development through a semester-long group project, focusing on teamwork, accountability, and real-world delivery practices.\n\n\n\n\nStudents completing this course will:\n\nGain practical exposure to the software development lifecycle, from requirements to implementation and testing.\nLearn to apply project management techniques including cost estimation, scheduling, and risk analysis.\nCollaborate in teams to design, develop, and deliver a working software product.\nUse software engineering artifacts and tools such as source control, UML, SQL schemas, and versioning systems.\nUnderstand software ethics, legal responsibilities, and industry best practices.\nPractice client/server programming concepts including socket communication and APIs.\n\n\n\n\n\n\nSoftware Process Models and Lifecycle\n\nRequirements Analysis & Specification\n\nArchitecture and System Design\n\nProject Scheduling, Risk Management, and Metrics\n\nTesting, Inspections, and Code Reviews\n\nSoftware Configuration & Source Control\n\nDatabase Schema Design and SQL\n\nObject-Oriented Concepts and Modeling\n\nNetworking & Client-Server Programming (HTTP, DNS, Sockets)\n\nDNS, IP, URI\nIntroduction to the client/server model\nHttp\nSocket APIs\nClient/server APIs\n\nProfessional Ethics in Computing\n\n\n\n\n\nAs a Teaching Assistant for COMS 3090, I supported over 150 students in mastering software development practices through lectures, labs, and team projects. My responsibilities included:\n\nMentoring Project Teams: Guided multiple student groups through requirements gathering, iterative design, and implementation milestones for their semester-long projects.\nLab Instruction: Facilitated labs covering Git, object-oriented modeling, system architecture, and backend programming fundamentals.\nTechnical Support: Helped students with debugging, API integrations, testing techniques, and version control during weekly office hours.\nEvaluation & Feedback: Assisted in grading project deliverables, presentations, and code quality, providing constructive feedback for improvement.\nProcess Coaching: Reinforced agile principles, milestone planning, documentation quality, and collaborative communication.\n\nThis role allowed me to blend technical mentorship with project coordination, while reinforcing industry practices and promoting engineering discipline among future software professionals.\n\n💡 Course Catalog | Email"
  },
  {
    "objectID": "teaching/coms3090/index.html#coms-3090-software-development-practices",
    "href": "teaching/coms3090/index.html#coms-3090-software-development-practices",
    "title": "COMS 3090",
    "section": "",
    "text": "This course provides a practical introduction to software engineering principles including process models, requirements engineering, system design, testing, and project management. It emphasizes collaborative software development through a semester-long group project, focusing on teamwork, accountability, and real-world delivery practices.\n\n\n\n\nStudents completing this course will:\n\nGain practical exposure to the software development lifecycle, from requirements to implementation and testing.\nLearn to apply project management techniques including cost estimation, scheduling, and risk analysis.\nCollaborate in teams to design, develop, and deliver a working software product.\nUse software engineering artifacts and tools such as source control, UML, SQL schemas, and versioning systems.\nUnderstand software ethics, legal responsibilities, and industry best practices.\nPractice client/server programming concepts including socket communication and APIs.\n\n\n\n\n\n\nSoftware Process Models and Lifecycle\n\nRequirements Analysis & Specification\n\nArchitecture and System Design\n\nProject Scheduling, Risk Management, and Metrics\n\nTesting, Inspections, and Code Reviews\n\nSoftware Configuration & Source Control\n\nDatabase Schema Design and SQL\n\nObject-Oriented Concepts and Modeling\n\nNetworking & Client-Server Programming (HTTP, DNS, Sockets)\n\nDNS, IP, URI\nIntroduction to the client/server model\nHttp\nSocket APIs\nClient/server APIs\n\nProfessional Ethics in Computing\n\n\n\n\n\nAs a Teaching Assistant for COMS 3090, I supported over 150 students in mastering software development practices through lectures, labs, and team projects. My responsibilities included:\n\nMentoring Project Teams: Guided multiple student groups through requirements gathering, iterative design, and implementation milestones for their semester-long projects.\nLab Instruction: Facilitated labs covering Git, object-oriented modeling, system architecture, and backend programming fundamentals.\nTechnical Support: Helped students with debugging, API integrations, testing techniques, and version control during weekly office hours.\nEvaluation & Feedback: Assisted in grading project deliverables, presentations, and code quality, providing constructive feedback for improvement.\nProcess Coaching: Reinforced agile principles, milestone planning, documentation quality, and collaborative communication.\n\nThis role allowed me to blend technical mentorship with project coordination, while reinforcing industry practices and promoting engineering discipline among future software professionals.\n\n💡 Course Catalog | Email"
  },
  {
    "objectID": "teaching/cheatsheet_pkgdev/index.html",
    "href": "teaching/cheatsheet_pkgdev/index.html",
    "title": "Pkg Dev at CSIDS",
    "section": "",
    "text": "Rstat"
  },
  {
    "objectID": "teaching/teaching.html",
    "href": "teaching/teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "As a Teaching Assistant at Iowa State University, I am dedicated to creating an inclusive and dynamic learning environment that enables students to excel in computer science. My teaching approach emphasizes practical application of theoretical concepts, encouraging students to develop hands-on experience in software development, user interface design, and database management. I believe in preparing students for real-world challenges in the technology industry through project-based learning and personalized support.\nCourses I have served as a Teaching Assistant or Mentor in:"
  },
  {
    "objectID": "rpkg/nowcast/index.html",
    "href": "rpkg/nowcast/index.html",
    "title": "nowcast",
    "section": "",
    "text": "https://github.com/csids/nowcast"
  },
  {
    "objectID": "rpkg/ggehr/index.html",
    "href": "rpkg/ggehr/index.html",
    "title": "ggehr",
    "section": "",
    "text": "ggehr (read: gg E-H-R) stands for ggplot2 extension for EHR data, which provides a set of tools to facilitate EHR (Electronic Health Records) visualization.\nggehr package helps you make visualize EHR data, so that you can\n\nhave an overview of the mixed type information related to a patient;\nvisually identify the errors in data recording.\n\nLearn more about ggehr"
  },
  {
    "objectID": "rpkg/noreden/index.html",
    "href": "rpkg/noreden/index.html",
    "title": "noreden",
    "section": "",
    "text": "This package provides tools to facilitate sustainable diet discovery.\nGitHub Link"
  },
  {
    "objectID": "rpkg/csalert/index.html",
    "href": "rpkg/csalert/index.html",
    "title": "csalert",
    "section": "",
    "text": "https://github.com/csids/csalert"
  },
  {
    "objectID": "rpkg/rpkg.html",
    "href": "rpkg/rpkg.html",
    "title": "Research",
    "section": "",
    "text": "I aim to advance the frontier of safe, interpretable, and adaptive AI for cyber-physical systems operating under uncertainty and dynamic constraints. My research sits at the intersection of machine learning, optimization, and control theory, with a particular focus on:\n\nPhysics-informed Deep Reinforcement Learning (DRL)\nProbabilistic & Bayesian Modeling\nLarge Language Models (LLMs) for autonomous reasoning\nVision-based simulation environments\n\nBy tightly integrating domain knowledge into learning frameworks, I design agents capable of robust decision-making in real-world, high-stakes environments such as smart grids, robotics, and intelligent infrastructure."
  },
  {
    "objectID": "rpkg/rpkg.html#research-vision",
    "href": "rpkg/rpkg.html#research-vision",
    "title": "Research",
    "section": "",
    "text": "I aim to advance the frontier of safe, interpretable, and adaptive AI for cyber-physical systems operating under uncertainty and dynamic constraints. My research sits at the intersection of machine learning, optimization, and control theory, with a particular focus on:\n\nPhysics-informed Deep Reinforcement Learning (DRL)\nProbabilistic & Bayesian Modeling\nLarge Language Models (LLMs) for autonomous reasoning\nVision-based simulation environments\n\nBy tightly integrating domain knowledge into learning frameworks, I design agents capable of robust decision-making in real-world, high-stakes environments such as smart grids, robotics, and intelligent infrastructure."
  },
  {
    "objectID": "rpkg/rpkg.html#research-areas",
    "href": "rpkg/rpkg.html#research-areas",
    "title": "Research",
    "section": "Research Areas",
    "text": "Research Areas\n\n🔬 Safe Reinforcement Learning\n\nPhysics-Informed RL with embedded domain constraints\nAdversarial robustness & uncertainty quantification\nSafe exploration and constrained policy optimization\n\n\n\n🎯 Transfer Learning & Meta-Learning\n\nPolicy transfer across varied network topologies\nSimulation-to-real transfer in safety-critical domains\nCross-domain generalization across environments\n\n\n\n🔎 Vision-Based Simulation\n\nPerception and control using CARLA simulator\nComputer vision for object detection, scene understanding\nVision-based control pipelines for autonomous systems\n\n\n\n🔗 LLM-Augmented Control Reasoning\n\nIntegrating large language models for high-level decision-making\nContext-aware planning and explainable reasoning\nHuman-AI collaboration in control and robotics\n\n\n\n⚡ Core Application Domains\n\nSmart Energy Systems (Volt-VAR Control, Grid Resilience)\nAutonomous Vehicles & Robotics\nAI Safety in Cyber-Physical Infrastructure"
  },
  {
    "objectID": "rpkg/rpkg.html#publications",
    "href": "rpkg/rpkg.html#publications",
    "title": "Research",
    "section": "📚 Selected Publications",
    "text": "📚 Selected Publications\n\n📝 Journal Papers🎤 Conference Papers\n\n\n\nKundan Kumar, Gelli Ravikumar\nPhysics-based Deep Reinforcement Learning for Grid-Resilient Volt-VAR Control (Under Review)\nIEEE Transactions on Smart Grid, 2025\n Paper Code Poster \n\n\n\n\n\nKundan Kumar, Gelli Ravikumar\nTransfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids\nIEEE PES Grid Edge Technologies Conference & Exposition, 2025\n Paper Code Poster \nKundan Kumar, Aditya Akilesh Mantha, Gelli Ravikumar\nBayesian Optimization for Deep Reinforcement Learning in Robust Volt-Var Control\nIEEE PES General Meeting, 2024\n Paper Code Poster \nKundan Kumar, Gelli Ravikumar\nDeep RL-based Volt-VAR Control and Attack Resiliency for DER-integrated Distribution Grids\nIEEE ISGT, 2024\n Paper Code Poster \nJK Francis, C Kumar, J Herrera-Gerena, Kundan Kumar, MJ Darr\nDeep Learning and Pattern-based Methodology for Multivariable Sensor Data Regression\nIEEE ICMLA, 2022\n Paper Code Poster \nKin Gwn Lore, Nicholas Sweet, Kundan Kumar, N Ahmed, S Sarkar\nDeep Value of Information Estimators for Collaborative Human-Machine Information Gathering\nACM/IEEE ICCPS, 2016\n Paper Code Poster"
  },
  {
    "objectID": "rpkg/medicode/index.html",
    "href": "rpkg/medicode/index.html",
    "title": "medicode",
    "section": "",
    "text": "This package provides metadata and tools for medical classification and clinical coding.\nGitHub Link\nIt is especially useful for English and Norwegian (Bokmål) languages.\nPlanned content:\n\nICD-10\nICPC-2\nEuropean shortlist for Cause of Death"
  },
  {
    "objectID": "rpkg/cstime/index.html",
    "href": "rpkg/cstime/index.html",
    "title": "cstime",
    "section": "",
    "text": "cstime provides convenient and consistent conversion between\n\nisoyear\nisoweek\ncalyear\nseason week (used in influenza surveillance)\n\nGitHub link"
  },
  {
    "objectID": "blog/technotes_20230205_clinreport_part1/index.html",
    "href": "blog/technotes_20230205_clinreport_part1/index.html",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 1",
    "section": "",
    "text": "This is a course provided by Genentech (part of Roche) on Coursera.\nCourse link"
  },
  {
    "objectID": "blog/technotes_20230205_clinreport_part1/index.html#introduction-to-clinical-trial",
    "href": "blog/technotes_20230205_clinreport_part1/index.html#introduction-to-clinical-trial",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 1",
    "section": "Introduction to clinical trial",
    "text": "Introduction to clinical trial\nClinical trial: aim to demonstrate that drug is safe and effective (safety, efficacy)\n\nphase 1: 10-20 people, focus on safety (healthy volunteers)\nphase 2: 100, study of side effects, determine best dose\nphase 3: 1000, demonstrate drug efficacy, fuller safety profile (common across multiple regions, ethnicities)\n\ncollecting data from different hospitals, hence important to ensure standards are being followed\n\nevidence must be submitted to health authorities (FDA, EMA European medicines agency)\nhealth authorities determine whether the drug is submitted to market\n\nSubmit the analysis plan in advance"
  },
  {
    "objectID": "blog/technotes_20230205_clinreport_part1/index.html#why-share-data",
    "href": "blog/technotes_20230205_clinreport_part1/index.html#why-share-data",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 1",
    "section": "Why share data",
    "text": "Why share data\n\nregulatory requirements\nscientific community interest\ncompany internal research interest\nmarketing materials\n\n\nData and results sharing\n\nRegulatory req (e.g. EMA require sharing clinical trial results to gain marketing authorization for pharma products, FDA require sharing data)\nscientific community (peer review check accuracy, perform additional analyses, derive new hypothesis)\nCDISC standards\n\nCDASH (clinical data acquisition standards harmonization)\nSDTM (study data tabulation model)\n\nformat for ‘raw’ data, define datasets, structures, contents, variable attributes\n\nSEND (standard for exchange of non clinical data)\nADaM (analysis data model)\n\ndata format for data processed for analysis (e.g. converted, imputed, derived)\n\n\nDictionary\n\ne.g. nose congestion, stuffy nose, … need to be standardized\nMedDRA: standard dictionary for medical conditions, events and procedures\nWHO drug dictionary (for pharma agents)\n\nSAP statistical analysis plan\n\nbased on study protocol, focus on statistical methodology, is regulated\n\nProgramming specification\n\nbased on SAP, provides additional details on datasets and tables, listing and figures (TLFs) required for statistical analysis. focus on programming details. Not regulated\n\n\n\n\nQuality assurance\n\nGood clinical practice (GCP), issued by ICH\npurpose: prevent mistakes, reduce inefficiencies/waste in a process, increase reliability/trustworthiness of the product of a process\nClinical monitoring: performed by a clinical research assistant (CRA) at investigator sites, checks that study protocols are executed as intended, and site processes result in accurate data capture. Focus on trial subjects’ safety. Traditional goal: 100% source data verification\nData quality checks (more relevant for data scientists). Checks data for technical conformance, and data plausibility. Focus on data quality. Traditional goal: 100% accurate and format compliant data\nCode review\nDouble programming\n\n\n\nData access restrictions\n\nreasons\n\ndata collected is very sensitive (health data), need data protection\nclinical trial data is a key asset and revenue predictor for pharma companies, high confidentiality levels\nscientific validity, data is ideally double blinded, no-one should know whether a subjecttreatment is as long as the data is still being collected\n\npseudonymization: data de-identification\n\nuse pseudonym (ID), link is recorded to allow re-identification\n\nanonymization: limit the risk of re-identification\n\nremove variables, remove values, replace more precise values with more general categories, replace personal identifiers with random identifiers\n\nFSP, CRO (out-sourcing), personnels require data access at different levels\nUnblinding"
  },
  {
    "objectID": "blog/readnotes_20240218_open_source/index.html",
    "href": "blog/readnotes_20240218_open_source/index.html",
    "title": "Working in Public: The Making and Maintenance of Open Source Software - Nadia Eghbal",
    "section": "",
    "text": "Github as a platform\n\nOn contribution\nNearly half of all contributors only contributed once; which accounted for less than 2% of total commits.\nThe pattern that one or a few developers do most of the work, followed by many casual contributors and even more passive users is the norm, not exception in open source.\nOn casual contributors: they primarily see themselves as users of the project, rather than a part of a contributor community.\nChallenge for maintainers: not how to get more contributors, but how to manage high volume of frequent, low-touch interactions (directing air traffic)\n\nGithub’s open source developers have more in common with solo creators on Twitter, Instagram, YouTube or Twitch.\n\nComparing early internet and social platform nowadays: the early online communities have mailing lists, online forums, membership groups, operated like villages that have their own culture, history and norms. Nowadays creators have much bigger potential audience but the relationship is one-sided, and can be overwhelming.\n\n\nOn free software and hacker\n“Free” means you are able to do what you want with the software, rather than the cost. Libre rather than gratis. At least at the beginning.\nBravado, showmanship, mischievousness, deep mistrust of authority. This culture in the 1980s and 90s was closely linked to the early open source software.\n“Bazaar”: highly participatory, versus “Cathedral”: restricted to a smaller group\nToday’s developer hardly even notice “open source” as a concept anymore, they just want to write and publish their code. They prioritize convenience over freedom or openness.\n\n\nOn licensing\nThe widespread use of permissive licensing is popularized by GH.\nCopyleft licensing (e.g. GNU General Public License GPL) is not commercial friendly as it requires companies to license their software that depend on open source GPL software to have the same license. However GPL gives developers more control over how others use their code in the long run.\n\nAs with any other online content today, sharing is the default.\n\n\n\n\nThe structure of an open source software\n\nOn how projects evolve\nCreate -&gt; Promote and distribute -&gt; Grow\nProjects are promoted like a founder would promote a startup: share on the relevant channels online, give talks at conference and meetups, encourage others to write and talk about it\nA sign that the software is used widely: when the maintainer starts doing more non-code (triage issues, review pull requests) rather than code work.\n\n\nContributor and users\nDepends on technical scope (whether there is much to do), support required (code and admin work), ease of participation (whether on Github) and user adoption (potential contributor base).\nFour types of projects\n\nhigh user growth, high contributor growth: federations. Rare, impactful, the ‘ideal’ of open source project. Roughly 3% of open source projects. Examples: Rust, Node.js, Linux\nhigh user growth, low contributor growth: stadiums: powered by one or a few developers. Centralized.\nlow user growth, high contributor growth: club. Similar to meetup or hobby groups, do not have a wide reach but are loved and built by enthusiasts.\nlow user growth, low contributor growth: toys. Personal project, isn’t trying to grow its user base. Projects on Github with less than 10 stars. Authors do not expect to receive contributions nor do they care about whether people are watching.\n\nDecentralized communities (clubs and federations) have the potential for high user growth - recruit new contributors, reduce contribution friction.\nCentralized communities (stadium) depends on the creators to manage user demand - automation, elimination of noise\n\n\n\nRoles, incentives and relationships\n\nFirms or communities\nFirms (companies, organizations): centralized resources; from a coordination standpoint, managing resources would be more efficient within the same organization - which does not explain why open source developers make software together without formal contracts and financial compensation.\n\n\nThe commons and peer production\nTragedy of the commons: resources depleted by people acting in their own self-interest rather than in the collective interest.\n(One of the 8 design principles by Ostrom on) successful commons:\n\nThose who are affecteed by the rules can participate in modifying them.\n\nStrong sense of group identity maks rules, dispute resolution more meaningful.\nCoordination cost is lower when self-organized based on who wants to do the work most, anyone can do the advertised work and volunteer.\nIn contrast, in companies - solicit, evaluate, hire, manage employees; only employees can do the work limited by their job functions.\nPeople collaborating online for no obvious reason beyond personal satisfaction (intrinsic motivation)\nModular and granular tasks: how tasks are organized, and how big each task is.\nLow coordination costs: quality control over thee modules, integrate the contributions into the finished product\n\n\nContribution beyond code\nSome users do not consider them a contributor, but do actually contribute by education, spreading the word, support (forum), bug reports and more.\nThese active users are similar to contributors but operate independently from project’s contributor community."
  },
  {
    "objectID": "blog/blog_20241105_positconf2024/index.html",
    "href": "blog/blog_20241105_positconf2024/index.html",
    "title": "Personal Highlights: Positconf 2024",
    "section": "",
    "text": "Tyler Morgan-wall: Quarto, AI, and the Art of Getting Your Left Back\nTransform old blog (Wordpress) to Quarto. Quarto can render pre-exisiting HTML\nCustomize website with AI, with css\nImage carousel, transform some grid images into an auto rotating carousel (JS). Can also add screenshot; explain what the code is doing\n\n\nAlenka Frim and Nic Crane: Mixing R, Python, and Quarto: Crafting the Perfect Open Source Cocktail\nComparing the tools to make dashboards:\n\n\n\n\n\n\n\n\n\n\n\nDashboard aesthetic\nMarkdown syntax\nDeploy with GH actions\nBoth R and python\n\n\n\n\nShiny\nYes\nNo\nNo / Yes with Shinylive\nYes\n\n\nQuarto doc\nNo\nYes\nYes\nYes\n\n\nQuarto dashboard\nYes\nYes\nYes\nYes\n\n\n\nEngine: knitr and jupyter\nSean Nguyen: Beyond Dashboards: Dynamic Data Storytelling with Python, R, and Quarto Emails\nRemove friction of too much dashboards\n\nlogging in can create barriers\nmeeting executives where they are (email, for example)\n“no-click” insights (e.g. add key metric/alerts in the subject line or notification)\n\nTools: quarto emails, pins (a package), posit connect\n\ndata sources -&gt; data warehouse (big query) -&gt; quarto (create pins.qmd) -&gt; pins (marketing data.csv pin, for example)\nchange quarto format to email\ngenerate multiple using purrr"
  },
  {
    "objectID": "blog/blog_20241105_positconf2024/index.html#quarto",
    "href": "blog/blog_20241105_positconf2024/index.html#quarto",
    "title": "Personal Highlights: Positconf 2024",
    "section": "",
    "text": "Tyler Morgan-wall: Quarto, AI, and the Art of Getting Your Left Back\nTransform old blog (Wordpress) to Quarto. Quarto can render pre-exisiting HTML\nCustomize website with AI, with css\nImage carousel, transform some grid images into an auto rotating carousel (JS). Can also add screenshot; explain what the code is doing\n\n\nAlenka Frim and Nic Crane: Mixing R, Python, and Quarto: Crafting the Perfect Open Source Cocktail\nComparing the tools to make dashboards:\n\n\n\n\n\n\n\n\n\n\n\nDashboard aesthetic\nMarkdown syntax\nDeploy with GH actions\nBoth R and python\n\n\n\n\nShiny\nYes\nNo\nNo / Yes with Shinylive\nYes\n\n\nQuarto doc\nNo\nYes\nYes\nYes\n\n\nQuarto dashboard\nYes\nYes\nYes\nYes\n\n\n\nEngine: knitr and jupyter\nSean Nguyen: Beyond Dashboards: Dynamic Data Storytelling with Python, R, and Quarto Emails\nRemove friction of too much dashboards\n\nlogging in can create barriers\nmeeting executives where they are (email, for example)\n“no-click” insights (e.g. add key metric/alerts in the subject line or notification)\n\nTools: quarto emails, pins (a package), posit connect\n\ndata sources -&gt; data warehouse (big query) -&gt; quarto (create pins.qmd) -&gt; pins (marketing data.csv pin, for example)\nchange quarto format to email\ngenerate multiple using purrr"
  },
  {
    "objectID": "blog/blog_20241105_positconf2024/index.html#python",
    "href": "blog/blog_20241105_positconf2024/index.html#python",
    "title": "Personal Highlights: Positconf 2024",
    "section": "Python",
    "text": "Python\nEmily Riederer: Python Rgonomics\nPython alternatives to R. Worth rewatching!"
  },
  {
    "objectID": "blog/blog_20241105_positconf2024/index.html#teaching-and-education",
    "href": "blog/blog_20241105_positconf2024/index.html#teaching-and-education",
    "title": "Personal Highlights: Positconf 2024",
    "section": "Teaching and education",
    "text": "Teaching and education\nAndrew Gard: Teaching and learning data science in the era of AI\nStudents don’t know enough to be able to edit the prompt to reach a sensible code chunk, AI guessed and guessed wrong. We should not expect AI to guess information that we do not provide!\nStudents should still learn to code, and teachers should ask better questions - instead of asking for the final result (create a bar plot), ask students to critically think: why doesn’t the AI-generated code work? what information is missing? how do you improve the prompt?\nJames Wade: Posit Academy in the Age of Generative AI - Lessons from the Frontlines\nchattr, gptstudio, github copilot\nPosit Academy learners (over half) give AI code assistants 2 star rating or less\nRewarding, high-growth period. Threshold concepts: once understood, transforms your perception and approach of a discipline, and these must be encountered not told.\nTC in DS:\n\ntidy data enables efficient analysis\nmodular code enhances re-usablity and clarity\nvisualization as a tool for exploration and communication\n\nHow to incorporate AI code assistants (in DS class)\n\nearly stage: explain this code piece by piece\nmid stage: add a roxygen skeleton to my code\nlate stage: try code assistants in the IDE\n\nTC for code assistants:\n\ndrive faster but don’t forget to steer\nprompting matters, learning how to use these tools is a skill"
  },
  {
    "objectID": "blog/blog_20241105_positconf2024/index.html#statistics",
    "href": "blog/blog_20241105_positconf2024/index.html#statistics",
    "title": "Personal Highlights: Positconf 2024",
    "section": "Statistics",
    "text": "Statistics\nHannah Frick: tidymodels for time-to-event data\nMax Kuhn: Evaluating time-to-event models is hard\nDemetri Pananos - Making sense of marginal effects"
  },
  {
    "objectID": "blog/technotes_20230301_clinreport_part4/index.html",
    "href": "blog/technotes_20230301_clinreport_part4/index.html",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "",
    "text": "This is a course provided by Genentech (part of Roche) on Coursera.\nCourse link"
  },
  {
    "objectID": "blog/technotes_20230301_clinreport_part4/index.html#open-source-packages",
    "href": "blog/technotes_20230301_clinreport_part4/index.html#open-source-packages",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "Open source packages",
    "text": "Open source packages\nExmample:\n\nsurvival: 8 developers, &gt;18 years\nadmiral: 25 developers, &gt;1 year\ntern: 77 developers, 5 years\nrtables: 21 developers, 4 years\n\nEngagement across these packages is different, some receive more issues and comments, some receive more code contributions.\nStale: stable? abandoned?\nContribution is highly skewed, a few contributors write the majority of the code.\nR package life cycles (indicative, not guaranteed)\n\nexperimental (ready to use?)\nstable (safe to use?)\ndeprecated, no longer maintained\nsuperseded, something better exists\n&lt;1.0: big changes likely; &gt;=v1.0: is it safe to use?"
  },
  {
    "objectID": "blog/technotes_20230301_clinreport_part4/index.html#risk-mitigation-for-r-packages",
    "href": "blog/technotes_20230301_clinreport_part4/index.html#risk-mitigation-for-r-packages",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "Risk mitigation for R packages",
    "text": "Risk mitigation for R packages\nCombine external and internal packages (CI/CD release)\n-&gt; automated package data collection\n-&gt; automated quality checks: if not pass, assess\n-&gt; package repo integration tests\n-&gt; publish to package repo, generate package validation report"
  },
  {
    "objectID": "blog/technotes_20230301_clinreport_part4/index.html#assess-external-packages-for-statistical-methods",
    "href": "blog/technotes_20230301_clinreport_part4/index.html#assess-external-packages-for-statistical-methods",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "Assess external packages for statistical methods",
    "text": "Assess external packages for statistical methods\nDoes it provide the required functionality?\n\nCorrect statistical method?\nCould you extend it?\nCorrect results? (compare with another software)\nDo you understand the method? (check the paper linked with package)\n\nDoes it work reliably?\n\nPublished? (e.g. on CRAN)\nDifferent inputs?\nFast?\nDo other people use it? (downloads)\nDoes other software use it? (reverse dependencies)\n\nDoes the code look robust and well tested?\n\nHow are the functions implemented\nIs the source code readable\nCoverage with unit tests\nMature package?\n\nIs it well documented?\n\nDocumented functions?\nVignettes?\nPublished?\nInformative NEWS entry?\n\nWho are the authors, are they responsive?\n\nDid they publish statistics papers on this topic\nIs a github site with issues available"
  },
  {
    "objectID": "blog/technotes_20230301_clinreport_part4/index.html#tools",
    "href": "blog/technotes_20230301_clinreport_part4/index.html#tools",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "Tools",
    "text": "Tools\ncovr and unit tests\nriskmetric and the R Validation Hub\npharmaverse.org, with end-to-end examples"
  },
  {
    "objectID": "blog/blog_20230904_cen2023/index.html",
    "href": "blog/blog_20230904_cen2023/index.html",
    "title": "Personal Highlights: CEN2023",
    "section": "",
    "text": "The IBS (International Biometric Society) conference of the Central European Network, CEN2023 has been a great opportunity to keep myself up to date with the latest development of biostatistics, both in academia and industry. Thanks to the great effort made by the organizing committee and almighty Google Meet/Zoom, I have been able to follow the talks without any issue, and have definitely learned a lot.\nGiven my background, I paid more attention on talks and workshops on\n\nStatistical software, R programming and simulation\nCausal inference\n\nThere were also two topics that drew my attention: one is on statistical education towards medial professionals, the other is on a Data Challenge using RCT data.\n\nStatistical software\n\nSoftware Engineering Working Group (SWE WG), MMRM\n\n\n\n\n\n\nTalk information\n\n\n\n\n\n\nDaniel Sabanes Bove (Roche). First year of the Software Engineering working group - working together across organizations\nGonzalo Duran-Pacheco (Roche). Comparing R libraries with SAS’s PROC MIXED for the analysis of longitudinal continuous endpoints using MMRM\n\n\n\n\nThe ASA Biopharmaceutical Section (BIOP) Software Engineering Working Group SWE WG was established in 2022. Currently they have 3 work streams:\n\nmmrm implements MMRM (mixed models with repeated measures)\nbrms.mmrm, the Bayesian version of MMRM\nHealth Technology Assessment with R\n\nAt a later talk, mmrm was compared with SAS’s PROC MIXED and R’s nlme, glmmTMB for analyses of longitudinal continuous endpoints. In terms of speed and convergence, mmrm is superior than others; while the estimate prodouced by mmrm is very close to PROC MIXED and glmmTMB.\nThis looks like a very interesting tool to try out! Vignette\n\n\nSimulation tools and RWD\n\n\n\n\n\n\nTalk information\n\n\n\n\n\n\nMichael Kammer (Medical University of Vienna). An overview of R software tools to support simulation studies: towards standardizing coding practices.\n\n\n\n\nKammer and colleagues did a review on R packages for simulation, and selected 14 top simulation packages, including simstudy, simdata, synthpop, bigsimr and others. The full list is made available here.\nA real-world dataset, NHANES was also introduced here. The data can be accessed with R package nhanesA.\n\n\n\nCausal Inference\n\n\n\n\n\n\nInformation\n\n\n\n\n\nWorkshop: Implementing the estimand framework in global drug development: Application of causal inference approaches (Mouna Akacha, Björn Bornkamp, Alex Ocampo, Jiawei Wei at Novartis)\nKeynote: Ruth Keogh (LSHTM). Causal inference with observational data: A survival guide\n\n\n\nThese two workshop / talk cover slightly different scenarios: one in RCT, one for observational data. It deserves a whole article or more to elaborate on this topic, so I’m only putting some resources here.\nCausal inference is definitely gaining traction in recent years in both academia and industry. Techniques such as g-computation, IPW and doubly robust estimation are starting to become mainstream. It is fascinating that these techniques themselves are not bound to a fixed model.\nResources:\n\nWorkshop repository Causal-inference-in-RCTs\nBook: Causal Inference: What If by Hernán and Robins (2020)\nPrincipal stratum strategy, Bornkamp et al. (2021)\nTime-dependent covariates, Keogh et al. (2023)\nTarget Trial Emulation (TTE), Hernán and Robins (2016)\n\n\n\nCovariate adjustment and data challenge with RCT data\n\n\n\n\n\n\nTalk information\n\n\n\n\n\n\nKelly Van Lancker (Ghent University). Improving Power in Randomized Trials by Leveraging Baseline Variables\nDominic Magirr (Novartis). Organizing a Data Challenge on Covariate Adjustment in RCTs\nCraig Wang (Novartis). Participating in a Data Challenge on Covariate Adjustment in RCTs\n\nPanel discussion: Jonathan Bartlett (LSHTM)\n\n\n\n23 teams at Novartis participated in a Data Challenge on Covariate Adjustment. They were given a fixed outcome model, and 5 prior studies trial data, and their task was to create the design matrices that improve the precision compared to unadjusted data.\nIf I were to select talks based on the category titles, I would probably missed the whole session. However, it is surprisingly similar to using not trial, but real-world data (such as EHR) to make predictions. The conclusion were similar as well: using “supercovariates” created by ML isn’t gaining much compared to simple models such as ANCOVA. Possible reasons:\n\nsmall to moderate data size\nlinear relationship between covariates and outcome\ngood enough prognostic variables\n\nIt was also mentioned that the winning team did some trick to reduce the variance among the covariates. Would be interesting to read about it.\nSome resources:\n\nLancker et al. The use of covariate adjustment in randomized controlled trials: an overview link\nCovariate adjustment tutorial, link\n\n\n\nStatistical education\n\n\n\n\n\n\nTalk information\n\n\n\n\n\n\nMaren Vens et al (University of Lübeck). Biostatistics/Biometrics for physicians – essential or unnecessary? How do practicing physicians and dentists evaluate biostatistics? A cross-sectional survey\n\n\n\n\nStatistical education to students / professionals who are not used to working with data has always been tricky. Students generally think statistics is difficult, and need help from a statistician. However there are only limited number of statisticians. The talk by Vens and colleagues confirms what practicing statisticians know, but can’t do much about: most (87%) physicians and dentists in the survey need a statistician to help with their work.\nHow to improve the statistical competency is an important and relevant topic for discussion, and might require systematic changes in how it is taught. Use of modern technology can help, yet it’s only helpful when students start to not fear, or not find math and technology boring."
  },
  {
    "objectID": "blog/blog_20230301_ds_clinreport/index.html",
    "href": "blog/blog_20230301_ds_clinreport/index.html",
    "title": "Course review: making DS work for clinical reporting",
    "section": "",
    "text": "This is a course provided by Genentech (part of Roche) on Coursera (course link). It is not necessary to have a paid coursera membership to view the course, everyone could access it.\nIt is a 4 part course released one month ago (Jan/Feb 2023), and it seems that a follow-up will be released in the future.\nOverall I think it strikes a good balance between high-level introduction of the good practices, and examples with how they are implemented. Even though the course focuses on clinical reporting in the pharmaceutical industry, the practices are highly relevant in other sectors as well (e.g. public health, academia, other industries that use open-source software).\nSpecific statistical methods, packages are introduced only at a high-level; which means the course is not for learning how to use this or that packages; but good practice guidelines.\nIn my opinion,\n\nit would be useful if the learner has some experience with software development and/or statistics; otherwise learners might not know how to practice them.\nmost of the examples are related to R packages (understandable), so some experience with R package (use or develop) is useful.\nit could be a very good study material for university students in related subjects.\n\n\n\n\nModule 1 (notes): what the requirements are regarding clinical reporting, what should be done to meet the quality standards;\nModule 2 (notes): DevOps and Agile\nModule 3 (notes): version Control, git workflows, reproducible clinical reporting\nModule 4 (notes): code quality, robust and reusable code, R packages\nModule 5 (notes): risk management with open source software\n\n\n\n\nI have a few years of experience as an R developer and academic researcher in related fields, so not all concepts are new to me. Nevertheless, I still learned quite a bit. For example,\n\n(Module 1) Data and results sharing needs to follow certain standards, such as CDISC; there are different industry standards to follow when it comes to data acquisition, tabulation and analysis (e.g. ADaM)\n(Module 2) Data scientists not only need hard skills, but also soft skills - they need to be able to wear many hats, and be more flexible and resilient.\n(Module 4, 5) Tests are extremely important. Think afar, develop your package so that they can be extended in the future. Design your package first, don’t start making your package immediately."
  },
  {
    "objectID": "blog/blog_20230301_ds_clinreport/index.html#each-module",
    "href": "blog/blog_20230301_ds_clinreport/index.html#each-module",
    "title": "Course review: making DS work for clinical reporting",
    "section": "",
    "text": "Module 1 (notes): what the requirements are regarding clinical reporting, what should be done to meet the quality standards;\nModule 2 (notes): DevOps and Agile\nModule 3 (notes): version Control, git workflows, reproducible clinical reporting\nModule 4 (notes): code quality, robust and reusable code, R packages\nModule 5 (notes): risk management with open source software"
  },
  {
    "objectID": "blog/blog_20230301_ds_clinreport/index.html#highlight",
    "href": "blog/blog_20230301_ds_clinreport/index.html#highlight",
    "title": "Course review: making DS work for clinical reporting",
    "section": "",
    "text": "I have a few years of experience as an R developer and academic researcher in related fields, so not all concepts are new to me. Nevertheless, I still learned quite a bit. For example,\n\n(Module 1) Data and results sharing needs to follow certain standards, such as CDISC; there are different industry standards to follow when it comes to data acquisition, tabulation and analysis (e.g. ADaM)\n(Module 2) Data scientists not only need hard skills, but also soft skills - they need to be able to wear many hats, and be more flexible and resilient.\n(Module 4, 5) Tests are extremely important. Think afar, develop your package so that they can be extended in the future. Design your package first, don’t start making your package immediately."
  },
  {
    "objectID": "blog/blog_20240923_quartofriends/index.html",
    "href": "blog/blog_20240923_quartofriends/index.html",
    "title": "Use Quarto, Make Friends: a two-year journey",
    "section": "",
    "text": "I wrote a blog back in early 2023 when I first switched from blogdown to Quarto on my initial impression (read here), and this is a two-year follow-up on my journey since I started using Quarto, for my personal website, teaching, scientific works and collaborative community projects."
  },
  {
    "objectID": "blog/blog_20240923_quartofriends/index.html#quarto-as-a-teaching-tool",
    "href": "blog/blog_20240923_quartofriends/index.html#quarto-as-a-teaching-tool",
    "title": "Use Quarto, Make Friends: a two-year journey",
    "section": "Quarto as a teaching tool",
    "text": "Quarto as a teaching tool\n\nFrom personal to workshop website\nI switched from blogdown to Quarto in late 2022, right after my PhD. It was initially a cure for a severe burnout from a combination of work-related stressors, when I desperately needed something other than research. My mental state was like the famous painting by Norwegian artist Edvard Munch:\n\n\n\n\n\nThe experience of the switch was explained in the previously mentioned blog. Briefly, it was light like a feather. Since I was quite satisfied, I thought, why don’t I make a workshop website? So I did.\nThe result was quite good, I made the (as far as I knew) first quarto workshop website at University of Oslo for the Oslo Bioinformatics Workshop Week 2022. Feedback from students were positive, and the instructor team thought it hosts the material in a more organized way.\n\n\nSingle day workshop -&gt; two week course\nI was greatly encouraged by the experience, so when I got a 50% position at University of Oslo as biostatistics lecturer, I thought, why don’t we have the same thing for the course?\n\n\n\n\n\nOh well, the workload is crushing. There were a few key differences:\n\nR scripts and material were unavailable since the course was originally in STATA. Everything need to be done from scratch, for at least 12 lab sessions;\nThe students generally have little IT skills, which means more effort need to be done to guide them through the ‘get started’ part.\n\n\n\n\n\n\nIt took one month to create the first version of the website. More details about the experience can be read here.\n\n\nAdding WebR to the course\nOne year later, as technology advances, we added new content to some parts of the website. Most notably is the interactivity achieved through WebR. For example, I made this page on randomness and statistical distribution where students can interactively modify code chunks in a web browser."
  },
  {
    "objectID": "blog/blog_20240923_quartofriends/index.html#quarto-as-a-collaboration-tool",
    "href": "blog/blog_20240923_quartofriends/index.html#quarto-as-a-collaboration-tool",
    "title": "Use Quarto, Make Friends: a two-year journey",
    "section": "Quarto as a collaboration tool",
    "text": "Quarto as a collaboration tool\nA static (or even interactive) website is not exactly what you call ‘collaborative tool’. However, if you work as a group towards something cool, Quarto might just be the tool you need. Check out the CAMIS project to find out what I mean by this!"
  },
  {
    "objectID": "blog/blog_20240923_quartofriends/index.html#what-else",
    "href": "blog/blog_20240923_quartofriends/index.html#what-else",
    "title": "Use Quarto, Make Friends: a two-year journey",
    "section": "What else?",
    "text": "What else?\nThe associated talk is available on YouTube, check it out!"
  },
  {
    "objectID": "blog/blog_20230921_positconf2023/index.html",
    "href": "blog/blog_20230921_positconf2023/index.html",
    "title": "Personal Highlights: Positconf 2023",
    "section": "",
    "text": "The yearly party of Positconf (formerly Rstudio conf) has come to an end. I joined the virtual experience at home, it is of course not the same as attending in-person, yet the atmosphere in discord was still great!\nIt’s hard to choose which talks to watch since multiple were scheduled at the same time, so one has to prioritize. I definitely will re-visit some of the talks at a later point, so this blog acts as a placeholder for links so that I can find them in the future."
  },
  {
    "objectID": "blog/blog_20230921_positconf2023/index.html#make-interactive-things",
    "href": "blog/blog_20230921_positconf2023/index.html#make-interactive-things",
    "title": "Personal Highlights: Positconf 2023",
    "section": "Make interactive things",
    "text": "Make interactive things\nWebDev is definitely a big thing at this year’s positconf. If I’m learning one thing from the conference, I’d check out webR.\nI still remember when R was mainly for statistical analysis and computing back when I learned it. Now it’s become much more fun! Strictly speaking, webRand quarto are not R per se. However, they’ve become the gateway drugs for R programmers to dabble in WebDev. With web assembly (wasm), now one can execute R code in a browser and even run shiny app.\nUnlock the power of dataViz animation and interactivity in quarto by Deepsha Menghani used a super fun example (F-bomb) to demonstrate how to add interactivity to your barplot (or other plots) with Crosstalk. Check out the talk here. The presentation was as interactive as the quarto slides, good job Deepsha!\nRunning shiny without a server by Joe Cheng (repo): this was a big announcement. I used shiny at work, but for my own projects or smaller teaching projects I tried to stay away from shiny - I was concerned about the fee. This looks like a promising thing to try out once it’s stable, although I’d probably do webR first."
  },
  {
    "objectID": "blog/blog_20230921_positconf2023/index.html#make-pretty-things",
    "href": "blog/blog_20230921_positconf2023/index.html#make-pretty-things",
    "title": "Personal Highlights: Positconf 2023",
    "section": "Make pretty things",
    "text": "Make pretty things\nIt is fascinating to see so many organizations and individual R developers make their own themes for better branding, recognition and storytelling. More and more peple have realized that making beautiful plots is important, and totally possible as well. Work on layout, color, font and sizes!\n\nThemes\nAdding a touch of glitr: Developing a package of themes on top of ggplot by Aaron Chafetz, Karishma Srikanth and colleagues at USAID. repo\n\n\nTables\nMaking tables with gt has been on my to-do list for a while now. It is very inspiring to see so many cool tables that makes you wonder, “is it really JUST a table?” For example, check out this gallery by Posit community.\nThe book Creating beautiful tables in R with gt by Albert Rapp would be a good place to learn how to make nice tables. Actually the reason why I wanted to use gt is that it seems to be the mainsteam in clinical reporting in pharma. I bumped into this blog post some time ago, and this would be my starting point.\n\n\nQuarto\nIf you want to go one step further and start making your quarto project pretty, there are a few things to try out.\nAlbert Rapp in his talk HTML and CSS for R Users stated that quarto is a gateway drug to WebDev. It reminds me of my very first presentation at my local R users community (2019) was about building a website with blogdown, and when I really spent a lot of time to make my markdown documentation colorful with span style - and that was about everything I knew.\nNow I want more. Learning HTML and CSS can make your dataviz, tables, slides and dashboards look not only professional but also special. I’m going to check out the scss variables in quarto which defines the theme, theme_file.scss. Emil Hvitfeldt (Styling and templating quarto documents) showed us how to make really pretty and animated (!) quarto sldies themes, and shared this template with us, quarto-revealjs-earth. I really like how revealjs slides look like, just that the MacOS Keynote (or MS ppt) drag-and-drop seems more flexible to me (?) Guess it’s something I should get used to over time.\nRichard Iannone (Extending quarto) introduced quarto shortcode extensions to add a bunch of fancy-looking icons to quarto files. To create extensions in general: https://quarto.org/docs/extensions/creating. This is for more pro-users since you needs to learn lua."
  },
  {
    "objectID": "blog/blog_20230921_positconf2023/index.html#quarto-updates",
    "href": "blog/blog_20230921_positconf2023/index.html#quarto-updates",
    "title": "Personal Highlights: Positconf 2023",
    "section": "Quarto updates",
    "text": "Quarto updates\nQuarto is definitely one of the most discussed topics in the year 2022-2023 in the R community. For good reasons. I need to catch up the the latest developments annd use-cases:\n\nWhat’s new in quarto? by Charlotte Wickham\nReproducible manuscripts with Quarto by Mine Çetinkaya-Rundel\nParametrized quarto reports improves understanding of soil health by Jadey Ryan\n\nand so many more. I couldn’t follow all the talks and I’m sure there are lots of great examples of how quarto is better than traditional ways of reporting."
  },
  {
    "objectID": "blog/blog_20230921_positconf2023/index.html#a-few-other-things-to-check-out",
    "href": "blog/blog_20230921_positconf2023/index.html#a-few-other-things-to-check-out",
    "title": "Personal Highlights: Positconf 2023",
    "section": "A few other things to check out",
    "text": "A few other things to check out\nBeyond the web and quarto topics, I think there are some existing and new tools that can be useful for my work. For example,\n\nI should review Hadley and Jenny’s R package book (2e).\nthis package targets for pipeline automation and management look like something that can be used for my analysis\n…\n\nIt will take a while to digest the latest developments. But little by little, we’ll get there! People in the R community are doing great things."
  },
  {
    "objectID": "blog/technotes_20230220_pkgdown/index.html",
    "href": "blog/technotes_20230220_pkgdown/index.html",
    "title": "R package website with pkgdown",
    "section": "",
    "text": "1. Create the website skeleton.\nBefore editing the details, we need to create the skeleton for the website. It can be done with usethis and pkgdown packages.\nIn R, run this:\nusethis::use_pkgdown()\nThis creates the _pkgdown.yml file, which is the place you configure your site.\nTo view the initial package website, use the following command:\npkgdown::build_site()\nThis creates docs/ directory containing a website\n\nREADME.md becomes the homepage,\ndocumentation in man/ generates a function reference,\nvignettes are rendered into articles/.\n\n\n\n2. Edit the vignette documentation\nMake sure that the vignette index is consistent with Title, otherwise it will not render.\n\n\n3. Build and preview your site\nNow check if the site looks good, and contents are correctly positioned.\npkgdown::preview_site()\npkgdown::build_site()\nYou can also do this to build the site.\npkgdown::build_site_github_pages()\n\n\n4. Deploy site with GitHub Pages\nThere seems to be two options:\n\nusethis::use_pkgdown_github_pages(), this function should take care of everything after pushing changes to GH.\nif you used pkgdown::build_site_github_pages() and pushed everything to GitHub, it might not automatically deploy your site to GH pages. I tried to go to Settings -&gt; Pages -&gt; Deploy from a branch -&gt; main -&gt; /docs, this makes Action deploy your site from the docs folder.\n\ndouble check if you have .nojekyll file\nif a website does not show, check whether you have docs in the .gitignore file; since you are deploying from that folder."
  },
  {
    "objectID": "blog/technotes_20230225_shinyappsio/index.html",
    "href": "blog/technotes_20230225_shinyappsio/index.html",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "",
    "text": "Useful references:"
  },
  {
    "objectID": "blog/technotes_20230225_shinyappsio/index.html#considerations",
    "href": "blog/technotes_20230225_shinyappsio/index.html#considerations",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "Considerations",
    "text": "Considerations\nA few ways to do it: Shiny Server (free), shinyapps.io (free and premium), and professional Rstudio Connect (paid).\nI choose to test out the second option, since it allows more possibilities compared to the free open-source Shiny Server.\nThe free option should allow me to create 5 apps, which is more than enough for personal use. It also allows 25 active hours per month; a note on that at the end."
  },
  {
    "objectID": "blog/technotes_20230225_shinyappsio/index.html#configuration",
    "href": "blog/technotes_20230225_shinyappsio/index.html#configuration",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "Configuration",
    "text": "Configuration\nSign up with GitHub account; or something else. It is possible to change account name afterwards.\nIn Rstudio,\n\nfirst install.packages('rsconnect')\nthen, configure the account. It can be done with rsconnect::setAccountInfo() with information provided in your own shinyapps.io page.\n\nBefore the last step, it is necessary to have an app to deploy!"
  },
  {
    "objectID": "blog/technotes_20230225_shinyappsio/index.html#create-my-first-shiny-project",
    "href": "blog/technotes_20230225_shinyappsio/index.html#create-my-first-shiny-project",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "Create my first shiny project",
    "text": "Create my first shiny project\nHere I use my usual workflow of creating a new R project:\n\nCreate a new repo on GitHub;\nClone the repo locally, by opening a new R project with version control.\n\nNow copy the two R scripts from the demo example:\n\nserver.R\nui.R\n\nTest locally by running shiny::runApp(). This should render the app."
  },
  {
    "objectID": "blog/technotes_20230225_shinyappsio/index.html#deploy-to-shinyapps.io",
    "href": "blog/technotes_20230225_shinyappsio/index.html#deploy-to-shinyapps.io",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "Deploy to shinyapps.io",
    "text": "Deploy to shinyapps.io\nrsconnect::deployApp() will deploy the app, with an automatically generated url that links to your account.\nThe demo app is deployed here.\n\nNote on active hours\nAfter deployment, the site seems to be active until you shut it down manually; or timeout. The default timeout is 15 minutes, which can be reduced to 5 minutes.\n25 hours per month suggests that I can open the site for 300 times (without manually shuting it down). It might be necessary to start using the paid options, if I have more than one site, or multiple users want to access it …"
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html",
    "href": "blog/technotes_20250703_research_guide/index.html",
    "title": "Research Scientist Interview Guide",
    "section": "",
    "text": "This guide provides a comprehensive framework to prepare for Research Scientist positions in academia, industry research labs (FAANG, OpenAI, DeepMind, etc.), and national labs. It combines insights from my own experience, interviews, and conversations with hiring managers."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#overview",
    "href": "blog/technotes_20250703_research_guide/index.html#overview",
    "title": "Research Scientist Interview Guide",
    "section": "",
    "text": "This guide provides a comprehensive framework to prepare for Research Scientist positions in academia, industry research labs (FAANG, OpenAI, DeepMind, etc.), and national labs. It combines insights from my own experience, interviews, and conversations with hiring managers."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#key-interview-components",
    "href": "blog/technotes_20250703_research_guide/index.html#key-interview-components",
    "title": "Research Scientist Interview Guide",
    "section": "Key Interview Components",
    "text": "Key Interview Components\n\n1️⃣ Research Portfolio Deep Dive\n\nBe able to explain your core research contributions in detail.\nClearly articulate: problem definition, novelty, methods, results, and real-world impact.\nPrepare multiple levels of technical depth (5-min, 15-min, 30-min versions).\nPractice connecting your work to broader research trends and applications.\n\n\n\n2️⃣ Technical Machine Learning Knowledge\n\nReinforcement Learning: algorithms, policy gradients, actor-critic, safe RL.\nDeep Learning: optimization, architecture design, generalization, transformers.\nProbabilistic Modeling: Bayesian inference, uncertainty estimation, graphical models.\nGenerative Models: GANs, VAEs, diffusion models.\nLarge Language Models: LLM scaling laws, prompting, fine-tuning, RAG architectures.\nVision: object detection, segmentation, multi-modal perception.\n\n\n\n3️⃣ System Design / Applied ML Problems\n\nBe able to discuss:\n\nEnd-to-end ML pipelines\nData challenges (imbalance, noisy labels, drift)\nModel serving and deployment challenges\nScalability, latency, interpretability\n\n\n\n\n4️⃣ Coding and Algorithmic Skills\n\nLeetcode-style DSA for research interviews (moderate level)\nData manipulation (pandas, numpy, SQL)\nModel prototyping (PyTorch, TensorFlow, JAX)\n\n\n\n5️⃣ Behavioral and Collaboration Skills\n\n“Tell me about a time…” questions.\nCollaboration across teams.\nHandling ambiguous open-ended research problems.\nCommunication with product teams or non-research stakeholders."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#recommended-preparation-resources",
    "href": "blog/technotes_20250703_research_guide/index.html#recommended-preparation-resources",
    "title": "Research Scientist Interview Guide",
    "section": "Recommended Preparation Resources",
    "text": "Recommended Preparation Resources\n\nPapers: Read papers from top-tier conferences (NeurIPS, ICML, ICLR, CVPR, ACL).\nCoding: Leetcode (medium), ML system design problems.\nSystem Design: Read “Designing Machine Learning Systems” by Chip Huyen.\nMock Interviews: Practice mock sessions with peers or mentors.\nPresentation: Prepare 1-2 strong 20-minute research talks."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#example-interview-questions",
    "href": "blog/technotes_20250703_research_guide/index.html#example-interview-questions",
    "title": "Research Scientist Interview Guide",
    "section": "Example Interview Questions",
    "text": "Example Interview Questions\n\nHow does your research contribute to state-of-the-art methods?\nWalk me through one of your recent papers.\nHow would you apply your methods to X domain?\nWhat challenges remain in your area of research?\nHow do you evaluate safety, robustness, or uncertainty in your models?\nHow would you adapt your methods if labeled data was extremely limited?"
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#my-personal-advice",
    "href": "blog/technotes_20250703_research_guide/index.html#my-personal-advice",
    "title": "Research Scientist Interview Guide",
    "section": "My Personal Advice",
    "text": "My Personal Advice\n\nClarity beats complexity — explain ideas simply.\nBe enthusiastic about your work and its impact.\nConnect your strengths to the job’s mission.\nShow your ability to collaborate and iterate."
  },
  {
    "objectID": "blog/technotes_20250703_research_guide/index.html#mentorship",
    "href": "blog/technotes_20250703_research_guide/index.html#mentorship",
    "title": "Research Scientist Interview Guide",
    "section": "Mentorship",
    "text": "Mentorship\nIf you’re preparing for Research Scientist interviews and would like advice or mentorship, feel free to reach out at cs.kundann@gmail.com."
  },
  {
    "objectID": "blog/blog_20230112_roche_opensource/index.html",
    "href": "blog/blog_20230112_roche_opensource/index.html",
    "title": "Open source reporting with R: clinical, public health, RSE and embrace the change",
    "section": "",
    "text": "Two days ago (Jan 11 2023) I watched a presentation by data scientists at Roche about why they are making their clinical trials in 2023 open source with R. As someone who uses R for most of the time and has done similar works (not in pharma, but in public health surveillance and reporting: watch my talk, slides to find out what we do), I watched the presentation with great interest. Here are my notes, combined with some thoughts on open-source in the industry, public sector and academia.\n\nThree reasons for why I am writing this blog\n\nNote down some of the technology which points towards the future of the field\nRelate to my experience of open-source applied in public health, specifically public health reporting\nShare some thoughts in statistical education of applied students/researchers (e.g. medicine), and training Research Software Engineers\n\n\n\nMy experience with statistical software\nTo put my opinions in perspective,\n\nI do not have experience with SAS or pharma, so I do not have first-hand knowledge on the functionality, ease-of-use or the popularity of commercial softwares in the industry.\nI did my MSc and PhD in statistics/biostatistics/medical informatics and R had always been a default choice.\nI worked in public health for a few years, where Excel is possibly the most common tool, and STATA and R are scarcely used (statisticians, epidemiologists, bioinformaticians).\nIn the past few years, my university has made the switch from SPSS to STATA for intro statistics for medical students (while students at higher level, or doing advanced analyses might use R/python), and a test-run with R might be in motion.\n\n\n\n\nClinical Reporting\nIn drug development at pharmaceutical companies (and/or research institutes and hospitals), these data related tasks are very common:\n\nsummarise safety and efficacy data\nprovide accurate picture of trial outcomes\nmanage data collection across different sites\n\nCompleting these tasks in a correct, efficient and reproducible manner is crucial for patient safety. However, these tasks are also highly resource intensive: highly trained scientist, statisticians and technincians must be involved in the process. Historically, pharma use commercial software such as SAS.\n\nRegulation and exploration needs\nThere are requirements for clinical reporting: both regulartory and exploratory. From the regulatory side, there exist industry standards (CDISC) in the clinical research process, such as SDTM (Model for Tabulation of Study Data) and ADaM (Analysis Data Model). Statistical analyses, tables, listings and graphs (TLGs) also fall into this cateogory.\nFrom the exploratory side, clinical data are highly context dependent, and new formats of data such as imaging are more and more used in prediction modeling and drug development.\nIn addition, it is not hard to imagine that the technical competency of employees differ, especially in large organizations. Enabling people with less experience to analyse trial data in a reproducible manner is helpful for not only the learning and growth of employees, but also the productivity of organizations.\nThe existing commercial tools are not able to adapt to the rapid changes in the field.\n\n\nTransition into Open-Source\nIn this talk, Dr Kieran Martin at Roche introduced that they started using R as their core data science tool, aiming to move their codebase to having a core R. In the future, they plan to have something that is lanugage agnostic: meaning that python, Stan, C++, Julia and beyond can be used for different tasks.\nI only noted down a few of the things they mentioned on the infrastructure side:\n\nOCEAN - a lanugage agnostic computing platform on AWS (docker)\nGit, Gitlab for version control and collaboration\nRstudio connect server\nSnakemake for orchestrate production\n\n\n\nR and Shiny\nThere are obvious benefits of using R. It is convenient to install and use (if you used python and R, you’d probably agree), and the latest development in Shiny made it very easy to develop interactive visualizations, suitable for exploration. Package development is critical for reproducibility and distributing works - which R does it very well. A few packages developed by pharma are Teal and admiral: the ADaM in R, which I intend to check out at one point.\nR has deep roots in academia which means the newest statistical methods are well covered; which also affects the skill sets that talents own - fresh graduates probably already learned it at university. R being open source means that collaboration with external partners is much more efficient, and transparent. Strong community support is another positive thing that encourages beginners to enter the field and learn.\n\n\n\n\nOpen Sourcing Public Health\n\nSurveillance and reporting\nOne key functionality of public health (PH) authorities is stay informed and inform. They collect data from labs, hospitals and clinics across the country, summarize into useful statistics in tables and graphs, make reports, then inform the policy makers to make decisions (such as vaccination campaigns).\nCompared to clinical reporting (in my understanding), there are many similarities - we make TLG (tables, listings and graphs). There are also features that make reporting in public health unique:\n\nPH surveillance and reporting are dynamic and real-time, which can change in a matter of days. That is because the situation of different infectious diseases can evolve rapidly, so PH authorities need to make appropriate adjustments.\nTime and location (spatial-temporal) are important. Different time granularity (daily, weekly) and geographical units (nation, county, municipality, city districts) are typically required for reporting.\n\n\n\nScale up and automate with open source tools\nTraditionally, these reports are made manually - one location, one graph per time on a certain disease. When a global pandemic hits, this is definitely not fast enough. At my team (Sykdomspulsen team at the Norwegian Institute of Public Health), we tried a different approach. Details of what we did can be found in this talk(slides), but to make it brief:\n\nWe developed a fully automated pipeline that connects 15 registries (vaccination, lab, hospital and intensive care and many more). The data is gathered, censored, cleaned and pre-processed for down-stream analysis\nStatistical analysis, tables, graphs and maps are made for all locations in Norway for various outcomes of interest, such as Covid, influenza, respiratory and gastrointestinal infections\nOver 1000 customized reports with over 30 graphs and tables are produced daily and sent to local PH officials, where we also had a shiny website (Kommunehelsetjenester for Kommunelege) for over 300 PH officials to get most up-to-date information about their own municipality\n\nBy automation, every year Sykdomspulsen can save 700 000 NOK (roughly 70k USD) while making 400 times more real-time reports for public health. Even better, with reproducibility and quality control.\n\n\nToolbox\nSykdomspulsen is a small team (8 people, 3 are statisticians and 1 engineer), and our infrastructure was built upon R packages, which we call splverse. Our infrastructure is not fundamentally different from the one Roche introduced, basically:\n\nR does the task planning and project organization. On top of this, the data cleaning, statistical analysis are implemented. Graphs, tables and maps are made with appropriate R packages\nRmarkdown does automated reporting into .docx and .xlsx. Some reports are also in .html tables to be embedded into customized emails\nRstudio Workbench and GitHub help with teamwork\n\nDocker, GoCD and Airflow do the CI/CD and orchestration\n\n\n\n\nEmbrace the transition\n\nCulture change needed\nUnfortunately, not all organizations are eager to abandon the old way. Even at our own institute where researchers are the majority, open source and modern day programming is hardly practiced (by my observation). Even worse, under the budget cuts in 2023-24, a large number of younger employees who have the technical skills have left - which left the public health surveillance even more vulnerable now that Covid is far from over.\nIn my opinion, public health needs open-source and good programming even more than pharmaceutical companies. Both save lifes - and PH has less money to invest in softwares, infrastructures and talents. In this situation, resources should be spent in fields that are critical and most cost-effective; yet in reality this is often not the case.\nThe slow culture change at big organizations can happen, but only if there is a sufficient amount of employees who are willing to embrace the new technology. In the talk by Roche they about about their training strategy. It is not possible to train all users, and not everyone has the same needs at the same time. Therefore, self study with certain study paths is encouraged and supported.\n\n\nTeach programming to students in various fields\nBased on my experience in the UK and Norway, students (myself included) learn R programming in one of the two ways\n\nLearning by Googling (self-taught): a university degree needs to use it: provides a short introduction, then students learn by using. This is how I learned R at my MSc Statistics degree, and this is probably the most common way\nWorkshops at university: organizations such as the Carpentries provide course material and teaching a few times per year, where interested students (usually from subjects such as biology) come and learn. These classes are quite popular, and usually have a long waiting list.\n\nFrom learning by googling to some organized teaching - that is already some good progress. However, if not, can we improve?\nIn my experience with statistical advising with the university hospital, clinical researchers and medical students are enthusastic to get their statistics done, some are also eager to do some analysis themselves. That is good. Yet, there is generally lack of capacity - either knowledge or software skills. Once the statistician who helps with the project stops, the project ends. There is the need to have in-house statistical capacity. To this end, open-source softwares such as R, and good programming practice (reproducibility for example) can help a lot: the license doesn’t end, and everything is documented so that the next person can continue the work.\nI’m glad that my university has made some transitional efforts in this regard: STATA instead of SPSS is being taught to medical students as part of their statistics course. There might be a test-run in R soon, which is very exciting (since I’ll be involved in the teaching)!\n\n\nStatistical engineering and RSEs\nThat was the capacity building to get beginners more independent. On the other side, there is also the need for better programming practice for researchers at more advanced level. Research Software Engineering (RSE) is starting to get more and more attention, because it is not only relevant for research (i.e. getting papers published), but in broader applications.\nFor example, in the talk by Roche, they mentioned that “RSE teams need to accelerate adoption of new statistical methods and biomarker data analysis”, and the implementation with R packages and templates is at its core. In the future more languages would be included such as Python, Stan, C++ and Julia.\nHowever, RSE as a job title or career path is still a new thing. I know two RSEs at my university, and RSE is definitely not your typical academic faculty position: only departments that think it’s important makes positions, often not permanent. To get any new methods actually used in either industry or the public sector outside research, translating methods into tools is must-do. In the future I hope RSE becomes a stable and common career path, and more exciting things can happen."
  },
  {
    "objectID": "blog/technotes_20230519_pkgcran/index.html",
    "href": "blog/technotes_20230519_pkgcran/index.html",
    "title": "R package workflow",
    "section": "",
    "text": "This checklist is being updated over time. Mostly for my own use; but great if it helps you as well!\nFor a complete treatment, please refer to R Packages (2e) by Hadley Wickham and Jennifer Bryan."
  },
  {
    "objectID": "blog/technotes_20230519_pkgcran/index.html#initialize-the-project",
    "href": "blog/technotes_20230519_pkgcran/index.html#initialize-the-project",
    "title": "R package workflow",
    "section": "Initialize the project",
    "text": "Initialize the project\n\nusethis::create_package('path_to_pkg/pkgname') \n\nIt opens a new R project (directory) named pkgname, with the following items:\n\nDESCRIPTION\nNAMESPACE\ndirectory R/\n.Rbuildignore and .gitignore\nand the project icon, pkgname.Rproj.\n\nIf you have an existing R project but wish to build a package there, copy everything but pkgname.Rproj, and modify the files in your existing pkg directory. Pay extra attention to the hidden files like .Rbuildignore.\n\nusethis::use_mit_license() # modify name to yours\nusethis::use_readme_md() # if you do not have this already\nusethis::use_news_md()\nusethis::use_test()\n\n# create a folder for future data documentation\nx &lt;- 1 \nusethis::use_data() \n\nIn addition, URL and bug reports should be added in the DESCRIPTION."
  },
  {
    "objectID": "blog/technotes_20230519_pkgcran/index.html#planning",
    "href": "blog/technotes_20230519_pkgcran/index.html#planning",
    "title": "R package workflow",
    "section": "Planning",
    "text": "Planning\nIt is good practice to start with planning the package, rather than directly start coding.\nCreate a folder called dev. To prevent it from being built, add the following line in .Rbuildignore"
  },
  {
    "objectID": "blog/technotes_20230519_pkgcran/index.html#write-test-and-document",
    "href": "blog/technotes_20230519_pkgcran/index.html#write-test-and-document",
    "title": "R package workflow",
    "section": "Write, test and document",
    "text": "Write, test and document\nCreate exported functions in R/, development code in script/ (or somewhere else, such as dev/).\n\nData: raw and processed\nNeed to be clear in mind where the data files go. There are a few data related folders:\n\nraw data files, in the format of excel sheets or csv. Usually placed as inst/data_name.csv\nR scripts to process the raw data so that we create data object inside the package, put inside data-raw\ndata objects that can be called as pkg::data_name, are placed in data. These files are usually directly generated by executing write.rda().\ndata documentation, usually placed in R/data_documentation.R. These are Roxygen2 documents for the data.\n\n\n\nDocumentation\nYou need to configure the Build tools.\nThese three things should be done:\n\nFunction documentation\nCreate a function f1, and put your cursor on it. Go to Code -&gt; Insert Roxygen Skeleton to create the template.\nAlternatively, use #' to start.\n\n#' A simple placehold function \n#'\n#' @param x a numeric value\n#'\n#' @return a value 3 greater than the input\n#' @export\n#'\n#' @examples \n#' f1(5)\nf1 &lt;- function(x){\n  x+3\n}\n\n\n\nData documentation\nIt can be beneficial to create a separate file to document data only, say data_documentation.R under the R/ directory.\n\n#' Placeholder data x\n#'\n#' This dataset contains one value, x\n#'\n#' @format\n#' \\describe{\n#' \\item{x}{The placeholder data x}\n#' }\n#' @examples\n#' print(x)\n\"x\"\n\n\n\nVignette documentation\n\nusethis::use_vignette('your_vignette')\n\n\n\nDeploy to pkgdown\nCheck this reference here"
  },
  {
    "objectID": "blog/technotes_20230519_pkgcran/index.html#build-package-and-check",
    "href": "blog/technotes_20230519_pkgcran/index.html#build-package-and-check",
    "title": "R package workflow",
    "section": "Build package and check",
    "text": "Build package and check\nIt is possible that your checks don’t pass on the first try.\n\nWhat to ignore when build?\n^.*\\.Rproj$\n^\\.Rproj\\.user$\n^dev$\n^_pkgdown\\.yml$\n^license\\.md$\nMakefile\ndata-raw\ncran-comments.md\n^\\.github$"
  },
  {
    "objectID": "talks/rstats_20230721_teaching/index.html",
    "href": "talks/rstats_20230721_teaching/index.html",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "",
    "text": "Time and place: July 21, 2023 10AM. Roche office, Basel, Switzerland\nSlides for this talk can be accessed here."
  },
  {
    "objectID": "talks/rstats_20230721_teaching/index.html#about-the-topic",
    "href": "talks/rstats_20230721_teaching/index.html#about-the-topic",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "About the topic",
    "text": "About the topic\nThe 8 day introductory statistics course (MF9130) at the Faculty of Medicine, University of Oslo is designed for PhD students in medicine, biology, psychology and other health related fields. Similar to other conventional teaching methods, the course has been focusing largely on theory and hand calculation. The software has been Stata and SPSS, and data analysis was mostly left for the students to figure out on their own.\nThis year, we made an attempt to transform the course with R, and aimed to teach more practical data analysis skills. We added one session per day where the instructor guide students on R and project management, importing data , basic manipulation and statistical methods. The IT skills of the students vary greatly, and therefore we used the ‘sticky notes’ help system borrowed from the Carpentries to make sure everyone could get help in the first days. We have created a course website using Quarto, where all the material and R exercises (with rendered solution) are available for self-study. We have witnessed amazing progress - by the end of the first week, students with the least computer / data skills were able to work on dataframes, make basic plots and do a chi-squared test. This helps build students confidence in data and statistics, and as a result, they can start to work on their own datasets using the skills immediately."
  },
  {
    "objectID": "talks/rstats_20190402_blogdown/index.html",
    "href": "talks/rstats_20190402_blogdown/index.html",
    "title": "Building Website in R: Step by Step Introduction to blogdown",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "talks/community_20240921_quartofriends/index.html",
    "href": "talks/community_20240921_quartofriends/index.html",
    "title": "Use Quarto, Make Friends",
    "section": "",
    "text": "It has been two years since Quarto became the most popular reproducible publication tool in data science and R community. However Quarto is so much more than just a publication tool! I started using it since late 2022, and it has helped me become more organized, productive and connected with people in the data science community.\nIn this talk I will not focus on the technical aspects on ‘how’ to use this tool. In the first part of the talk, I would like to report the latest news and trends seen in the useR conference and Posit conf, the two biggest global R events. In the second part, I will share my own experience in using Quarto for my career: from learning new skills, collaborating with co-workers, teaching university courses to networking and building a community (CAMIS collaboration). It is a powerful tool to share your work, and make new connections - both for work and for fun! I hope this talk will provide you with some new ideas on how to use this fantastic technology to fulfill your goals."
  },
  {
    "objectID": "talks/ehr_20210218_biday/index.html",
    "href": "talks/ehr_20210218_biday/index.html",
    "title": "Network Analysis of Hospital EHR data",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "talks/talks.html",
    "href": "talks/talks.html",
    "title": "Talks",
    "section": "",
    "text": "Explore Real-World hospital Electronic Health Records data with ggehr"
  },
  {
    "objectID": "talks/talks.html#upcoming",
    "href": "talks/talks.html#upcoming",
    "title": "Talks",
    "section": "",
    "text": "Explore Real-World hospital Electronic Health Records data with ggehr"
  },
  {
    "objectID": "talks/talks.html#selected-previous-talks",
    "href": "talks/talks.html#selected-previous-talks",
    "title": "Talks",
    "section": "Selected previous talks",
    "text": "Selected previous talks\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nDescription\n\n\nDate\n\n\n\n\n\n\nUse Quarto, Make Friends\n\n\nKolkata UseR meetup\n\n\n2024-09-21\n\n\n\n\nOne step closer to better Electronic Health Records data\n\n\nPHUSE Single Day Event Basel\n\n\n2024-09-18\n\n\n\n\nCAMIS: An Open-Source, Community endeavour for Comparing Analysis Method Implementations\n\n\nUseR! 2024 Salzburg\n\n\n2024-07-10\n\n\n\n\nA one year recap on teaching statistcis to medical students: how can R and Quarto help?\n\n\nR/Medicine 2024 - Online\n\n\n2024-06-13\n\n\n\n\nTransforming medical statistics classroom with R and Quarto\n\n\nBasel R meeting\n\n\n2023-07-21\n\n\n\n\nSykdomspulsen: An automated public health surveillance platform\n\n\nOslo UseR meetup \n\n\n2022-06-16\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/ph_20220616_splverse/index.html",
    "href": "talks/ph_20220616_splverse/index.html",
    "title": "Sykdomspulsen: An automated public health surveillance platform",
    "section": "",
    "text": "About the talk\nWatch the talk on YouTube\nSykdomspulsen is a real-time analysis and disease surveillance system designed at developed at the Norwegian Institute of Public Health (FHI). Sykdomspulsen processes new data collected from 15 data sources (e.g., covid-19 cases), runs 1000.000+ statistical analysis automatically for all locations (nation, county, municipality) in Norway, produces 1000+ reports and alerts for public health authorities and shares data to the public on GitHub.\nSykdomspulsen runs on a collection of R packages, the {splverse}. {splverse} is an ecosystem for infectious disease surveillance, from analysis planning, statistical analysis to reporting via visualization, shiny website and Rmarkdown generated reports. In this talk, Chi will present how Sykdomspulsen does public health real-time surveillance during the pandemic using R. Chi will introduce some of the core packages and illustrate how they work together, with an example using real surveillance data published daily on GitHub.\n\n\nAbout the speaker\nChi is currently working at the Sykdomspulsen team as a researcher and R developer, at the Norwegian Institute of Public Health. Before she joined Sykdomspulsen in the middle of the pandemic (2020), she was a PhD student at the Department of Biostatistics at University of Oslo (OCBE), working on hospital EHR data."
  },
  {
    "objectID": "projects/cv.html",
    "href": "projects/cv.html",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package ggehr to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\nggplot2 extension to EHR data\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “Deep Reinforcement Learning.” https://kundan-kumarr.github.io/projects/cv.html."
  },
  {
    "objectID": "projects/phuse/index.html",
    "href": "projects/phuse/index.html",
    "title": "PHUSE - CAMIS",
    "section": "",
    "text": "I am contributing to two working groups at PHUSE: CAMIS, and RWD - Real World Data Guideline (early stage).\n\nCAMIS: Comparing Analysis Method Implementations in Software\nCAMIS is a cross-industry group formed of members from PHUSE, PSI and ASA.\nSeveral discrepancies have been discovered in statistical analysis results between different programming languages, even in fully qualified statistical computing environments. Observing differences across languages can reduce the analyst’s confidence in reliability and, by understanding the source of any discrepancies, one can reinstate confidence in reliability.\nThe goal of this project is to demystify conflict when doing QC and to help ease the transitions to new languages and techniques with comparison and comprehensive explanations.\n\n\nRWD Working Group\nThis is a newly formed working group, working on statistical programming guidelines while working on RWD (read world data)."
  },
  {
    "objectID": "projects/sykdomspulsen/index.html",
    "href": "projects/sykdomspulsen/index.html",
    "title": "CSIDS",
    "section": "",
    "text": "CSIDS: the Consortium for Statistics in Disease Surveillance (previously Sykdomspulsen) is a real-time analysis and disease surveillance system designed and developed at the Norwegian Institute of Public Health (NIPH/FHI). It is a unique project that processes new data (e.g. covid-19 cases) shortly after it is available. Complex statistical analyses are automatically run for all locations in Norway, producing reports and alerting various stakeholders. This provides the health authorities the ability to make proactive strategic decisions with the most up-to-date information.\n\n\ncsverse is a number of R packages that have been developed by the Sykdomspulsen team to help with infectious diseases surveillance.\n\n\n\nIn November 2022, the core components of Sykdomspulsen and splverse R packages have been migrated to CSIDS: the Consortium for Statistics in Disease Surveillance. Please refer to CSIDS for ongoing developments.\n\n\n\n\nWe receive data from more than 15 data sources every day\n2 000 000 000+ rows of data and results (1TB)\n1 000+ database tables\n1 000 000+ analyses per day\n1 000+ automatic reports per day\n\n\n\nDownload poster (Norwegian)"
  },
  {
    "objectID": "projects/sykdomspulsen/index.html#overview",
    "href": "projects/sykdomspulsen/index.html#overview",
    "title": "CSIDS",
    "section": "",
    "text": "CSIDS: the Consortium for Statistics in Disease Surveillance (previously Sykdomspulsen) is a real-time analysis and disease surveillance system designed and developed at the Norwegian Institute of Public Health (NIPH/FHI). It is a unique project that processes new data (e.g. covid-19 cases) shortly after it is available. Complex statistical analyses are automatically run for all locations in Norway, producing reports and alerting various stakeholders. This provides the health authorities the ability to make proactive strategic decisions with the most up-to-date information.\n\n\ncsverse is a number of R packages that have been developed by the Sykdomspulsen team to help with infectious diseases surveillance.\n\n\n\nIn November 2022, the core components of Sykdomspulsen and splverse R packages have been migrated to CSIDS: the Consortium for Statistics in Disease Surveillance. Please refer to CSIDS for ongoing developments.\n\n\n\n\nWe receive data from more than 15 data sources every day\n2 000 000 000+ rows of data and results (1TB)\n1 000+ database tables\n1 000 000+ analyses per day\n1 000+ automatic reports per day\n\n\n\nDownload poster (Norwegian)"
  },
  {
    "objectID": "projects/projects.html",
    "href": "projects/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package ggehr to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\nggplot2 extension to EHR data\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “Projects.” https://kundan-kumarr.github.io/projects/projects.html."
  },
  {
    "objectID": "projects/robo.html",
    "href": "projects/robo.html",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package ggehr to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\nggplot2 extension to EHR data\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/os_teaching/index.html",
    "href": "projects/os_teaching/index.html",
    "title": "Teach in R and Quarto",
    "section": "",
    "text": "MF9130E - Introductory course in statistics\n8-day intensive course on introductory statistics. April 2023 we made it with R rather than propriety software, coupled with live-coding sessions to enhance understanding of basic concepts such as distribution and hypothesis tests.\nRepository\nCourse website\nRead more about the experience in\n\nblogpost\npresentation"
  },
  {
    "objectID": "projects/drl.html",
    "href": "projects/drl.html",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package ggehr to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\nggplot2 extension to EHR data\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "misc/literature.html",
    "href": "misc/literature.html",
    "title": "Hello, I'm Kundan",
    "section": "",
    "text": "antibiotics stewardship\ninfections (community, hospital)\nelectronic health records\nquality assurance: prescripton and use"
  },
  {
    "objectID": "misc/literature.html#ab-x-ehr",
    "href": "misc/literature.html#ab-x-ehr",
    "title": "Hello, I'm Kundan",
    "section": "AB x EHR",
    "text": "AB x EHR\nMoehring 2021, EHR identify AB use among hospitalized patients\nReenggli 2021: assess EHR conversion into AB stewardship indicators\nCairns 2021: integrate AB stewardship with EHR in australia\nJenkins 2022: AB stewardship using electornic prescribing systems, review of intervention and outcome measures\nKuijpers 2024, excessive length of AB duration for HAI, support AB stewardship high relevance"
  },
  {
    "objectID": "misc/literature.html#ehr-doctor-experience-system-design",
    "href": "misc/literature.html#ehr-doctor-experience-system-design",
    "title": "Hello, I'm Kundan",
    "section": "EHR doctor experience, system design",
    "text": "EHR doctor experience, system design\nOstrer 2023, real time benefit tools must be designed to serve both clinicians and patients. mentions burden\nKawamoto 2019 Association of EHR add-on app for neonatal bilirubin management, physician efficiency and care quality. Good system and add-on can save clinician time and improve patient care.\nTsai 2020, EHR implementation, barriers to adoption and use high relevance\nOverhage 2020, time spent using EHR\n\nburnout\nTajirian 2020, influence of EHR on physician burnout high relevance\nKhairat 2020, EHR use with physician fatigue and efficiency\nKroth 2019, factors of EHR design and use for physician stress and burnout high relevance\nMelnick 2020, EHR usability, task load and burnout high relevance\nHilliard 2020, factors associated with burnout\nMore on this topic\nhttps://www.sciencedirect.com/science/article/abs/pii/S0897189718301356\nhttps://journals.sagepub.com/doi/full/10.1177/20552076231220241\nhttps://jamanetwork.com/journals/jamanetworkopen/article-abstract/2778909\n\n\nDesign\nGascon 2013 the process of designing a EHR lab request module\nTorres 2017, effect of EHR design on documentation and compliance high relevance"
  },
  {
    "objectID": "misc/literature.html#ehr-data-quality-use-for-prediction-model-development",
    "href": "misc/literature.html#ehr-data-quality-use-for-prediction-model-development",
    "title": "Hello, I'm Kundan",
    "section": "EHR data quality, use for prediction model development",
    "text": "EHR data quality, use for prediction model development\n\nerror\nbell 2020\nhttps://www.sciencedirect.com/science/article/abs/pii/S2213076420300439\nLi 2024, comparison of error reporting systems. 82% undetected, 90% had no corresponding incident report, some errors not reported at all\nWestbrook 2020, changes in medication administration error rates, associated with EHR\nGildon 2019, impact of EHR on prescribing errors in pediatric clinics. introduction is relevant for pointing out the need for well designed system\nGinzburg 2018, use clinical decision support within EHR to reduce incorrect prescribing for acute sinusitis. shows prompt for physicians to fill in reason for why AB is required high relevance\n\n\nbias\nGianfrancesco 2018, potential biases in ML algorithms using EHR data. Existing EHR disparities should not be amplified by thoughtless or excess reliance on machines.\nAgniel 2018, biases in EHR due to process within the healthcare system. healthcare processes must be addressed in the analysis of observational health data, context is important. high relevance\nBower, biases in EHR based surveillance of cardiovascular disease risk\nHarding2024, addressing common sources of bias in type 2 diabetes following covid, using EHR\nBica 2020, current and future methods to address underlying challenges from EHR to treatment effects mid relevance to manuscript, but important to myself\nDesai 2020, prediction using admin EHR, hear failure"
  },
  {
    "objectID": "misc/literature.html#ab-stewardship-not-so-relevant",
    "href": "misc/literature.html#ab-stewardship-not-so-relevant",
    "title": "Hello, I'm Kundan",
    "section": "AB stewardship (not so relevant)",
    "text": "AB stewardship (not so relevant)\nRenggli 2021: consumption of anti-meticillin resistant staphylococcus aureus antibiotics in Swiss hospitals is associateed with antibiotic stewardship measures\nVaughn 2021: AB overuse and stewardship at hospital discharge\nGraber 2019: electronic tools to decrease AB use"
  },
  {
    "objectID": "guide/guide.html",
    "href": "guide/guide.html",
    "title": "Research Scientist Interview Guide",
    "section": "",
    "text": "This guide provides a comprehensive framework to prepare for Research Scientist positions in academia, industry research labs (FAANG, OpenAI, DeepMind, etc.), and national labs. It combines insights from my own experience, interviews, and conversations with hiring managers."
  },
  {
    "objectID": "guide/guide.html#overview",
    "href": "guide/guide.html#overview",
    "title": "Research Scientist Interview Guide",
    "section": "",
    "text": "This guide provides a comprehensive framework to prepare for Research Scientist positions in academia, industry research labs (FAANG, OpenAI, DeepMind, etc.), and national labs. It combines insights from my own experience, interviews, and conversations with hiring managers."
  },
  {
    "objectID": "guide/guide.html#key-interview-components",
    "href": "guide/guide.html#key-interview-components",
    "title": "Research Scientist Interview Guide",
    "section": "Key Interview Components",
    "text": "Key Interview Components\n\n1️⃣ Research Portfolio Deep Dive\n\nBe able to explain your core research contributions in detail.\nClearly articulate: problem definition, novelty, methods, results, and real-world impact.\nPrepare multiple levels of technical depth (5-min, 15-min, 30-min versions).\nPractice connecting your work to broader research trends and applications.\n\n\n\n2️⃣ Technical Machine Learning Knowledge\n\nReinforcement Learning: algorithms, policy gradients, actor-critic, safe RL.\nDeep Learning: optimization, architecture design, generalization, transformers.\nProbabilistic Modeling: Bayesian inference, uncertainty estimation, graphical models.\nGenerative Models: GANs, VAEs, diffusion models.\nLarge Language Models: LLM scaling laws, prompting, fine-tuning, RAG architectures.\nVision: object detection, segmentation, multi-modal perception.\n\n\n\n3️⃣ System Design / Applied ML Problems\n\nBe able to discuss:\n\nEnd-to-end ML pipelines\nData challenges (imbalance, noisy labels, drift)\nModel serving and deployment challenges\nScalability, latency, interpretability\n\n\n\n\n4️⃣ Coding and Algorithmic Skills\n\nLeetcode-style DSA for research interviews (moderate level)\nData manipulation (pandas, numpy, SQL)\nModel prototyping (PyTorch, TensorFlow, JAX)\n\n\n\n5️⃣ Behavioral and Collaboration Skills\n\n“Tell me about a time…” questions.\nCollaboration across teams.\nHandling ambiguous open-ended research problems.\nCommunication with product teams or non-research stakeholders."
  },
  {
    "objectID": "guide/guide.html#recommended-preparation-resources",
    "href": "guide/guide.html#recommended-preparation-resources",
    "title": "Research Scientist Interview Guide",
    "section": "Recommended Preparation Resources",
    "text": "Recommended Preparation Resources\n\nPapers: Read papers from top-tier conferences (NeurIPS, ICML, ICLR, CVPR, ACL).\nCoding: Leetcode (medium), ML system design problems.\nSystem Design: Read “Designing Machine Learning Systems” by Chip Huyen.\nMock Interviews: Practice mock sessions with peers or mentors.\nPresentation: Prepare 1-2 strong 20-minute research talks."
  },
  {
    "objectID": "guide/guide.html#example-interview-questions",
    "href": "guide/guide.html#example-interview-questions",
    "title": "Research Scientist Interview Guide",
    "section": "Example Interview Questions",
    "text": "Example Interview Questions\n\nHow does your research contribute to state-of-the-art methods?\nWalk me through one of your recent papers.\nHow would you apply your methods to X domain?\nWhat challenges remain in your area of research?\nHow do you evaluate safety, robustness, or uncertainty in your models?\nHow would you adapt your methods if labeled data was extremely limited?"
  },
  {
    "objectID": "guide/guide.html#my-personal-advice",
    "href": "guide/guide.html#my-personal-advice",
    "title": "Research Scientist Interview Guide",
    "section": "My Personal Advice",
    "text": "My Personal Advice\n\nClarity beats complexity — explain ideas simply.\nBe enthusiastic about your work and its impact.\nConnect your strengths to the job’s mission.\nShow your ability to collaborate and iterate."
  },
  {
    "objectID": "guide/guide.html#mentorship",
    "href": "guide/guide.html#mentorship",
    "title": "Research Scientist Interview Guide",
    "section": "Mentorship",
    "text": "Mentorship\nIf you’re preparing for Research Scientist interviews and would like advice or mentorship, feel free to reach out at cs.kundann@gmail.com."
  },
  {
    "objectID": "guide/literature.html",
    "href": "guide/literature.html",
    "title": "Hello, I'm Kundan",
    "section": "",
    "text": "antibiotics stewardship\ninfections (community, hospital)\nelectronic health records\nquality assurance: prescripton and use"
  },
  {
    "objectID": "guide/literature.html#ab-x-ehr",
    "href": "guide/literature.html#ab-x-ehr",
    "title": "Hello, I'm Kundan",
    "section": "AB x EHR",
    "text": "AB x EHR\nMoehring 2021, EHR identify AB use among hospitalized patients\nReenggli 2021: assess EHR conversion into AB stewardship indicators\nCairns 2021: integrate AB stewardship with EHR in australia\nJenkins 2022: AB stewardship using electornic prescribing systems, review of intervention and outcome measures\nKuijpers 2024, excessive length of AB duration for HAI, support AB stewardship high relevance"
  },
  {
    "objectID": "guide/literature.html#ehr-doctor-experience-system-design",
    "href": "guide/literature.html#ehr-doctor-experience-system-design",
    "title": "Hello, I'm Kundan",
    "section": "EHR doctor experience, system design",
    "text": "EHR doctor experience, system design\nOstrer 2023, real time benefit tools must be designed to serve both clinicians and patients. mentions burden\nKawamoto 2019 Association of EHR add-on app for neonatal bilirubin management, physician efficiency and care quality. Good system and add-on can save clinician time and improve patient care.\nTsai 2020, EHR implementation, barriers to adoption and use high relevance\nOverhage 2020, time spent using EHR\n\nburnout\nTajirian 2020, influence of EHR on physician burnout high relevance\nKhairat 2020, EHR use with physician fatigue and efficiency\nKroth 2019, factors of EHR design and use for physician stress and burnout high relevance\nMelnick 2020, EHR usability, task load and burnout high relevance\nHilliard 2020, factors associated with burnout\nMore on this topic\nhttps://www.sciencedirect.com/science/article/abs/pii/S0897189718301356\nhttps://journals.sagepub.com/doi/full/10.1177/20552076231220241\nhttps://jamanetwork.com/journals/jamanetworkopen/article-abstract/2778909\n\n\nDesign\nGascon 2013 the process of designing a EHR lab request module\nTorres 2017, effect of EHR design on documentation and compliance high relevance"
  },
  {
    "objectID": "guide/literature.html#ehr-data-quality-use-for-prediction-model-development",
    "href": "guide/literature.html#ehr-data-quality-use-for-prediction-model-development",
    "title": "Hello, I'm Kundan",
    "section": "EHR data quality, use for prediction model development",
    "text": "EHR data quality, use for prediction model development\n\nerror\nbell 2020\nhttps://www.sciencedirect.com/science/article/abs/pii/S2213076420300439\nLi 2024, comparison of error reporting systems. 82% undetected, 90% had no corresponding incident report, some errors not reported at all\nWestbrook 2020, changes in medication administration error rates, associated with EHR\nGildon 2019, impact of EHR on prescribing errors in pediatric clinics. introduction is relevant for pointing out the need for well designed system\nGinzburg 2018, use clinical decision support within EHR to reduce incorrect prescribing for acute sinusitis. shows prompt for physicians to fill in reason for why AB is required high relevance\n\n\nbias\nGianfrancesco 2018, potential biases in ML algorithms using EHR data. Existing EHR disparities should not be amplified by thoughtless or excess reliance on machines.\nAgniel 2018, biases in EHR due to process within the healthcare system. healthcare processes must be addressed in the analysis of observational health data, context is important. high relevance\nBower, biases in EHR based surveillance of cardiovascular disease risk\nHarding2024, addressing common sources of bias in type 2 diabetes following covid, using EHR\nBica 2020, current and future methods to address underlying challenges from EHR to treatment effects mid relevance to manuscript, but important to myself\nDesai 2020, prediction using admin EHR, hear failure"
  },
  {
    "objectID": "guide/literature.html#ab-stewardship-not-so-relevant",
    "href": "guide/literature.html#ab-stewardship-not-so-relevant",
    "title": "Hello, I'm Kundan",
    "section": "AB stewardship (not so relevant)",
    "text": "AB stewardship (not so relevant)\nRenggli 2021: consumption of anti-meticillin resistant staphylococcus aureus antibiotics in Swiss hospitals is associateed with antibiotic stewardship measures\nVaughn 2021: AB overuse and stewardship at hospital discharge\nGraber 2019: electronic tools to decrease AB use"
  },
  {
    "objectID": "about_me.html",
    "href": "about_me.html",
    "title": "Kundan Kumar",
    "section": "",
    "text": "Gmail\n  \n  \n    \n     Github\n  \n  \n    \n     Linkedin\n  \n  \n    \n     YouTube\n  \n  \n    \n     Substack\n  \n  \n    \n     Scholar"
  },
  {
    "objectID": "projects/nor_mortality/index.html",
    "href": "projects/nor_mortality/index.html",
    "title": "Mortality Surveillance in Norway",
    "section": "",
    "text": "NorMOMO"
  },
  {
    "objectID": "projects/nor_mortality/index.html#norwegian-surveillance-system-for-excess-mortality",
    "href": "projects/nor_mortality/index.html#norwegian-surveillance-system-for-excess-mortality",
    "title": "Mortality Surveillance in Norway",
    "section": "",
    "text": "NorMOMO"
  },
  {
    "objectID": "projects/nor_mortality/index.html#r-packages-for-mortality-surveillance",
    "href": "projects/nor_mortality/index.html#r-packages-for-mortality-surveillance",
    "title": "Mortality Surveillance in Norway",
    "section": "R packages for mortality surveillance",
    "text": "R packages for mortality surveillance\n(continue…)\n\nnowcast\nsplalert\nmortanor"
  },
  {
    "objectID": "projects/nor_mortality/index.html#collaboration-with-cause-of-death-registry",
    "href": "projects/nor_mortality/index.html#collaboration-with-cause-of-death-registry",
    "title": "Mortality Surveillance in Norway",
    "section": "Collaboration with Cause of Death Registry",
    "text": "Collaboration with Cause of Death Registry"
  },
  {
    "objectID": "projects/dl.html",
    "href": "projects/dl.html",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package ggehr to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\nggplot2 extension to EHR data\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/dan/index.html",
    "href": "projects/dan/index.html",
    "title": "Data Apothecary’s Notes",
    "section": "",
    "text": "(This is my own note-taking system using quarto)\n\nAbout the notes\nData Apothecary’s Notes is a note-taking repository for modern data science skills with a focus on drug development and clinical trials. Content will be gradually added while I learn the topics. Therefore, it is by no means a complete guide by the time you read it!\nI try to organize the content in a modular way. I think these should cover the important aspects in which a data scientist / modern statistician should know.\n\nstudy design\ninference\nmodels\nreporting\nprogramming\n\n\n\nWhy quarto\nIn short, quarto has the advantage of making a very well structured website with code chunks easy. No more worry about scattered notes in different folders - put them together, render it so you can find your notes quickly!"
  },
  {
    "objectID": "projects/noreden/index.html",
    "href": "projects/noreden/index.html",
    "title": "Noreden",
    "section": "",
    "text": "Noreden"
  },
  {
    "objectID": "projects/ehr/index.html",
    "href": "projects/ehr/index.html",
    "title": "ggehr",
    "section": "",
    "text": "ggehr (read: gg E-H-R) stands for ggplot2 extension for EHR data, which provides a set of tools to facilitate EHR (Electronic Health Records) visualization.\nggehr package helps you make visualize EHR data, so that you can\n\nhave an overview of the mixed type information related to a patient;\nvisually identify the errors in data recording.\n\nLearn more about ggehr\n\n\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “Ggehr.” https://kundan-kumarr.github.io/projects/ehr/."
  },
  {
    "objectID": "projects/stat.html",
    "href": "projects/stat.html",
    "title": "Statistics",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package ggehr to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\nggplot2 extension to EHR data\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoreden\n\n\nR tools to faciliate sustainable nutrition research\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/ehr_20221013_ml_icu/index.html",
    "href": "talks/ehr_20221013_ml_icu/index.html",
    "title": "Machine Learning in Intensive Care Units",
    "section": "",
    "text": "A 45 minutes trial lecture to fulfill the requirement of my PhD degree."
  },
  {
    "objectID": "talks/rstats_20240613_teaching/index.html",
    "href": "talks/rstats_20240613_teaching/index.html",
    "title": "A one year recap on teaching statistcis to medical students: how can R and Quarto help?",
    "section": "",
    "text": "Time and place: June 13 2024. Online\nSlides for this talk can be accessed here."
  },
  {
    "objectID": "talks/rstats_20240613_teaching/index.html#about-the-topic",
    "href": "talks/rstats_20240613_teaching/index.html#about-the-topic",
    "title": "A one year recap on teaching statistcis to medical students: how can R and Quarto help?",
    "section": "About the topic",
    "text": "About the topic\nThe Department of Biostatistics at University of Oslo offer statistics courses at different levels for medical students and PhD candidates with clinical backgrounds. The courses were traditionally taught with a focus on theory instead of data analysis, where SPSS and STATA were the tools of choice.\nSince 2023 spring semester, we have been gradually transforming some of our statistics courses into R, using Quarto course websites and Carpentries style live-coding instruction. With new Quarto tools (such as WebR) we also added interactivity in the code blocks. So far we have transformed two courses with over 100 students who have almost no programming experience. We have observed impressive progress in the skill development, and received significantly more positive feedback when it comes to statistics education.\nIn this talk, I would like to share our experience on the successes and challenges throughout the process. Looking back, is it cost-effective? Definitely. Can we do better in the future? Almost surely. If you are also planning to adopt new technology in your teaching activities, join us to learn more about what you can do to make the transition happen!\nCourse website can be accessed here"
  },
  {
    "objectID": "talks/community_20240710_camis/index.html",
    "href": "talks/community_20240710_camis/index.html",
    "title": "CAMIS: An Open-Source, Community endeavour for Comparing Analysis Method Implementations",
    "section": "",
    "text": "2024.7.8-11, Salzburg, Austria. Conference link: UseR!\nStatisticians using multiple softwares (SAS, R, Python) will have found differences in analysis results that warrant further justification. Whilst some industries may accept results not being the same as long as they are “close”, the highly regulated pharmaceutical industry would require an identical match in results. Yet, discrepancies might still occur, and knowing the reasons (different methods, options, algorithms etc) is critical to the modern statistician and subsequent regulatory submissions.\nIn this talk I will introduce CAMIS: Comparing Analysis Method Implementations in Software. https://psiaims.github.io/CAMIS/ It is a joint-project between PHUSE, the R Validation Hub, PSI AIMS, R consortium and openstatsware. The aim of CAMIS is to investigate and document differences and similarities between different statistical softwares such as SAS and R. We use Quarto and Github to document methods, algorithms and comparisons between softwares through small case studies, and all articles are contributed by the community. In the transition from proprietary to open source technology in the industry, CAMIS can serve as a guidebook to navigate this process.\n\nkeywords: cross industry collaboration, multi-lingua, open-source, quarto"
  },
  {
    "objectID": "talks/ehr_20240918_betterehr/index.html",
    "href": "talks/ehr_20240918_betterehr/index.html",
    "title": "One step closer to better Electronic Health Records data",
    "section": "",
    "text": "Real-World Data (RWD) like Electronic Health Records (EHR) is crucial for understanding drug usage and various treatments and generating Real-World Evidence (RWE). Risk prediction has been a major application where EHR is used, and there is now a shift towards causal inference, which requires data of even higher quality. Patients undergo treatments (drugs, procedures) at various times during their hospital stays, yet the data being recorded are messy and error-prone for various reasons. Analysts spend significant amount of time to sit together with clinicians to identify and understand abnormal records, and unfortunately this process is challenging to automate.\nThis talk will use an example on antibiotics prescription and use at a Nordic hospital to illustrate how some EHR systems can improve for better clinical decision-making and better data for research. I will also introduce a pilot R package (ggehr) that facilitates visual exploration of EHR data, and how it can help reconstruct patient journeys and enable analysts to perform effective quality control."
  },
  {
    "objectID": "talks/ph_20230330_sp/index.html",
    "href": "talks/ph_20230330_sp/index.html",
    "title": "Public health surveillance and reporting",
    "section": "",
    "text": "Time and place: Mar. 30, 2023 12:00 PM–1:00 PM\nHybrid: Georg Sverdrups hus and Zoom\nEvent page"
  },
  {
    "objectID": "talks/ph_20230330_sp/index.html#about-the-topic",
    "href": "talks/ph_20230330_sp/index.html#about-the-topic",
    "title": "Public health surveillance and reporting",
    "section": "About the topic",
    "text": "About the topic\nSituational awareness is key to fast response during a public health emergency, such as COVID-19 pandemic. However, making disease surveillance reports that cover different geographical units for various metrics and data registries is both resource intensive and time consuming. Open source tools such as R packages, GitHub and Airflow can make this process automatic, reproducible and scalable.\nEvery day during the pandemic, Sykdomspulsen team at the Norwegian Institute of Public Health (FHI/NIPH) fetched data from more than 15 data sources, cleaned, censored datasets and carried out a wide range of statistical analyses. Over 1000 situational reports containing automated graphs and tables were produced before breakfast time.\nGrab you matpakke and join us for a presentation from Chi Zhang about how Sykdomspulsen team used and developed open source software to make public health surveillance and reporting more efficient, followed up by a discussion on the benefits and concerns of making these data public. We will end with an open Q&A session as usual!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kundan Kumar",
    "section": "",
    "text": "Email\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     GitHub\n  \n  \n    \n     YouTube\n  \n  \n    \n     Substack\n  \n  \n    \n     Scholar\n  \n\n  \n  \nHello!\nI’m Kundan Kumar — a Ph.D. candidate and researcher building safe and trustworthy AI systems for real-world autonomous applications. My research integrates deep reinforcement learning (DRL), physics-informed AI, and safe control to create robust agents for smart energy systems, robotics, and autonomous vehicles.\nIn 2024, I worked as a Machine Learning Engineer Intern at the National Renewable Energy Laboratory (NREL), where I designed a Bayesian semi-supervised learning algorithm for phase identification on small, limited, and unreliable datasets. This work will be presented at the IEEE PES General Meeting 2025.\nI’m passionate about democratizing AI knowledge. I create educational content simplifying AI, ML, and statistics on Substack and YouTube.\nOutside of research, I enjoy cooking and skating. 🛼"
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "Kundan Kumar",
    "section": "Recent Posts",
    "text": "Recent Posts\nCheck out the latest  Papers ,  News ,  Events , and  More »\n\n\n\n\nAll Posts »"
  },
  {
    "objectID": "blog/blog_20230103_blogdown2quarto/index.html",
    "href": "blog/blog_20230103_blogdown2quarto/index.html",
    "title": "Website reboot: switching from Blogdown to Quarto",
    "section": "",
    "text": "Since the first time I tried the “academic” template in the popular blogdown package in 2019, three years have passed. Back then, it was THE way to build a personal website using R. The “academic” template was notoriously rich in content, and my solution was to delete components, compile, if it works - great; if not, I put the deleted content back. It worked for a while.\nWhen the distill package came out (probably in 2020?), I rebooted my website since I preferred its clean, minimalistic style. The look was possibly more appropriate for websites for an organisation or tutorials rather than personal blog, yet I appreciated the simplicity.\nThen I stopped updating my website. Between mid 2020 and early 2022, I was too stressed about completing my PhD, and balancing my other two jobs wasn’t the easiest thing. During this period, my mind had been going back to the old site from time to time, but it was hard to find enough time or energy to write about stuff."
  },
  {
    "objectID": "blog/blog_20230103_blogdown2quarto/index.html#from-blogdown-to-distill",
    "href": "blog/blog_20230103_blogdown2quarto/index.html#from-blogdown-to-distill",
    "title": "Website reboot: switching from Blogdown to Quarto",
    "section": "",
    "text": "Since the first time I tried the “academic” template in the popular blogdown package in 2019, three years have passed. Back then, it was THE way to build a personal website using R. The “academic” template was notoriously rich in content, and my solution was to delete components, compile, if it works - great; if not, I put the deleted content back. It worked for a while.\nWhen the distill package came out (probably in 2020?), I rebooted my website since I preferred its clean, minimalistic style. The look was possibly more appropriate for websites for an organisation or tutorials rather than personal blog, yet I appreciated the simplicity.\nThen I stopped updating my website. Between mid 2020 and early 2022, I was too stressed about completing my PhD, and balancing my other two jobs wasn’t the easiest thing. During this period, my mind had been going back to the old site from time to time, but it was hard to find enough time or energy to write about stuff."
  },
  {
    "objectID": "blog/blog_20230103_blogdown2quarto/index.html#time-to-try-quarto",
    "href": "blog/blog_20230103_blogdown2quarto/index.html#time-to-try-quarto",
    "title": "Website reboot: switching from Blogdown to Quarto",
    "section": "Time to try Quarto",
    "text": "Time to try Quarto\nNow that I’ve finally completed the more pressing tasks in October 2022, I can catch up to the cool kids on twitter: create a website with Quarto!\nThere were quite a lot of discussions about Quarto in the summer 2022. I wasn’t following the discussions closely, but I remember there were quite a few talks in the Rstudio conference this year. Then more and more people switched to Quarto on Twitter. Then people I know also switched to Quarto. What’s the fuzz about?\nMy experience with Quarto is focused on websites. I have not tried other forms of publishing. So far I have created:\n\na workshop website for my colleagues\na personal website (the one you are reading right now)\nan R package (qtwAcademic)that wraps three Quarto website templates for beginners\n\nHere are a few things I like about Quarto. Given that I’m not very experienced in front-end development, these comments are going to be about ease-of-use and design, rather than the technicalities.\n\nClean look for both personal and workshop/courses\nWhen I was using “academic” template in blogdown, I liked the structure of the site: projects, talks, blog, softwares and publications sections are clearly displayed at the top. What I didn’t like is that the default homepage was a very long single page; yet its customisation wasn’t the easist. Other templates were either too simple (for blog only), or more suitable for image display (photography projects). I wanted a website that keep the good structure of “academic”, which is quite suitable for academics (hence the name); while keeping each section independent.\nWith distill I could achieve the structure I wanted; but I didn’t enjoy it too much as a personal website (at least it wasn’t as flexible as Quarto). distill is still pretty decent for organisations or documentation site.\nWith Quarto, I can achieve the desired looks for not only a personal website (with or without blogs), but also a workshop, event or even course website. This is fantastic! The top, sidebar or hybrid navigation makes the site structure very clear, especially when there are lots of content. As an aspiring lecturer at university, this is really One Quarto Rules Them All.\n\n\nFlexible yet not overwhelming\nAs I mentioned above, hacking “academic” in blogdown was not that easy - simply because there were too many folders that you are not actually supposed to modify. It was confusing to know what to change in order to achieve the desired output, and multiple folders were having the same names, making it very challenging for beginners. Ironically, this is usually the first template beginners start with!\nThat’s why I immediately fell for Quarto: you only need 4 components to make a decent minimalistic website work:\n\n_quarto.yml to control the overall layout\nindex.qmd at the root folder to control the homepage\nabout.qmd for some basic information about the creator or the website\nproject.qmd for projects or any other content that the creator wants to display\n\nThe way that _quarto.yml clearly specifies the .qmd files really helps beginners to understand where things are. This has been extremely useful for me when I wanted to learn how people made their website by reading the source code - I could understand exactly where to find the information I needed. The clear structure greatly helps the creators themselves, and also those who want to learn.\n\n\nGreat community\nRstats people have a great community. I wouldn’t be able to make my site the way I wanted if people haven’t been sharing their works. I have learned a lot by reading the source code by Dr Emi Tanaka, Dr David Schoch, Bea Milz, Prof Mine Cetinnkaya-Rundel’s STA 210 - Regression Analysis course.\nI also made my own R package that wraps three templates to create Quarto websites that are frequently used by academics, qtwAcademic. In the following days I plan to write up more detailed explanations on how to use the package, along with some new features."
  },
  {
    "objectID": "blog/readnotes_2023010x_preventable_sridhar/index.html",
    "href": "blog/readnotes_2023010x_preventable_sridhar/index.html",
    "title": "Preventable: How a Pandemic Changed the World & How to Stop the Next One - Devi Sridhar",
    "section": "",
    "text": "Advice on some measures to prepare for the next pandemic\n(From Five ways to prepare for the next pandemic by Prof. Devi Sridhar)\n\nMonitor zoonoses. Identify patogens with pandemic potential, regulate better wet markets\nSequence globally. Investment in genetic-sequencing capability\nStrengthen manufacturing. Vaccine inequality, fragility of vaccine production. Private and public sector work together - vaccine research, production and distribution.\nVaccine preparedness. For known diseases (e.g. influenza), invest in vaccines that protect against a wide range of variants. New technology and research for unknown threats\nStop the spread (long enough for the vaccines) to save lives."
  },
  {
    "objectID": "blog/readnotes_2023010x_pandemic_gates/index.html",
    "href": "blog/readnotes_2023010x_pandemic_gates/index.html",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "",
    "text": "Some mistakes\n\nFederal agencies refused to share data\nPersons responsible did not have training inn epidemiology\nNot enough testing, not fast enough\nLacks information sharing\n\n\n\n\nThere should be a global expert team to help preventing the pandemic: they should be responsible for\n\nsurveillance of potential disease outbreaks, sound the alert when necessary;\ncreation and sharing of data system and data on cases\nstandardising policy making and training\nevaluation of the capacity of individual countries\ncoordination of personnels\n\nHowever it is difficult even for all countries to reach an agreement, and secure the funds. There is no one organisation that is able to join forces from all parties. Organisations depend on volunteers. WHO lacks funds, experts who are specialised in pandemic research, and relies on the free global response networks.\nA team, GERM (Global Epidemic Response and Mobilisation) should be established. The main task should be disease surveillance and modeling; rather than treating patients.\n\n\n\n\n\nPassive surveillance: healthcare workers report cases that use healthcare services (e.g. clinic, hospital) to public health authorities.\nActive surveillance: workers go to communities to find potential patients that have not been to clinics or hospitals due to inconvenience, or mild symptoms.\nWhen there are many cases (clusters), the signal might be picked up by a computer algorithm, and alerts sent to healthcare workers so that they pay more attention.\nIn some countries, personnels other than healthcare workers (e.g. teachers, post office staff) might also participate in disease surveillance. In addition, some virus can be detected in the environment such as waste water (e.g. polio, illeagal drug).\n\n\n\nIn LMIC (low and middle income countries), there are higher percentage of unrecorded births ad deaths. Some of them carry out census every other year - no real time data. Some data are also lack information such as cause of death. Without knowing causes of death (such as diarrhea), it is impossible to prevent the disease.\nPost-mortem (autopsy) can be unattainable, especially in LMIC. It could also be undesirable for families who have lost their loved ones - procedures are very invasive. Nevertheless, alternatives exist, such as minimally invasive autopsy technologies like MRI and MRI guided fine-needle biopsy.\n\n\n\n\nThe effect of NPI (non-pharmaceutical intervention, such as masks and lockdown) is difficult to quantify; yet it is a very important measure.\nParadox: NPI is effective -&gt; reduced cases -&gt; people think NPI is not necessary\nLockdown can slow down spread, yet it has a huge impact on economy, especially for LMIC.\nContact tracing\n\nnot a new technology; used for smallpox, Ebola, AIDS.\nnot widely applicable: some countries do it better than others. Need more trust from people towards public health agencies.\nsmartphone apps, not very useful: limited by users\n\nGood ventilation system\n\nviruses survive in air, but for different length of time\n\nSocial distance\n\n6 inch isn’t a magic distance\ndepends on circumstances: indoor/outdoor\n\n\n\n\nInfodemic\n(these two chapters are highly technical, and they deserve a separate note)\n\n\n\nDisaster simulation and drill\nDrill: assume a city is at risk of a disease that has epidemic potential\n\nhow to develop diagnostic tests and large-scale manufacturing and distribution\ngovernment, timely and comprehensive information dissemination\nmanagement of quarantine\nset up system for case reporting\n\n\n\n\nThe impact is different among different groups.\nVaccination distribution is highly imbalanced; yet it is only one of the many aspects where inequality exists, and not even the most unequal.\n\n\n\n\n\nInvest in better vaccine, treatment, diagnosis\nTesting and approval process\nFinding new treatment and vaccine\n\ncreate a large database for anti-virus chemicals, open to all\nAI and software to speed-up development\n\n\n\n\nPublic health agencies are under-funded, this is true for all levels: state/county, country and international organizations (WHO).\n\n\n\nImprove census, birth and death in LMIC; then expand into sequencing pathogens, environmental monitoring.\nAggregate disease surveillance systems internationally, and provide real-time data\n\n\n\nRebuild the system after Covid, invest more in healthcare, more staff\nSpend more on basic prevention for all and early diagnosis, rather than in-hospital treatment for severe cases\nManagement, clear tasks and responsibility"
  },
  {
    "objectID": "blog/readnotes_2023010x_pandemic_gates/index.html#learn-from-covid",
    "href": "blog/readnotes_2023010x_pandemic_gates/index.html#learn-from-covid",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "",
    "text": "Some mistakes\n\nFederal agencies refused to share data\nPersons responsible did not have training inn epidemiology\nNot enough testing, not fast enough\nLacks information sharing"
  },
  {
    "objectID": "blog/readnotes_2023010x_pandemic_gates/index.html#create-a-pandemic-prevention-team",
    "href": "blog/readnotes_2023010x_pandemic_gates/index.html#create-a-pandemic-prevention-team",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "",
    "text": "There should be a global expert team to help preventing the pandemic: they should be responsible for\n\nsurveillance of potential disease outbreaks, sound the alert when necessary;\ncreation and sharing of data system and data on cases\nstandardising policy making and training\nevaluation of the capacity of individual countries\ncoordination of personnels\n\nHowever it is difficult even for all countries to reach an agreement, and secure the funds. There is no one organisation that is able to join forces from all parties. Organisations depend on volunteers. WHO lacks funds, experts who are specialised in pandemic research, and relies on the free global response networks.\nA team, GERM (Global Epidemic Response and Mobilisation) should be established. The main task should be disease surveillance and modeling; rather than treating patients."
  },
  {
    "objectID": "blog/readnotes_2023010x_pandemic_gates/index.html#get-better-at-detecting-outbreaks-early",
    "href": "blog/readnotes_2023010x_pandemic_gates/index.html#get-better-at-detecting-outbreaks-early",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "",
    "text": "Passive surveillance: healthcare workers report cases that use healthcare services (e.g. clinic, hospital) to public health authorities.\nActive surveillance: workers go to communities to find potential patients that have not been to clinics or hospitals due to inconvenience, or mild symptoms.\nWhen there are many cases (clusters), the signal might be picked up by a computer algorithm, and alerts sent to healthcare workers so that they pay more attention.\nIn some countries, personnels other than healthcare workers (e.g. teachers, post office staff) might also participate in disease surveillance. In addition, some virus can be detected in the environment such as waste water (e.g. polio, illeagal drug).\n\n\n\nIn LMIC (low and middle income countries), there are higher percentage of unrecorded births ad deaths. Some of them carry out census every other year - no real time data. Some data are also lack information such as cause of death. Without knowing causes of death (such as diarrhea), it is impossible to prevent the disease.\nPost-mortem (autopsy) can be unattainable, especially in LMIC. It could also be undesirable for families who have lost their loved ones - procedures are very invasive. Nevertheless, alternatives exist, such as minimally invasive autopsy technologies like MRI and MRI guided fine-needle biopsy."
  },
  {
    "objectID": "blog/readnotes_2023010x_pandemic_gates/index.html#help-people-protect-themselves-right-away",
    "href": "blog/readnotes_2023010x_pandemic_gates/index.html#help-people-protect-themselves-right-away",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "",
    "text": "The effect of NPI (non-pharmaceutical intervention, such as masks and lockdown) is difficult to quantify; yet it is a very important measure.\nParadox: NPI is effective -&gt; reduced cases -&gt; people think NPI is not necessary\nLockdown can slow down spread, yet it has a huge impact on economy, especially for LMIC.\nContact tracing\n\nnot a new technology; used for smallpox, Ebola, AIDS.\nnot widely applicable: some countries do it better than others. Need more trust from people towards public health agencies.\nsmartphone apps, not very useful: limited by users\n\nGood ventilation system\n\nviruses survive in air, but for different length of time\n\nSocial distance\n\n6 inch isn’t a magic distance\ndepends on circumstances: indoor/outdoor"
  },
  {
    "objectID": "blog/readnotes_2023010x_pandemic_gates/index.html#find-new-treatment-fast-get-readyy-to-make-vaccines",
    "href": "blog/readnotes_2023010x_pandemic_gates/index.html#find-new-treatment-fast-get-readyy-to-make-vaccines",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "",
    "text": "Infodemic\n(these two chapters are highly technical, and they deserve a separate note)"
  },
  {
    "objectID": "blog/readnotes_2023010x_pandemic_gates/index.html#practice-practice-practice",
    "href": "blog/readnotes_2023010x_pandemic_gates/index.html#practice-practice-practice",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "",
    "text": "Disaster simulation and drill\nDrill: assume a city is at risk of a disease that has epidemic potential\n\nhow to develop diagnostic tests and large-scale manufacturing and distribution\ngovernment, timely and comprehensive information dissemination\nmanagement of quarantine\nset up system for case reporting"
  },
  {
    "objectID": "blog/readnotes_2023010x_pandemic_gates/index.html#close-the-health-gap-between-rich-and-poor-countries",
    "href": "blog/readnotes_2023010x_pandemic_gates/index.html#close-the-health-gap-between-rich-and-poor-countries",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "",
    "text": "The impact is different among different groups.\nVaccination distribution is highly imbalanced; yet it is only one of the many aspects where inequality exists, and not even the most unequal."
  },
  {
    "objectID": "blog/readnotes_2023010x_pandemic_gates/index.html#make-and-fund-a-plan-for-preventing-pandemics",
    "href": "blog/readnotes_2023010x_pandemic_gates/index.html#make-and-fund-a-plan-for-preventing-pandemics",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "",
    "text": "Invest in better vaccine, treatment, diagnosis\nTesting and approval process\nFinding new treatment and vaccine\n\ncreate a large database for anti-virus chemicals, open to all\nAI and software to speed-up development\n\n\n\n\nPublic health agencies are under-funded, this is true for all levels: state/county, country and international organizations (WHO).\n\n\n\nImprove census, birth and death in LMIC; then expand into sequencing pathogens, environmental monitoring.\nAggregate disease surveillance systems internationally, and provide real-time data\n\n\n\nRebuild the system after Covid, invest more in healthcare, more staff\nSpend more on basic prevention for all and early diagnosis, rather than in-hospital treatment for severe cases\nManagement, clear tasks and responsibility"
  },
  {
    "objectID": "blog/technotes_20230111_deployqt/index.html",
    "href": "blog/technotes_20230111_deployqt/index.html",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "",
    "text": "After you have the public repo, clone it to your local repo."
  },
  {
    "objectID": "blog/technotes_20230111_deployqt/index.html#create-a-public-repository-on-github",
    "href": "blog/technotes_20230111_deployqt/index.html#create-a-public-repository-on-github",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "",
    "text": "After you have the public repo, clone it to your local repo."
  },
  {
    "objectID": "blog/technotes_20230111_deployqt/index.html#create-quarto-project",
    "href": "blog/technotes_20230111_deployqt/index.html#create-quarto-project",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "2. Create Quarto project",
    "text": "2. Create Quarto project\nThis can be a website, a book (a specific type of website) or something else.\nTest compilation by quarto render, or click the Render button."
  },
  {
    "objectID": "blog/technotes_20230111_deployqt/index.html#configure-quarto-project",
    "href": "blog/technotes_20230111_deployqt/index.html#configure-quarto-project",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "3. Configure Quarto project",
    "text": "3. Configure Quarto project\nIn _quarto.yml, change the project configuration to use docs as the output-dir:\nproject:\n  type: website\n  output-dir: docs\n\n\n\n\n\nThen add .nojekyll to the root of the repository. Can do this by (in terminal)\ntouch .nojekyll\nPush everything to your repository."
  },
  {
    "objectID": "blog/technotes_20230111_deployqt/index.html#configure-github-pages",
    "href": "blog/technotes_20230111_deployqt/index.html#configure-github-pages",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "4. Configure GitHub Pages",
    "text": "4. Configure GitHub Pages\nGo to Settings &gt; Pages, publish from docs of the main branch.\n\n\n\n\n\nCan check GitHub Action and deployment status.\n\n\n\n\n\n\n\n\n\n\nAfter the deployment is successful, go to view deployment, and a successful website should be published."
  },
  {
    "objectID": "blog/readnotes_20240606_bad_pharma/index.html",
    "href": "blog/readnotes_20240606_bad_pharma/index.html",
    "title": "Bad Pharma: How medicine is broken, and how we can fix it - Ben Goldacre",
    "section": "",
    "text": "Industry funded trials were twenty times more likely to give results that are favoring the test drug\n\nOn the need for meta-analysis\n\nPeople would write long review articles surveying the literature - in which they would cite the trial data they come across in a completely unsystematic fashion, often reflecting their own prejudice and values.\n\nOn trials\n\n… mild torture economy: you’re not being paid to do a job, you’re being paid to endure.\n\nOn regulators\n\nfree movement of staff between regulators and drug companies… a fifth of those surveyed said they had been pressured to approve a drug despite reservation about efficacy and safety\n\n\napplication from large companies, which have greater experience with the regulatory process, pass through to approval faster than those from smaller companies\n\nOn surrogate outcomes\n\nthey are approved for showing a benefit on surrogate outcomes, such as blood test, that is only weakly or theoretically associated with the real suffering and death we’re trying to avoid. Sometimes drugs which work well to change surrogate outcomes simply don’t make any difference to the real outcome.\n\nOn comparative effectiveness research\n\nit is a vitally important filed, in many cases the value of finding out what works best among the drugs we already have would hugely exceed the value of developing entirely new ones.\n\nOn safety and efficacy for approved drugs\n\n39 percent patients believe that FDA only approves ‘extremely effective’ drugs, and 25 percent that it only approves drugs without serious side effect. However regulators frequently approve drugs that are only vaguely effective, with serious side effects, on the off-chance that they might be useful to someone, somewhere, when other interventions aren’t an option.\n\nOn drug reviewing\n\nRegulators that have approved a drug are often reluctant to take it off the market, in case it is seen as an admissio of their failure to spot problems in the first place\n\nOn trial patients\n\nthe ‘ideal’ patients are likely to get better, they exaggerate the benefits of drugs, and help expensive new medicines appear to be more cost effective than they really are. ‘External validity’: trial patient being unrepresentative\n\nOn comparison drugs\n\nit is common to see trials where a new drug is compared to a competitor that is known to be useless; or with a good competitor at a stupidly low (or high) dose\n\nOn random variation in the data\n\nearly stopping because you peeked in the results. should set up stopping rules, specified before the trial begins\nneed a large trial to detect a small difference between two treatments, and a very large trial to be confident that two drugs are equally effective\n(multiple testing, sub-group analysis): measuring lots of things, some will be statistically significant, simply from the natural random variation in all trial data.\n\nOn presenting the results\n\npercent reduction in the risk of heart attack (risk difference)\nrelative risk reduction\npresenting the results as relative risk reduction overstates the benefits\n\nWays trials go wrong\n\nunrepresentative patients\ntoo brief\nmeasure the wrong outcomes\ngo missing, if the result is unflattering\nanalysed wrongly\n\nOn ‘simple trial’ using EHR\n\nat present trials are very expensive. Many struggle to recruit enough patients, many struggle to recruit everyday doctors who don’t want to get involved in the mess of filing out patient report forms, calling patients back for extra appointments, doing extra measurements and so on\nsimple trials have disadvantage of being not blinded - patients know what drug they’ve received\npragmatic trials are cheap\nthese trials run forever and follow-up data are easy to get\n\nOn marketing\n\nSome have estimated that the pharmaceutical industry overall spends twice as much on marketing and promotion as it does on research and developments\n\n\nDrugs are advertised more when the number of potential patients, rather than the current patients, is large."
  },
  {
    "objectID": "blog/readnotes_20240606_bad_pharma/index.html#notes-from-the-book",
    "href": "blog/readnotes_20240606_bad_pharma/index.html#notes-from-the-book",
    "title": "Bad Pharma: How medicine is broken, and how we can fix it - Ben Goldacre",
    "section": "",
    "text": "Industry funded trials were twenty times more likely to give results that are favoring the test drug\n\nOn the need for meta-analysis\n\nPeople would write long review articles surveying the literature - in which they would cite the trial data they come across in a completely unsystematic fashion, often reflecting their own prejudice and values.\n\nOn trials\n\n… mild torture economy: you’re not being paid to do a job, you’re being paid to endure.\n\nOn regulators\n\nfree movement of staff between regulators and drug companies… a fifth of those surveyed said they had been pressured to approve a drug despite reservation about efficacy and safety\n\n\napplication from large companies, which have greater experience with the regulatory process, pass through to approval faster than those from smaller companies\n\nOn surrogate outcomes\n\nthey are approved for showing a benefit on surrogate outcomes, such as blood test, that is only weakly or theoretically associated with the real suffering and death we’re trying to avoid. Sometimes drugs which work well to change surrogate outcomes simply don’t make any difference to the real outcome.\n\nOn comparative effectiveness research\n\nit is a vitally important filed, in many cases the value of finding out what works best among the drugs we already have would hugely exceed the value of developing entirely new ones.\n\nOn safety and efficacy for approved drugs\n\n39 percent patients believe that FDA only approves ‘extremely effective’ drugs, and 25 percent that it only approves drugs without serious side effect. However regulators frequently approve drugs that are only vaguely effective, with serious side effects, on the off-chance that they might be useful to someone, somewhere, when other interventions aren’t an option.\n\nOn drug reviewing\n\nRegulators that have approved a drug are often reluctant to take it off the market, in case it is seen as an admissio of their failure to spot problems in the first place\n\nOn trial patients\n\nthe ‘ideal’ patients are likely to get better, they exaggerate the benefits of drugs, and help expensive new medicines appear to be more cost effective than they really are. ‘External validity’: trial patient being unrepresentative\n\nOn comparison drugs\n\nit is common to see trials where a new drug is compared to a competitor that is known to be useless; or with a good competitor at a stupidly low (or high) dose\n\nOn random variation in the data\n\nearly stopping because you peeked in the results. should set up stopping rules, specified before the trial begins\nneed a large trial to detect a small difference between two treatments, and a very large trial to be confident that two drugs are equally effective\n(multiple testing, sub-group analysis): measuring lots of things, some will be statistically significant, simply from the natural random variation in all trial data.\n\nOn presenting the results\n\npercent reduction in the risk of heart attack (risk difference)\nrelative risk reduction\npresenting the results as relative risk reduction overstates the benefits\n\nWays trials go wrong\n\nunrepresentative patients\ntoo brief\nmeasure the wrong outcomes\ngo missing, if the result is unflattering\nanalysed wrongly\n\nOn ‘simple trial’ using EHR\n\nat present trials are very expensive. Many struggle to recruit enough patients, many struggle to recruit everyday doctors who don’t want to get involved in the mess of filing out patient report forms, calling patients back for extra appointments, doing extra measurements and so on\nsimple trials have disadvantage of being not blinded - patients know what drug they’ve received\npragmatic trials are cheap\nthese trials run forever and follow-up data are easy to get\n\nOn marketing\n\nSome have estimated that the pharmaceutical industry overall spends twice as much on marketing and promotion as it does on research and developments\n\n\nDrugs are advertised more when the number of potential patients, rather than the current patients, is large."
  },
  {
    "objectID": "blog/technotes_20230228_clinreport_part3/index.html",
    "href": "blog/technotes_20230228_clinreport_part3/index.html",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 3",
    "section": "",
    "text": "This is a course provided by Genentech (part of Roche) on Coursera.\nCourse link"
  },
  {
    "objectID": "blog/technotes_20230228_clinreport_part3/index.html#principles-and-tools",
    "href": "blog/technotes_20230228_clinreport_part3/index.html#principles-and-tools",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 3",
    "section": "Principles and tools",
    "text": "Principles and tools\nReproducibility: Git (code versioning), dependencies (renv for r package dependencies, Docker for system dependencies)\n\nClean code\nCode comments: not recommended! Better to write code in a way that does not need additional comments.\nDRY: don’t repeat yourself (principle of software development), avoid copy and paste everywhere.\nSRP: single-responsibility prinicple, a function should do one thing: either plot a chart, saves a file, changes variables etc, but not all.\nNaming conventions\n\nReserve dots (.) for S3 methods (print.patient)\nReserve CamelCase for R6 classes or package names (OurPatients)\nUse snake cases (all_patients) for function names and arguments, use verb noun pattern (plot_this())\n\n\n\nCode smells\nA function might be too large: break into smaller ones (e.g. could fit in one screen)\nA function violates SRP: break into smaller ones, and be explicit in what result it is expected to return\nA function with multiple arguments: the scenarios to be tested increase rapidly. Recommended to minimize number of critical function arguments, and break the function into smaller ones.\nBad comments in the code: drop the unnecessary, unclear, outdated comments, write code that are self-explanatory.\n\n\nDevelopment workflow\nCode refactoring: change existing code without its functionality\nTDD: Test-Driven Development\n\nstart with writing a new (failing) test\nwrite code thtat passes the nenw tetst\nrefactor the code\nand repeat\n\nBenefits: your code is covered by tests; you think of testing scenarios first; “fail fast” - can immediately repair the code; more freedom to refactor (improve) the code.\nHow to test\n\nautomatically: CI/CD, after pushing Git commits\nmanually:\n\nrun all unit tests in the package (Build / Test package)\nrun tests in a selected test file (Run Tests)\nrun a single test in Rstudio console\n\n\nHow to check\n\nR CMD CHECK"
  },
  {
    "objectID": "blog/technotes_20230228_clinreport_part3/index.html#writing-robust-statistical-software",
    "href": "blog/technotes_20230228_clinreport_part3/index.html#writing-robust-statistical-software",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 3",
    "section": "Writing robust statistical software",
    "text": "Writing robust statistical software\nImplement complext statistical methods such that the software is reliable, and includes appropriate testing to ensure high quality and validity and ultimately credibility of statistical analysis results.\n\nchoose the right method and understand them\nsolve the core implementation problem with prototype code\n\nNeed to try a few different solutions, compare and select the best one. Might also need to involve domain experts.\n\nspend enough time on planning the design of the R package\n\nDon’t write the package right away; instead define the scope, discuss with users, and design the package.\nStart to draw a flow diagram, align names, arguments and classes; write prototype code.\n\nassume the package will evolve over time\n\nPackages you depend on will change; users will require new features\nWrite tests\n\nunit tests\nintegration tests\n\nMake the package extensible\n\nconsider object oriented package designs\ncombine functions in pipelines\n\nKeep it manageable\n\navoid too many arguments\navoid too large functions"
  },
  {
    "objectID": "blog/technotes_20230228_clinreport_part3/index.html#key-components",
    "href": "blog/technotes_20230228_clinreport_part3/index.html#key-components",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 3",
    "section": "Key components",
    "text": "Key components\n\nDependency management\nInstall dependencies (system/OS level; R packages)\n\nSet repos (can be specified in options()) to e.g. CRAN, BioConductor\nrenv\ncontainer with dependencies pre-installed\n\n\n\nStatic code analysis\n\nLinting (for programmatic and syntax errors) via lintr package\nCode style enforcement via styler package\nSpell checks identifies misspelled words in vignettes, docs and R code via spelling package\n\n\n\nTesting\n\nR CMD build builds R packages as a installable artifact\nR CMD check runs 20+ checks including unit tests, reports errors, warnigns and notes\nTest coverage reports with covr, checks how many lines of code are covered with tests\nR CMD INSTALL tests R package installation\n\n\n\nDocumentation\nAuto-generated docs via Roxygen and pkgdown\n\n\nRelease and deployments\nRelease artifacts and deployments to target systems\n\nChangelog (features, bug fixes) in the NEWS.md\nRelease: create the package with R CMD build. Validation report with thevalidatoR\nPublishing: CRAN, BioConductor"
  },
  {
    "objectID": "blog/technotes_20240506_ohdsi_part1/index.html",
    "href": "blog/technotes_20240506_ohdsi_part1/index.html",
    "title": "Notes: The Book of OHDSI - Data Analytics",
    "section": "",
    "text": "The Book of OHDSI written by the OHDSI community.\nWhat is required to go from origin (source data) to destination (evidence):\nOMOP: Observational Medical Outcomes Partnership, aims to identify true drug safety association.\nOMOP CDM: common data model, a mechanism to standardize the structure, content and semantics to make it possible to write statistical code that can be reused at every data site.\nOHDSI community (2014) has created libraries of open-source analytics tools atop OMOP CDM to support:"
  },
  {
    "objectID": "blog/technotes_20240506_ohdsi_part1/index.html#characterization",
    "href": "blog/technotes_20240506_ohdsi_part1/index.html#characterization",
    "title": "Notes: The Book of OHDSI - Data Analytics",
    "section": "Characterization",
    "text": "Characterization\n\nWhat happened to the patients.\n\nChapter 11 Characterization\nTypical characterization questions:\n\nHow many patients…?\nHow often does…? What proportion of patients …?\nWhat is the distribution of values for …?\nWhat is the median length of exposure for patients on …?\nOther drugs the patient is using?\n\nDesired output:\n\ncount, percentage\naverages and other descriptive statistics\nprevalence, incidence rate\nrule-based phenotype\ndrug utilization, adherence, treatment pathways, line of therapy\ndisease natural history, co-morbidity profile"
  },
  {
    "objectID": "blog/technotes_20240506_ohdsi_part1/index.html#population-level-estimation",
    "href": "blog/technotes_20240506_ohdsi_part1/index.html#population-level-estimation",
    "title": "Notes: The Book of OHDSI - Data Analytics",
    "section": "Population-level estimation",
    "text": "Population-level estimation\n\nWhat are the causal effects\n\nChapter 12 Population-level Estimation\nTypical questions:\n\nWhat is the effect of …?\nWhich treatment works better?\nWhat is the risk of X on Y?\nWhat is the time-to-event of …?\n\nDesired output:\n\nRR, HR, OR\nAssociation, correlation\nATE, causal effect"
  },
  {
    "objectID": "blog/technotes_20240506_ohdsi_part1/index.html#patient-level-prediction",
    "href": "blog/technotes_20240506_ohdsi_part1/index.html#patient-level-prediction",
    "title": "Notes: The Book of OHDSI - Data Analytics",
    "section": "Patient-level prediction",
    "text": "Patient-level prediction\n\nWhat will happen to A?\n\nChapter 13 Patient-level Prediction\nTypical questions:\n\nWhat is the chance that this patient will…?\nWho are the candidate for…?\n\nDesired output:\n\nprobability for an individual\nprediction model\nhigh/low risk groups\nprobabilistic phenotype"
  },
  {
    "objectID": "blog/technotes_20231001_qt_webr/index.html",
    "href": "blog/technotes_20231001_qt_webr/index.html",
    "title": "Use WebR in your existing quarto website",
    "section": "",
    "text": "WebR is the new hot topic in the R community. Coupled with Quarto, you can run R code interactively in a web browser. This is achieved with the great quarto extension, quarto-webr developed by James J Balamuta.\nIn the positconf 2023 talk, documentation and YouTube, James introduced how to make a webR empowered quarto document. It is simple enough, and you can make it work quite smoothly."
  },
  {
    "objectID": "blog/technotes_20231001_qt_webr/index.html#when-your-render-gets-stuck",
    "href": "blog/technotes_20231001_qt_webr/index.html#when-your-render-gets-stuck",
    "title": "Use WebR in your existing quarto website",
    "section": "When your render gets stuck",
    "text": "When your render gets stuck\nBut there is a twist. This works perfectly fine with a new quarto project, where no output-dir is specified yet. When I tried to replicate the same thing for my existing quarto website (with output-dir: docs so that I could deploy it with GitHub Pages), my rendered html file got stuck:\n\nIf you read the troubleshooting documentation, you’ll see that it’s a problem with the two js files. This agrees with what Rstudio Background Jobs tells us.\n\nI moved the two files (manually..) around, then render again, nothing changed.\n\nSolution: set channel-type option\nThis is a solution provided by the authors, although I don’t quite understand what it did, but it did the magic. (Thanks to Linh’s help!)\nThis is where you specify this option.\n\nRender again, now it works! WebR status turns green, and I can run code interactively in the browser."
  },
  {
    "objectID": "blog/blog_20230104_qtwAcademic/index.html",
    "href": "blog/blog_20230104_qtwAcademic/index.html",
    "title": "qtwAcademic: a quick and easy way to start your Quarto website",
    "section": "",
    "text": "qtwAcademic stands for Quarto Websites for Academics, which provides a few Quarto templates for Quarto website that are commonly used by academics.\nThe templates are designed to make it quick and easy for users with little or no Quarto experience to create a website for their personal portfolio or courses. Each template is fully customizable once the user is more familiar with Quarto.\nRead more about the package here.\nMore details about the package is being written …"
  },
  {
    "objectID": "blog/technotes_20231018_qt_styling/index.html",
    "href": "blog/technotes_20231018_qt_styling/index.html",
    "title": "Styling your quarto project",
    "section": "",
    "text": "Useful references:\n\nTalk by Emil Hvitfeldt on Styling and Templating Quarto Documents"
  },
  {
    "objectID": "blog/technotes_20230222_clinreport_part2/index.html",
    "href": "blog/technotes_20230222_clinreport_part2/index.html",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 2",
    "section": "",
    "text": "This is a course provided by Genentech (part of Roche) on Coursera.\nCourse link"
  },
  {
    "objectID": "blog/technotes_20230222_clinreport_part2/index.html#agile-mindset-and-devops-practices",
    "href": "blog/technotes_20230222_clinreport_part2/index.html#agile-mindset-and-devops-practices",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 2",
    "section": "Agile mindset and DevOps practices",
    "text": "Agile mindset and DevOps practices\n\nData science as a new way of thinking\nNew way of working means\n\nleverage standards and automation (CI/CD)\nadopt new data types quickly, reusing data for multiple purposes, pooling data, data marts\nopen-sourcing and collaborating cross pharma (small, readable, self-tested code)\ncoding for reusability, moving away from single-use programs\nrapidly re-arranginng re-usable components to meet analytical need at hand\n\nData scientist need to have hard skills, such as\n\nSAS, R, Python, JS, bash\ncloud, containers\nCI/CD tools\nvisualisation\nknowledge of various data types\n\nand also soft skills:\n\ncollaborative and inclusive\ntransparent and practical\ncreative and proactive\nasking the right questions\nable to wear many hats, be more flexible and resilient\n\n\n\nAgile\nProject management; a mindset: uncover better ways of working, by doing and helping others do it.\n1st principle: highest priority is to satisfy the customer through early and continuous delivery of valuable software.\nImplementations: Kanban, Scrum, Lean, Extreme programming\nTools:\n\nbacklog\nkanban board (not started, in progress, done)\nWIP (work in progress limit)\nprogress measures: e.g. team velocity\n\n\n\nDevOps\nIncrease efficiency by improving the connection between Dev (software development) and Ops (IT operations).\nThe goal is continuous delivery and continuous improvement.\nPractices:\n\nmodular architecture\nversion control\nmerge into trunk daily\nautomated and continuous testing, continuous integration\nautomated deployment\n\n\nDevOps in clinical reporting\nRisks around production run:\n\nare all dependencies in production?\nwas all quality control completed and successful?\nis all documentation complete?\nwas the transfer to eDMS correct and successful?"
  },
  {
    "objectID": "blog/technotes_20230222_clinreport_part2/index.html#version-control",
    "href": "blog/technotes_20230222_clinreport_part2/index.html#version-control",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 2",
    "section": "Version control",
    "text": "Version control\nFeature branch (as opposed to master branch): one task per branch\nname feature branch: issue number and description\nEach issue should have a clear description, short and specific; instead of being long and overarching.\n\nWorkflow for clinical reporting\nRestraints of clinical deliveries: timing annd multiple deliveries; resourcing challenges\nMight need to choose between feature and GitFlow."
  },
  {
    "objectID": "blog/technotes_20230222_clinreport_part2/index.html#reproducible-projects-in-r",
    "href": "blog/technotes_20230222_clinreport_part2/index.html#reproducible-projects-in-r",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 2",
    "section": "Reproducible projects in R",
    "text": "Reproducible projects in R\nTo reproduce your work:\n\nGit (version control)\nR libraries\nWell structured projects\nUnderlying dependencies (e.g. operating systems, C++/C)\n\n\nWell structured projects\nClear names\nGood documentation\n\n\nR libraries and versions\nCheck session info; but not the most practical way.\nUse global libraries, .libPaths(), this gives you the path where all the packages are installed. Global libraries is useful when using a server for multiple R sessions, where they look for the packages in the same place.\nSolutions\n\nrenv package: makes each project in R self-contained.\nCheckpoint: project level library paths based on snapshots of CRAN\n\nUse Docker images! Saves R version, operating system, underlying dependencies"
  },
  {
    "objectID": "blog/blog_20230717_teaching/index.html",
    "href": "blog/blog_20230717_teaching/index.html",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "",
    "text": "Earlier this year (2023) I wrote a blog about my thoughts on the role of open source software in statisical education. Naturally, I advocate for more use of open source tools such as R/python in teaching introductory statistics to applied scientists. Nonetheless, how the material is taught will make a huge difference in the understanding and interest in the material.\nI was taught statistics in the classic way: lectures with tons of mathematical formulae and proofs, while programming and data analyses were left for students themselves to figure out. Those who were the fastest learners were the ones who already had a degree in computer science, which probably doesn’t sound surprising. I, for one, definitely struggled."
  },
  {
    "objectID": "blog/blog_20230717_teaching/index.html#does-statistics-have-to-be-daunting",
    "href": "blog/blog_20230717_teaching/index.html#does-statistics-have-to-be-daunting",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "Does statistics have to be daunting?",
    "text": "Does statistics have to be daunting?\nFor applied scientists in various fields, data analysis is a core task, and also a challenging one. You must have met clinicians or biologists who would love their data to be analysed yet don’t know how to. Yes, statistics and data skills can take some time to learn; but with the right method, they don’t have to be daunting. It is up to the educator to find a way that benefits the most students. An observation is that many researchers do not know or remember advanced math; yet do they need advanced math to grasp many fundamental statistical concepts?\nI believe that it is far more important and useful to teach basic IT skills and exploratory data analysis so that students can develop an understanding of their own data; rather than using a test blindly."
  },
  {
    "objectID": "blog/blog_20230717_teaching/index.html#rebooting-mf9130e-classroom",
    "href": "blog/blog_20230717_teaching/index.html#rebooting-mf9130e-classroom",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "Rebooting MF9130E classroom",
    "text": "Rebooting MF9130E classroom\nWhen I heard that the teaching team at Biostatistics Department, Faculty of Medicine was thinking about trying a novel pedagogical method on the MF9130E (2023 spring) class, I was more than excited to contribute. This is a PhD level course of 8 days long, offered three times a year (twice in Norwegian language). Students come from a variey of backgrounds in health and life sciences. Since this is an introductory course, the topics are broad rather than specialised.\nA few years ago, statistical software for the course made the transition from SPSS to Stata. To be more precise, students were introduced to, but not really explained to, or elaborated on how to use Stata proficiently. Why? The course is about statistics so only statistics is taught. Data skills such as manipulation are not part of statistics. \nWell, we will change that by starting to use R.\n\nThree open source musketeers\nR, quarto and GitHub the three musketeers in facilitating the transformation. We build a quarto course website where all the material are public, hosted with GitHub Pages. Having a course website is beneficial for students to have an overview of the course, in contrast to many scattered lecture notes and exercises to be downloaded.\nThe biggest advantage of using quarto is the rendered output from code. From a student’s perspective, it is reassuring to see the same result and plots using the data and code provided by the instructor. For the instructor, it is also convenient to see whether the code functions as expected. When we do not want to show the output, it is also very easy to suppress. We have created one copy with and one withtout rendered output as exercises, and are glad to see some students challenging themselves by attempting to solve the problems without solution.\nUsing Github and quarto together to build a course website is rather straightforward. I think the site structure is simple yet flexible enough to navigate. Collaboration across a small teaching team is also manageable. Github Pages was easy to set up, and changes made on the main branch is deployed within the minute. This proved to be useful in quite a few moments (where we had to replace some datasets or add some notice).\n\n\nThe Carpentries pedagogical model\nThe Carpentries is an organisation that teaches foundational coding and data science skills to researchers. I myself benefited from their workshop on version control and git taught at University of Oslo, and I think the traditional classroom could use some of the methods at these data science workshops.\nTo put simply, there are two things I tried with the course setup for MF9130E:\n\nLive coding demonstration, plenty of it\nSticky-notes flag and helper (teaching assistant) in class\n\nIn the live coding demonstration (which I was responsible for), I made sure that students were taught the most commonly used R commands for data manipulation and exploration. Quarto webpages on introduction to R, basic EDA, intermediate EDA have been created and guided through in class, mixed with statistical concepts and visualizations. Without knowing how your data looks like, blindly using statistical tests is dangerous - that is the motivation for doing so.\nWhether students feel supported can make a huge difference in their willingness to learn. Taking it slow at the beginning, and solve the problems on an individual basis can prevent early drop-outs, especially when programming and IT systems are involved. Naturally, when we don’t have helpers we can not help everyone; this is a limitation for this model. Students should be encouraged to help each other.\n\n\nLet them explore\nThe last important change in the class was to give time to students themselves. We reduced the lecturing on theory and computation, and added time for practice and discussion. The guided practice with live demo also came with solution and comments, so students could explore at their own pace. We left plenty of time for them to ask questions, and made sure most people can follow the exercises."
  },
  {
    "objectID": "blog/blog_20230717_teaching/index.html#how-did-it-go",
    "href": "blog/blog_20230717_teaching/index.html#how-did-it-go",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "How did it go?",
    "text": "How did it go?\nAfter the 8 day course we carried out a small survey among the ~50 students in the spring 2023 class. Student backgrounds are diverse, they work on lab data, clinical data or observational/epidemiological data:\n\nobservational study on humans 36%\nRCT 18%\nin vitro research 15%\nothers are in animal research, meta analysis or something else\n\nStatistical competency (method, software) among students are generally on the basic end. Over 75% of the cohort report themselves to have basic to very basic knowledge of statistics; 33% do not use any statistical software, around 45% have used SPSS or Stata. On the other hand, some students (7%) report to have advanced knowledge and have some R experience.\n\nSome feedback\nThis is the first time we do the course with R, live demo and put an emphasis on basic data manipulation and exploration - which means we do not have enough data, it is just an initial impression.\nHere’s what we have received. On the positive side, 86% find the course useful for their own PhD research. 75% felt they are able to use the correct methods for their analyses, which is quite encouraging. Most felt the examples and exercises were able to demonstrate the theory. Students have generally positive experience with the live demo, and find the instructors supportive. This is good!\nIn the meantime, it is only natural that some are dissatisfied (21%) in some ways. Common complaints are: R is not user friendly to absolute beginners; the leap from no software to a programming language is too big for some.\nAs for whether students have really mastered the knowledge intended, we do not have enough data to draw a conclusion. We do observe that the take home project show somewhat better understanding, but can not say for sure just yet.\nThis is a class with very diverse backgrounds, hence it is challenging to cater to everyone’s needs. Yet, we are satisfied with the trial-transformation with our introductory statistics class, and we plan to gradually implement more classes with R, and possibly hands-on practice (depending on capacity)."
  },
  {
    "objectID": "blog/blog.html",
    "href": "blog/blog.html",
    "title": "Blogs",
    "section": "",
    "text": "Research Scientist Interview Guide\n\n\n\n\n\n\nData science\n\n\nInterview Guide\n\n\n\nResearch Scientist Interview Guide \n\n\n\n\n\nJul 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPersonal Highlights: Positconf 2024\n\n\n\n\n\n\nData science\n\n\n\nMy picked talks for 2024 after Posit released the talks on YouTube \n\n\n\n\n\nNov 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUse Quarto, Make Friends: a two-year journey\n\n\n\n\n\n\nQuarto\n\n\nWebsite\n\n\n\n\n\n\n\n\n\nSep 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBad Pharma: How medicine is broken, and how we can fix it - Ben Goldacre\n\n\n\n\n\n\nNotes\n\n\n\nReading notes on some of the chapters. Book on Amazon: link \n\n\n\n\n\nJun 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nNotes: The Book of OHDSI - Data Analytics\n\n\n\n\n\n\nData science\n\n\nObservational data\n\n\n\nObservational Health Data Sciences and Informatics \n\n\n\n\n\nMay 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nParameterized Reports with Quarto: R-Ladies Abuja Workshop\n\n\n\n\n\n\nR\n\n\nQuarto\n\n\nparameterized reports\n\n\nworkshop\n\n\n\n2-hour code-along workshop on parameterized reports with Quarto\n\n\n\n\n\nFeb 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWorking in Public: The Making and Maintenance of Open Source Software - Nadia Eghbal\n\n\n\n\n\n\nNotes\n\n\n\nReading notes on some of the chapters. Book on Amazon: link \n\n\n\n\n\nFeb 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nParameterized Reports with Quarto: R-Ladies DC Workshop\n\n\n\n\n\n\nR\n\n\nQuarto\n\n\nparameterized reports\n\n\nworkshop\n\n\n\n2-hour code-along workshop on parameterized reports with Quarto\n\n\n\n\n\nJan 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nStyling your quarto project\n\n\n\n\n\n\nWebsite\n\n\n\nLearning notes on how to customize a quarto project, such as website. \n\n\n\n\n\nOct 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nUse WebR in your existing quarto website\n\n\n\n\n\n\nQuarto\n\n\nWebsite\n\n\n\nYou might get stuck when you try to add the trending webR to quarto extension in your website. This is one way to fix it. \n\n\n\n\n\nOct 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nParameterized Quarto reports improve understanding of soil health\n\n\n\n\n\n\nR\n\n\nQuarto\n\n\nparameterized reports\n\n\nagriculture\n\n\nsoil health\n\n\n\nCreating custom soil health reports with Quarto\n\n\n\n\n\nSep 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPersonal Highlights: Positconf 2023\n\n\n\n\n\n\nData science\n\n\n\nCuration of content to check out when I’ve got time \n\n\n\n\n\nSep 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPersonal Highlights: CEN2023\n\n\n\n\n\n\nBiostatistics\n\n\n\nSelected summaries on the 5th Conference of the Central European Network of the International Biometric Society (IBS). \n\n\n\n\n\nSep 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nShiny optimization of climate benefits from a statewide agricultural grant program\n\n\n\n\n\n\nR\n\n\nshiny\n\n\nagriculture\n\n\nclimate\n\n\n\nDevelopment process of {WaCSE} shiny app\n\n\n\n\n\nAug 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTransforming medical statistics classroom with R and Quarto\n\n\n\n\n\n\nQuarto\n\n\nEducation\n\n\n\nSome reflection on the experimental 8-day introductory statisics course with new teaching methods. Visit the course website here. \n\n\n\n\n\nJul 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWashington Soil Health Initative and Climate Smart Estimator\n\n\n\n\n\n\nagriculture\n\n\nsoil health\n\n\nclimate\n\n\n\nOverview of the WA Soil Health Initative & WA Climate Smart Estimator {WaCSE} shiny app\n\n\n\n\n\nJun 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nR package workflow\n\n\n\n\n\n\nRpkg\n\n\nRSE\n\n\n\nA step-to-step guide to make CRAN-worthy R packages \n\n\n\n\n\nMay 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping & Mapping {orcas} Encounters\n\n\n\n\n\n\nR\n\n\nweb scraping\n\n\nleaflet\n\n\n\nWeb scraping with {rvest} & mapping with {leaflet}\n\n\n\n\n\nApr 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPreventable: How a Pandemic Changed the World & How to Stop the Next One - Devi Sridhar\n\n\n\n\n\n\nNotes\n\n\n\nReading notes on the book. Also serves as a collection of notes from Professor Devi Sridhar’s articles. \n\n\n\n\n\nMar 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCourse review: making DS work for clinical reporting\n\n\n\n\n\n\nReporting\n\n\nData science\n\n\nRSE\n\n\n\nA review of the Coursera course provivded by Genentech and Roche, on “Making data science work for clinical reporting”. \n\n\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNotes: Making Data Science work for Clinical Reporting - Part 4\n\n\n\n\n\n\nClinical trial\n\n\nData science\n\n\nReporting\n\n\n\nThis is the Part 4 of a four-part course on Coursera. \n\n\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNotes: Making Data Science work for Clinical Reporting - Part 3\n\n\n\n\n\n\nClinical trial\n\n\nData science\n\n\nReporting\n\n\n\nThis is the Part 3 of a four-part course on Coursera. In this part, innerSource and OpenSource concepts are introduced, and R package development is discussed. \n\n\n\n\n\nFeb 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTesting Shiny app and deploy to shinyapps.io\n\n\n\n\n\n\nShiny\n\n\nWebsite\n\n\n\nNotes on how to set up the free shinyapp.io to deploy a demo shiny app. \n\n\n\n\n\nFeb 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNotes: Making Data Science work for Clinical Reporting - Part 2\n\n\n\n\n\n\nClinical trial\n\n\nData science\n\n\nReporting\n\n\n\nThis is the Part 2 of a four-part course on Coursera. In this part, agile and DevOps practices are introduced, along with version control with Git and reproducible R projects. \n\n\n\n\n\nFeb 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nR package website with pkgdown\n\n\n\n\n\n\nRpkg\n\n\nWebsite\n\n\n\nA workflow that worked for me: when you have a few vignette documents, and want to display them nicely in a website format. \n\n\n\n\n\nFeb 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNotes: Making Data Science work for Clinical Reporting - Part 1\n\n\n\n\n\n\nClinical trial\n\n\nData science\n\n\nReporting\n\n\n\nThis is the Part 1 of a four-part course on Coursera. In this part, there is an introduction to clinical trial phases, and motivation to share data. In addition, some terms (such as CDISC standard) have been introduced. \n\n\n\n\n\nFeb 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nOpen source reporting with R: clinical, public health, RSE and embrace the change\n\n\n\n\n\n\nReporting\n\n\nData science\n\n\nEducation\n\n\nRSE\n\n\n\nMy thoughts on the open source transition in pharma, public (health) sector and academia. A culture change is needed, and it’s done better at some places than others. As educators and researchers, there are many things that can be done. \n\n\n\n\n\nJan 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPublishing Quarto Website with GitHub Pages\n\n\n\n\n\n\nQuarto\n\n\nWebsite\n\n\n\nA workflow that worked for me. This is the third time that I go through the Quarto website publishing with GitHub Pages - even more reason to note it down! \n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nqtwAcademic: a quick and easy way to start your Quarto website\n\n\n\n\n\n\nQuarto\n\n\nWebsite\n\n\n\n\n\n\n\n\n\nJan 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHow to prevent the next pandemic - Bill Gates\n\n\n\n\n\n\nNotes\n\n\n\nReading notes on some of the chapters. \n\n\n\n\n\nJan 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWebsite reboot: switching from Blogdown to Quarto\n\n\n\n\n\n\nQuarto\n\n\nWebsite\n\n\n\nTime to reboot the personal website. Now, with Quarto \n\n\n\n\n\nJan 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWashington Climate Smart Estimator: Using ArcGIS Dashboards and Experience Builder\n\n\n\n\n\n\nArcGIS\n\n\nagriculture\n\n\nclimate\n\n\n\nOverview of WA Climate Smart Estimator ArcGIS Experience web app\n\n\n\n\n\nMay 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nAquatic Risk Assessment: Organophosphate insecticide mixtures in Washington surface waters\n\n\n\n\n\n\nagriculture\n\n\nwater quality\n\n\n\nAquartic risk assessment of insecticide mixtures in WA streams\n\n\n\n\n\nNov 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nAquatic Risk Assessment: Organophosphate insecticide mixtures in Washington surface waters\n\n\n\n\n\n\nagriculture\n\n\nwater quality\n\n\n\nAquartic risk assessment of insecticide mixtures in WA streams\n\n\n\n\n\nNov 9, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "rpkg/rpkg1.html",
    "href": "rpkg/rpkg1.html",
    "title": "Research",
    "section": "",
    "text": "Research Focus\n\nMy research focuses on building safe, robust, and adaptable AI systems for real-world cyber-physical environments, integrating:\n\nPhysics-Informed Reinforcement Learning (PIRL)\nSafe Deep Reinforcement Learning (DRL)\nLarge Language Models (LLMs) for Control Reasoning\nVision-Based Simulation (CARLA) for Autonomous Systems\nTransfer Learning & Meta-Learning\nAdversarial Robustness and Safe Policy Adaptation\n\nApplication domains include smart energy systems, autonomous driving, robotics, and AI safety.\n\n\n\nPublications\n\nJournal Papers\n\nKundan Kumar, Gelli Ravikumar\nPhysics-based Deep Reinforcement Learning for Grid-Resilient Volt-VAR Control (Under Review)\nIEEE Transactions on Smart Grid, 2025\n Paper Code Poster \n\n\n\nConference Papers\n\nKundan Kumar, Gelli Ravikumar\nTransfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids\nIEEE PES Grid Edge Technologies Conference & Exposition, 2025\n Paper Code Poster \nKundan Kumar, Aditya Akilesh Mantha, Gelli Ravikumar\nBayesian Optimization for Deep Reinforcement Learning in Robust Volt-Var Control\nIEEE PES General Meeting, 2024\n Paper Code Poster \nKundan Kumar, Gelli Ravikumar\nDeep RL-based Volt-VAR Control and Attack Resiliency for DER-integrated Distribution Grids\nIEEE ISGT, 2024\n Paper Code Poster \nJK Francis, C Kumar, J Herrera-Gerena, Kundan Kumar, MJ Darr\nDeep Learning and Pattern-based Methodology for Multivariable Sensor Data Regression\nIEEE ICMLA, 2022\n Paper Code Poster \nKin Gwn Lore, Nicholas Sweet, Kundan Kumar, N Ahmed, S Sarkar\nDeep Value of Information Estimators for Collaborative Human-Machine Information Gathering\nACM/IEEE ICCPS, 2016\n Paper Code Poster"
  },
  {
    "objectID": "rpkg/csdata/index.html",
    "href": "rpkg/csdata/index.html",
    "title": "csdata",
    "section": "",
    "text": "https://github.com/csids/csdata"
  },
  {
    "objectID": "rpkg/qtwAcademic/index.html",
    "href": "rpkg/qtwAcademic/index.html",
    "title": "qtwAcademic",
    "section": "",
    "text": "qtwAcademic stands for Quarto Websites for Academics, which provides a few Quarto templates for Quarto website that are commonly used by academics.\nThe templates are designed to make it quick and easy for users with little or no Quarto experience to create a website for their personal portfolio or courses. Each template is fully customizable once the user is more familiar with Quarto.\nRead more about the package here.\n\nTemplates\nSo far, 3 templates have been implemented in this package:\n\nPersonal website\nWebsite for courses or workshops\nMinimal website template that can be easily customized\n\nYou can find more details on each option in the vignettes."
  },
  {
    "objectID": "rpkg/csmaps/index.html",
    "href": "rpkg/csmaps/index.html",
    "title": "csmaps",
    "section": "",
    "text": "https://github.com/csids/csmaps"
  },
  {
    "objectID": "rpkg/mortanor/index.html",
    "href": "rpkg/mortanor/index.html",
    "title": "mortanor",
    "section": "",
    "text": "https://github.com/csids/mortanor"
  },
  {
    "objectID": "rpkg/covidnor/index.html",
    "href": "rpkg/covidnor/index.html",
    "title": "covidnor",
    "section": "",
    "text": "https://github.com/csids/covidnor"
  },
  {
    "objectID": "rpkg/bayesynergy/index.html",
    "href": "rpkg/bayesynergy/index.html",
    "title": "bayesynergy",
    "section": "",
    "text": "An R package for Bayesian semi-parametric modelling of in-vitro drug combination experiments\nGitHub Link"
  },
  {
    "objectID": "teaching/coms3630/index.html#coms-3630-introduction-to-database-management-systems",
    "href": "teaching/coms3630/index.html#coms-3630-introduction-to-database-management-systems",
    "title": "COMS 3630",
    "section": "COMS 3630 – Introduction to Database Management Systems",
    "text": "COMS 3630 – Introduction to Database Management Systems\n\n📘 Course Description\nCOMS 3630 offers a comprehensive introduction to database systems, covering both theoretical foundations and practical implementation. Students explore various data models (relational, object-oriented, and semistructured), learn query languages such as SQL and NoSQL, and build full-stack applications that integrate modern database backends with web interfaces.\nThe course emphasizes database design, optimization, and application development, using tools like MySQL, MongoDB, and Neo4J.\n\n\n\n🧑‍🏫 My Role as Teaching Assistant\nAs a Teaching Assistant for COMS 3630, I supported over 100 students in understanding key database concepts and building end-to-end data-driven applications. My responsibilities included:\n\nLab Instruction & Demonstrations: Led weekly sessions on topics like SQL programming, ER modeling, and NoSQL databases (MongoDB, Neo4J).\nProject Mentorship: Guided students through semester-long projects, helping them design schemas, write efficient queries, and integrate databases with web applications.\nTechnical Support: Assisted students in implementing transactions, managing storage, and debugging issues related to query execution and performance.\nDesign Review & Grading: Evaluated assignments and database design submissions, providing constructive feedback aligned with best practices.\nOffice Hours: Offered one-on-one and group-based academic support on database internals, relational algebra, and query optimization strategies.\n\n\n\n\n🎯 Learning Outcomes\nBy the end of the course, students are expected to:\n\nDesign and implement relational databases using ER modeling and normalization techniques.\n\nDevelop database-driven applications using SQL, APIs, and ORM frameworks.\n\nWork with NoSQL systems including document-based (MongoDB) and graph-based (Neo4J) databases.\n\nUnderstand the inner workings of a DBMS, including query processing, transaction management, and storage optimization.\n\n\n\n\n📚 Topics Covered\n\nEntity-Relationship (ER) Modeling\n\nRelational Model & Relational Algebra\n\nSQL Programming and Constraints\n\nNoSQL Systems (MongoDB, Neo4J)\n\nSchema Normalization & Data Dependencies\n\nDatabase Storage & Indexing\n\nCost Estimation & Query Optimization\n\nTransaction Management & Concurrency Control\n\nWeb Application Integration using SQL APIs & ORMs\n\nApplication Development using Host Languages (e.g., Python, JavaScript)\n\n\n💡 Course Catalog | Email"
  },
  {
    "objectID": "teaching/coms3190/index.html",
    "href": "teaching/coms3190/index.html",
    "title": "COMS 3190",
    "section": "",
    "text": "COMS 3190 introduces students to the fundamentals of user interface (UI) and user experience (UX) design through both theory and extensive hands-on development. The course covers human-computer interaction (HCI) concepts, front-end and back-end technologies, and the use of modern frameworks and APIs for developing web and Windows-based user interfaces.\nStudents gain experience with:\n\nUI design principles\n\nHTML, CSS, JavaScript\n\nReact, Node.js, Express\n\nDatabases (MongoDB and MySQL)\n\nUML modeling and event-driven architecture\n\nWeb and desktop-based client/server applications\n\n\n\n\n\nAs a Teaching Assistant for COMS 3190, I supported over 100 students in learning full-stack web development principles within the context of UI/UX design. My key responsibilities included:\n\nTechnical Instruction: Provided lab support and walkthroughs for building responsive interfaces using HTML, CSS, JavaScript, React, and Node.js.\nProject Mentorship: Guided students through semester-long UI/UX projects, including front-end development, API design, and database integration.\nDesign & Modeling Help: Assisted students in using UML for system modeling and behavioral analysis, including use case and interaction diagrams.\nTesting Support: Supported unit testing and UI testing procedures in JavaScript and helped students debug their web applications.\nCode Review & Feedback: Reviewed student submissions and provided feedback on usability, design consistency, and code efficiency.\nOffice Hours: Held weekly office hours to help students troubleshoot development issues and understand best practices in UI/UX design.\n\n\n\n\n\nUpon completing the course, students are expected to:\n\nDesign and implement interactive and accessible user interfaces.\nApply principles of HCI, UI design, UX testing, and event-driven architecture.\nUse industry-standard tools to build and evaluate responsive, full-stack applications.\nAnalyze and model system behavior using UML and interaction diagrams.\nDeploy and test applications with modern development environments and version control.\n\n\n\n\n\n\nDesign Principles for User Interfaces\n\nHuman-Computer Interaction (HCI) Fundamentals\n\nUX Testing and Evaluation\n\nHTML, CSS, JavaScript\n\nReact, Node.js, Express\n\nMongoDB, MySQL\n\nEvent-Driven Programming\n\nAPI & Framework Integration\n\nUML Diagrams (Structural, Behavioral, Interaction)\n\nUnit & UI Testing in JavaScript\n\nWindows-based UI Development\n\nClient/Server Architecture\n\n\n💡 Course Catalog | Email"
  },
  {
    "objectID": "teaching/coms3190/index.html#coms-3190-user-interface-design",
    "href": "teaching/coms3190/index.html#coms-3190-user-interface-design",
    "title": "COMS 3190",
    "section": "",
    "text": "COMS 3190 introduces students to the fundamentals of user interface (UI) and user experience (UX) design through both theory and extensive hands-on development. The course covers human-computer interaction (HCI) concepts, front-end and back-end technologies, and the use of modern frameworks and APIs for developing web and Windows-based user interfaces.\nStudents gain experience with:\n\nUI design principles\n\nHTML, CSS, JavaScript\n\nReact, Node.js, Express\n\nDatabases (MongoDB and MySQL)\n\nUML modeling and event-driven architecture\n\nWeb and desktop-based client/server applications\n\n\n\n\n\nAs a Teaching Assistant for COMS 3190, I supported over 100 students in learning full-stack web development principles within the context of UI/UX design. My key responsibilities included:\n\nTechnical Instruction: Provided lab support and walkthroughs for building responsive interfaces using HTML, CSS, JavaScript, React, and Node.js.\nProject Mentorship: Guided students through semester-long UI/UX projects, including front-end development, API design, and database integration.\nDesign & Modeling Help: Assisted students in using UML for system modeling and behavioral analysis, including use case and interaction diagrams.\nTesting Support: Supported unit testing and UI testing procedures in JavaScript and helped students debug their web applications.\nCode Review & Feedback: Reviewed student submissions and provided feedback on usability, design consistency, and code efficiency.\nOffice Hours: Held weekly office hours to help students troubleshoot development issues and understand best practices in UI/UX design.\n\n\n\n\n\nUpon completing the course, students are expected to:\n\nDesign and implement interactive and accessible user interfaces.\nApply principles of HCI, UI design, UX testing, and event-driven architecture.\nUse industry-standard tools to build and evaluate responsive, full-stack applications.\nAnalyze and model system behavior using UML and interaction diagrams.\nDeploy and test applications with modern development environments and version control.\n\n\n\n\n\n\nDesign Principles for User Interfaces\n\nHuman-Computer Interaction (HCI) Fundamentals\n\nUX Testing and Evaluation\n\nHTML, CSS, JavaScript\n\nReact, Node.js, Express\n\nMongoDB, MySQL\n\nEvent-Driven Programming\n\nAPI & Framework Integration\n\nUML Diagrams (Structural, Behavioral, Interaction)\n\nUnit & UI Testing in JavaScript\n\nWindows-based UI Development\n\nClient/Server Architecture\n\n\n💡 Course Catalog | Email"
  },
  {
    "objectID": "teaching/coms1130/index.html",
    "href": "teaching/coms1130/index.html",
    "title": "COMS 1130",
    "section": "",
    "text": "Credits: 3\nInstitution: Iowa State University\nFormat: In-Person Course (Offered Fall & Spring)\nCOMS 1130 is a 3-credit undergraduate course that teaches essential skills in Microsoft Excel and Microsoft Access, with a focus on hands-on, real-world data projects.\n\n\n\nBy the end of this course, students will:\n\nApply Microsoft Excel and Access tools to solve real-world problems.\nConduct data analysis and modeling for business and industrial use cases.\nBuild, manage, and query spreadsheets and relational databases.\n\n\n\n\n\nMicrosoft Excel (Spreadsheets):\n\nCreate and format spreadsheets\n\nApply mathematical, statistical, logical, and lookup functions\n\nBuild charts and data visualizations\n\nUse PivotTables and subtotals for data summarization\n\nConduct What-If Analysis\n\nImport and export spreadsheet data\n\nMicrosoft Access (Databases):\n\nDesign relational tables and define relationships\n\nCreate forms, queries, and reports\n\nSort, filter, and update records\n\nImplement data validation\n\nPerform decision-making tasks based on database queries\n\n\n\n\n\nAs a Teaching Assistant for COMS 1130, I contributed to this large foundational course by supporting student learning in both lectures and lab sessions. My responsibilities included:\n\nLab Instruction & Demonstrations: Conducted in-person lab sessions covering Excel and Access applications in real-world scenarios.\n\nStudent Mentorship: Provided 1-on-1 guidance during lab hours and office hours, assisting students with assignments and projects.\n\nContent Support: Answered student queries through Canvas and email in a timely and supportive manner.\n\nAssessment & Feedback: Evaluated lab work and provided detailed feedback to improve student understanding.\n\nCourse Coordination: Collaborated with the instructor to enhance teaching materials and improve classroom engagement.\n\nThis role helped me strengthen my teaching, mentoring, and technical skills while supporting students in building core competencies that are essential across many academic and professional domains.\n\n💡 Course Catalog | Email"
  },
  {
    "objectID": "teaching/coms1130/index.html#coms-1130-introduction-to-spreadsheets-and-databases",
    "href": "teaching/coms1130/index.html#coms-1130-introduction-to-spreadsheets-and-databases",
    "title": "COMS 1130",
    "section": "",
    "text": "Credits: 3\nInstitution: Iowa State University\nFormat: In-Person Course (Offered Fall & Spring)\nCOMS 1130 is a 3-credit undergraduate course that teaches essential skills in Microsoft Excel and Microsoft Access, with a focus on hands-on, real-world data projects.\n\n\n\nBy the end of this course, students will:\n\nApply Microsoft Excel and Access tools to solve real-world problems.\nConduct data analysis and modeling for business and industrial use cases.\nBuild, manage, and query spreadsheets and relational databases.\n\n\n\n\n\nMicrosoft Excel (Spreadsheets):\n\nCreate and format spreadsheets\n\nApply mathematical, statistical, logical, and lookup functions\n\nBuild charts and data visualizations\n\nUse PivotTables and subtotals for data summarization\n\nConduct What-If Analysis\n\nImport and export spreadsheet data\n\nMicrosoft Access (Databases):\n\nDesign relational tables and define relationships\n\nCreate forms, queries, and reports\n\nSort, filter, and update records\n\nImplement data validation\n\nPerform decision-making tasks based on database queries\n\n\n\n\n\nAs a Teaching Assistant for COMS 1130, I contributed to this large foundational course by supporting student learning in both lectures and lab sessions. My responsibilities included:\n\nLab Instruction & Demonstrations: Conducted in-person lab sessions covering Excel and Access applications in real-world scenarios.\n\nStudent Mentorship: Provided 1-on-1 guidance during lab hours and office hours, assisting students with assignments and projects.\n\nContent Support: Answered student queries through Canvas and email in a timely and supportive manner.\n\nAssessment & Feedback: Evaluated lab work and provided detailed feedback to improve student understanding.\n\nCourse Coordination: Collaborated with the instructor to enhance teaching materials and improve classroom engagement.\n\nThis role helped me strengthen my teaching, mentoring, and technical skills while supporting students in building core competencies that are essential across many academic and professional domains.\n\n💡 Course Catalog | Email"
  },
  {
    "objectID": "teaching/coms4170/index.html",
    "href": "teaching/coms4170/index.html",
    "title": "COMS 4170",
    "section": "",
    "text": "COMS 4170 is an upper-level undergraduate/graduate course at Iowa State University, taught by Professor Myra Cohen, focusing on rigorous methods for software testing and quality assurance.\nI served as the Teaching Assistant for this course in Spring 2025, where I was responsible for:\n\nLeading weekly lab sessions and guiding students through practical exercises on test case generation and software testing tools.\nAssisting in course content delivery and reinforcing core topics such as black-box and white-box testing, test adequacy criteria, integration, and regression testing.\nDesigning and grading assignments and exams, ensuring alignment with pedagogical goals.\nProviding one-on-one mentoring to students to support their understanding of software quality assurance methodologies.\n\n\n\n\nPrinciples and methodologies of software testing\nTest design techniques: black-box and white-box\nTest models and adequacy criteria\nIntegration and system testing\nRegression testing strategies\nUse of automated software testing tools\nTesting management and process strategies\n\nTextbook: Introduction to Software Testing, 2nd edition by Paul Ammann and Jeff Offutt."
  },
  {
    "objectID": "teaching/coms4170/index.html#coms-4170-software-testing",
    "href": "teaching/coms4170/index.html#coms-4170-software-testing",
    "title": "COMS 4170",
    "section": "",
    "text": "COMS 4170 is an upper-level undergraduate/graduate course at Iowa State University, taught by Professor Myra Cohen, focusing on rigorous methods for software testing and quality assurance.\nI served as the Teaching Assistant for this course in Spring 2025, where I was responsible for:\n\nLeading weekly lab sessions and guiding students through practical exercises on test case generation and software testing tools.\nAssisting in course content delivery and reinforcing core topics such as black-box and white-box testing, test adequacy criteria, integration, and regression testing.\nDesigning and grading assignments and exams, ensuring alignment with pedagogical goals.\nProviding one-on-one mentoring to students to support their understanding of software quality assurance methodologies.\n\n\n\n\nPrinciples and methodologies of software testing\nTest design techniques: black-box and white-box\nTest models and adequacy criteria\nIntegration and system testing\nRegression testing strategies\nUse of automated software testing tools\nTesting management and process strategies\n\nTextbook: Introduction to Software Testing, 2nd edition by Paul Ammann and Jeff Offutt."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Kundan Kumar",
    "section": "",
    "text": "Gmail\n  \n  \n    \n     Github\n  \n  \n    \n     Linkedin\n  \n  \n    \n     YouTube\n  \n  \n    \n     Substack\n  \n  \n    \n     Scholar\n  \n\n\n\n\nI am a researcher focused on developing intelligent, secure, and adaptable AI systems for next-generation cyber-physical infrastructure. My work spans deep reinforcement learning (DRL), multi-agent systems, large language models (LLMs), safe and explainable AI, computer vision, and robotics, with real-world applications in smart energy systems, autonomous vehicles, and critical infrastructure.\nThe core of my Ph.D. research centers on developing physics-informed and safety-critical deep reinforcement learning (DRL) frameworks that embed domain knowledge and system constraints directly into the learning process. By incorporating physical laws, safety boundaries, and system dynamics into policy optimization, I design agents capable of making robust, interpretable, and reliable decisions in dynamic, high-stakes environments. My work addresses challenges such as uncertainty quantification, adversarial resilience, and safe exploration, while enabling agents to generalize across diverse network topologies, environmental conditions, and task distributions through advanced transfer learning and meta-learning techniques.\nI also leverage the CARLA simulator for autonomous driving research, combining computer vision, trajectory planning, and policy learning in complex traffic environments. My work integrates vision-based perception models for object detection, semantic segmentation, and sensor fusion, enabling robust situational awareness for autonomous agents. In parallel, I integrate LLM-based reasoning into simulation and control frameworks to support high-level planning, adaptive decision-making, and interactive human-AI collaboration for robotics and safety-critical control.\nBeyond autonomous and energy systems, my broader research interests include probabilistic modeling, statistical machine learning, and developing AI systems that are robust, trustworthy, and deployable in real-world complex environments.\nOther Research Interests:\n\nComputer Vision: Visual perception, object detection, semantic segmentation, sensor fusion for autonomous systems.\nSoftware Systems: Scalable software engineering, simulation framework development, real-time systems integration.\nStatistical Machine Learning: Uncertainty quantification, probabilistic modeling, data-driven inference in dynamic environments.\nRobotics: Learning-based control, adaptive planning, safe human-robot interaction, multi-modal robotic systems.\n\n\n\nNews\n\n\n\n\n\n\n[Jan 2025]\n\n\nOur paper on Transfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids has been accepted to IEEE PES Grid Edge Technologies Conference & Exposition 2025.\n\n\n\n\n[Aug 2024]\n\n\nOur paper on Workshop for High Performance Computing has been accepted at Pittsburgh Supercomputing Center 2024.\n\n\n\n\n[Jul 2024]\n\n\nOur paper on Bayesian Optimization for Deep Reinforcement Learning in Robust Volt-Var Control has been accepted to IEEE PES General Meeting 2024.\n\n\n\n\n[Nov 2023]\n\n\nOur paper Deep RL-based Volt-VAR Control and Attack Resiliency for DER-Integrated Distribution Grids was accepted to IEEE ISGT 2024.\n\n\n\n\n[Aug 2022]\n\n\nParticipated in the Oxford Machine Learning Summer School, completing tracks in MLx Health and MLx Finance.\n\n\n\n\n[Apr 2022]\n\n\nOur paper on Pattern-Based Multivariate Regression using Deep Learning (PBMR-DP) was accepted to ICMLA 2022."
  },
  {
    "objectID": "teaching/teaching1.html",
    "href": "teaching/teaching1.html",
    "title": "Teaching",
    "section": "",
    "text": "As a Teaching Assistant at Iowa State University, I am dedicated to creating an inclusive and dynamic learning environment that enables students to excel in computer science. My teaching approach emphasizes practical application of theoretical concepts, encouraging students to develop hands-on experience in software development, user interface design, and database management. I believe in preparing students for real-world challenges in the technology industry through project-based learning and personalized support.\nCourses I have served as a Teaching Assistant or Mentor in:"
  },
  {
    "objectID": "teaching/teaching1.html#class-1",
    "href": "teaching/teaching1.html#class-1",
    "title": "Teaching",
    "section": "Class 1",
    "text": "Class 1\n\n\n\n\n\nFall 2024,   Fall 2023\nClass 1 introduction."
  },
  {
    "objectID": "teaching/teaching1.html#class-2",
    "href": "teaching/teaching1.html#class-2",
    "title": "Teaching",
    "section": "Class 2",
    "text": "Class 2\n\n\n\n\n\nSpring 2025,   Fall 2023\nClass 2 introduction."
  },
  {
    "objectID": "teaching/teaching1.html#class-3",
    "href": "teaching/teaching1.html#class-3",
    "title": "Teaching",
    "section": "Class 3",
    "text": "Class 3\n\n\n\n\n\nSpring 2025,   Spring 2024\nClass 3 introduction.\n\n\n\nFuture Goals\n\nI am committed to continually improving teaching methods and curriculum design. My goals include:\n\nIncorporating emerging AI and software development tools into teaching.\nCreating interactive, hands-on, project-based learning environments.\nContributing to the computer science education community through curriculum research and mentoring.\nExpanding opportunities for students to apply theory to real-world computing challenges.\n\nFeel free to reach out if you would like to collaborate or learn more about my teaching and mentorship activities."
  },
  {
    "objectID": "teaching/teaching1.html#future-goals",
    "href": "teaching/teaching1.html#future-goals",
    "title": "Teaching",
    "section": "Future Goals",
    "text": "Future Goals\nI am committed to continually improving teaching methods and curriculum design. My goals include:\n\nIncorporating emerging AI and software development tools into teaching.\nCreating interactive, hands-on, project-based learning environments.\nContributing to the computer science education community through curriculum research and mentoring.\nExpanding opportunities for students to apply theory to real-world computing challenges.\n\nReach out via email if you’d like to collaborate or learn more."
  },
  {
    "objectID": "teaching/teaching1.html#coms-1130-introduction-to-spreadsheets-and-databases",
    "href": "teaching/teaching1.html#coms-1130-introduction-to-spreadsheets-and-databases",
    "title": "Teaching",
    "section": "COMS 1130 – Introduction to Spreadsheets and Databases",
    "text": "COMS 1130 – Introduction to Spreadsheets and Databases\n\n\n\n\n\nSpring 2025\nAn introductory course that teaches problem solving through programming in Python. Focuses on functions, control flow, loops, data structures, and basic algorithms."
  },
  {
    "objectID": "teaching/teaching1.html#class-coms-1130-introduction-to-python",
    "href": "teaching/teaching1.html#class-coms-1130-introduction-to-python",
    "title": "Teaching",
    "section": "Class: COMS 1130 – Introduction to Python",
    "text": "Class: COMS 1130 – Introduction to Python\n\n\n\nPython\n\n\nSpring 2025, Fall 2023\nAn introductory course that teaches problem solving through programming in Python. Focuses on functions, control flow, loops, data structures, and basic algorithms."
  },
  {
    "objectID": "teaching/teaching1.html#class-coms-3630-database-management-systems",
    "href": "teaching/teaching1.html#class-coms-3630-database-management-systems",
    "title": "Teaching",
    "section": "Class: COMS 3630 – Database Management Systems",
    "text": "Class: COMS 3630 – Database Management Systems\n\n\n\nDBMS\n\n\nSpring 2024, Fall 2023\nA comprehensive introduction to database theory and applications. Students design schemas, write complex queries, and build database-backed web apps."
  },
  {
    "objectID": "teaching/teaching1.html#class-coms-3620-software-engineering-fundamentals",
    "href": "teaching/teaching1.html#class-coms-3620-software-engineering-fundamentals",
    "title": "Teaching",
    "section": "Class: COMS 3620 – Software Engineering Fundamentals",
    "text": "Class: COMS 3620 – Software Engineering Fundamentals\n\n\n\nSoftware\n\n\nFall 2024\nCovers software development lifecycle, UML modeling, version control, Agile methodology, and teamwork skills with real-world software projects."
  },
  {
    "objectID": "teaching/teaching1.html#coms-1130-introduction-to-python",
    "href": "teaching/teaching1.html#coms-1130-introduction-to-python",
    "title": "Teaching",
    "section": "COMS 1130 – Introduction to Python",
    "text": "COMS 1130 – Introduction to Python\n\n\n\nSpring 2025, Fall 2023\nAn introductory course that teaches problem solving through programming in Python. Focuses on functions, control flow, loops, data structures, and basic algorithms."
  },
  {
    "objectID": "teaching/teaching1.html#coms-3630-database-management-systems",
    "href": "teaching/teaching1.html#coms-3630-database-management-systems",
    "title": "Teaching",
    "section": "COMS 3630 – Database Management Systems",
    "text": "COMS 3630 – Database Management Systems\n\n\n\n\n\nSpring 2024\nCovers the fundamentals of database architecture, data modeling, and SQL. Students gain hands-on experience with relational databases and explore indexing, transactions, normalization, and database design."
  },
  {
    "objectID": "teaching/teaching1.html#coms-3620-software-engineering-fundamentals",
    "href": "teaching/teaching1.html#coms-3620-software-engineering-fundamentals",
    "title": "Teaching",
    "section": "COMS 3620 – Software Engineering Fundamentals",
    "text": "COMS 3620 – Software Engineering Fundamentals\n\n\n\nFall 2024\nIntroduces core software engineering concepts including Agile development, UML modeling, and teamwork through large-scale projects."
  },
  {
    "objectID": "teaching/teaching1.html#coms-3630-database-management-systems-1",
    "href": "teaching/teaching1.html#coms-3630-database-management-systems-1",
    "title": "Teaching",
    "section": "COMS 3630 – Database Management Systems",
    "text": "COMS 3630 – Database Management Systems\n\n\n\nSpring 2024, Fall 2023\nCovers theory and practice of relational databases. Students design schemas, write complex SQL queries, and integrate databases with full-stack apps."
  },
  {
    "objectID": "teaching/teaching1.html#coms-3630-database-management-systems-2",
    "href": "teaching/teaching1.html#coms-3630-database-management-systems-2",
    "title": "Teaching",
    "section": "COMS 3630 – Database Management Systems",
    "text": "COMS 3630 – Database Management Systems\n\n\n\nSpring 2024, Fall 2023\nCovers theory and practice of relational databases. Students design schemas, write complex SQL queries, and integrate databases with full-stack apps."
  },
  {
    "objectID": "teaching/teaching1.html#coms-3630-database-management-systems-3",
    "href": "teaching/teaching1.html#coms-3630-database-management-systems-3",
    "title": "Teaching",
    "section": "COMS 3630 – Database Management Systems",
    "text": "COMS 3630 – Database Management Systems\n\n\n\nSpring 2024, Fall 2023\nCovers theory and practice of relational databases. Students design schemas, write complex SQL queries, and integrate databases with full-stack apps."
  },
  {
    "objectID": "teaching/teaching1.html#ccoms-3620-object-oriented-analysis-and-design",
    "href": "teaching/teaching1.html#ccoms-3620-object-oriented-analysis-and-design",
    "title": "Teaching",
    "section": "CCOMS 3620 – Object-Oriented Analysis and Design",
    "text": "CCOMS 3620 – Object-Oriented Analysis and Design\n\n\n\nFall 2024\nIntroduces core software engineering concepts including Agile development, UML modeling, and teamwork through large-scale projects."
  },
  {
    "objectID": "teaching/teaching1.html#coms-3090-software-development-practices",
    "href": "teaching/teaching1.html#coms-3090-software-development-practices",
    "title": "Teaching",
    "section": "COMS 3090 – Software Development Practices",
    "text": "COMS 3090 – Software Development Practices\n\n\n\n\n\nSpring 2024,   Fall 2023\nHands-on course focused on modern software development practices, including Git, Agile methodologies, code reviews, unit testing, and CI/CD. Students engage in team-based projects that simulate real-world workflows, emphasizing collaboration, documentation, and iterative development."
  },
  {
    "objectID": "teaching/teaching1.html#coms-3190-user-interface-design",
    "href": "teaching/teaching1.html#coms-3190-user-interface-design",
    "title": "Teaching",
    "section": "COMS 3190 – User Interface Design",
    "text": "COMS 3190 – User Interface Design\n\n\n\n\n\nSpring 2024\nExplores the principles of designing user-friendly and efficient interfaces. Combines hardware and data-driven UI practices, integrating Raspberry Pi for IoT applications and Nodejs for fronted visualization and interaction."
  },
  {
    "objectID": "teaching/teaching1.html#coms-3620-object-oriented-analysis-and-design",
    "href": "teaching/teaching1.html#coms-3620-object-oriented-analysis-and-design",
    "title": "Teaching",
    "section": "COMS 3620 – Object-Oriented Analysis and Design",
    "text": "COMS 3620 – Object-Oriented Analysis and Design\n\n\n\n\n\nSpring 2024,   Fall 2023\nTeaches how to model and design software systems using object-oriented principles. Emphasis is on UML diagrams, design patterns, and implementation in Java to build modular, maintainable, and scalable software."
  },
  {
    "objectID": "teaching/teaching1.html#coms-3630-introduction-to-database-management-systems",
    "href": "teaching/teaching1.html#coms-3630-introduction-to-database-management-systems",
    "title": "Teaching",
    "section": "COMS 3630 – Introduction to Database Management Systems",
    "text": "COMS 3630 – Introduction to Database Management Systems\n\n\n\n\n\nSpring 2024\nComprehensive introduction to relational databases including SQL, schema design, normalization, transactions, and database-backed apps."
  },
  {
    "objectID": "teaching/teaching1.html#ccoms-4170-software-testing",
    "href": "teaching/teaching1.html#ccoms-4170-software-testing",
    "title": "Teaching",
    "section": "CCOMS 4170 – Software Testing",
    "text": "CCOMS 4170 – Software Testing\n\n\n\nSpring 2025\nIntroduces core software engineering concepts including Agile development, UML modeling, and teamwork through large-scale projects."
  },
  {
    "objectID": "teaching/teaching1.html#coms-4170-software-testing",
    "href": "teaching/teaching1.html#coms-4170-software-testing",
    "title": "Teaching",
    "section": "COMS 4170 – Software Testing",
    "text": "COMS 4170 – Software Testing\n\n\n\n\n\nFall 2024,   Fall 2023\nFocuses on the principles and practices of software verification and validation. Students learn to design and execute test cases, use debugging tools, and apply automation frameworks to ensure software quality and reliability."
  },
  {
    "objectID": "teaching/teaching1.html#coms-4170-software-testing-1",
    "href": "teaching/teaching1.html#coms-4170-software-testing-1",
    "title": "Teaching",
    "section": "COMS 4170 – Software Testing",
    "text": "COMS 4170 – Software Testing\n\n\n\n\n\nFall 2024,   Fall 2023\nIntroduces core software engineering concepts including Agile development, UML modeling, and teamwork through large-scale projects."
  },
  {
    "objectID": "teaching/teaching1.html#coms-4170-introduction-to-database-management-systems",
    "href": "teaching/teaching1.html#coms-4170-introduction-to-database-management-systems",
    "title": "Teaching",
    "section": "COMS 4170 – Introduction to Database Management Systems",
    "text": "COMS 4170 – Introduction to Database Management Systems\n\n\n\n\n\nFall 2024,   Fall 2023\nIntroduces core software engineering concepts including Agile development, UML modeling, and teamwork through large-scale projects."
  },
  {
    "objectID": "teaching/teaching1.html#coms-1130-spreadsheets-and-databases",
    "href": "teaching/teaching1.html#coms-1130-spreadsheets-and-databases",
    "title": "Teaching",
    "section": "COMS 1130 – Spreadsheets and Databases",
    "text": "COMS 1130 – Spreadsheets and Databases\n\n\n\n\n\nSpring 2025\nFoundational course in using tools like Microsoft Excel and Access for data organization, analysis, and reporting. Students learn formulas, pivot tables, queries, and form design for practical business and data applications."
  },
  {
    "objectID": "teaching/teaching.html#coms-1130-spreadsheets-and-databases",
    "href": "teaching/teaching.html#coms-1130-spreadsheets-and-databases",
    "title": "Teaching",
    "section": "COMS 1130 – Spreadsheets and Databases",
    "text": "COMS 1130 – Spreadsheets and Databases\n\n\n\n\n\nFall 2016\nFoundational course in using tools like Microsoft Excel and Access for data organization, analysis, and reporting. Students learn formulas, pivot tables, queries, and form design for practical business and data applications."
  },
  {
    "objectID": "teaching/teaching.html#coms-3090-software-development-practices",
    "href": "teaching/teaching.html#coms-3090-software-development-practices",
    "title": "Teaching",
    "section": "COMS 3090 – Software Development Practices",
    "text": "COMS 3090 – Software Development Practices\n\n\n\n\n\nFall 2022,   Spring 2023, ,   Spring 2024\nHands-on course focused on modern software development practices, including Git, Agile methodologies, code reviews, unit testing, and CI/CD. Students engage in team-based projects that simulate real-world workflows, emphasizing collaboration, documentation, and iterative development."
  },
  {
    "objectID": "teaching/teaching.html#coms-3190-user-interface-design",
    "href": "teaching/teaching.html#coms-3190-user-interface-design",
    "title": "Teaching",
    "section": "COMS 3190 – User Interface Design",
    "text": "COMS 3190 – User Interface Design\n\n\n\n\n\nFall 2023\nExplores the principles of designing user-friendly and efficient interfaces. Combines hardware and data-driven UI practices, integrating Raspberry Pi for IoT applications and Nodejs for fronted visualization and interaction."
  },
  {
    "objectID": "teaching/teaching.html#coms-3620-object-oriented-analysis-and-design",
    "href": "teaching/teaching.html#coms-3620-object-oriented-analysis-and-design",
    "title": "Teaching",
    "section": "COMS 3620 – Object-Oriented Analysis and Design",
    "text": "COMS 3620 – Object-Oriented Analysis and Design\n\n\n\n\n\nFall 2020,   Spring 2021,   Fall 2021,   Fall 2024\nTeaches how to model and design software systems using object-oriented principles. Emphasis is on UML diagrams, design patterns, and implementation in Java to build modular, maintainable, and scalable software."
  },
  {
    "objectID": "teaching/teaching.html#coms-3630-database-management-systems",
    "href": "teaching/teaching.html#coms-3630-database-management-systems",
    "title": "Teaching",
    "section": "COMS 3630 – Database Management Systems",
    "text": "COMS 3630 – Database Management Systems\n\n\n\n\n\nSpring 2022\nCovers the fundamentals of database architecture, data modeling, and SQL. Students gain hands-on experience with relational databases and explore indexing, transactions, normalization, and database design."
  },
  {
    "objectID": "teaching/teaching.html#coms-4170-software-testing",
    "href": "teaching/teaching.html#coms-4170-software-testing",
    "title": "Teaching",
    "section": "COMS 4170 – Software Testing",
    "text": "COMS 4170 – Software Testing\n\n\n\n\n\nSpring 2025\nFocuses on the principles and practices of software verification and validation. Students learn to design and execute test cases, use debugging tools, and apply automation frameworks to ensure software quality and reliability."
  },
  {
    "objectID": "teaching/teaching.html#future-goals",
    "href": "teaching/teaching.html#future-goals",
    "title": "Teaching",
    "section": "Future Goals",
    "text": "Future Goals\nI am committed to continually improving teaching methods and curriculum design. My goals include:\n\nIncorporating emerging AI and software development tools into teaching.\nCreating interactive, hands-on, project-based learning environments.\nContributing to the computer science education community through curriculum research and mentoring.\nExpanding opportunities for students to apply theory to real-world computing challenges.\n\nReach out via email if you’d like to collaborate or learn more."
  },
  {
    "objectID": "projects/projects1.html",
    "href": "projects/projects1.html",
    "title": "Projects",
    "section": "",
    "text": "Data Apothecary’s Notes\n\n\n\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\n\n\n\n\n\n\n\n\n\n\n\nMortality Surveillance in Norway\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoreden\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/projects1.html#large-language-models",
    "href": "projects/projects1.html#large-language-models",
    "title": "Projects",
    "section": "",
    "text": "contents: projects/llm type: grid fields: [image, title, description] sort: “date desc” image-align: right image-height: 100% :::"
  },
  {
    "objectID": "projects/projects1.html#deep-reinforcement-learning",
    "href": "projects/projects1.html#deep-reinforcement-learning",
    "title": "Projects",
    "section": "🚀 Deep Reinforcement Learning",
    "text": "🚀 Deep Reinforcement Learning\n\ncontents: projects/drl type: grid fields: [image, title, description] sort: “date desc” image-align: left image-height: 100%\n\n:::"
  },
  {
    "objectID": "projects/projects1.html#deepmachine-learning",
    "href": "projects/projects1.html#deepmachine-learning",
    "title": "Projects",
    "section": "🧠 Deep/Machine Learning",
    "text": "🧠 Deep/Machine Learning\n\n```nctuuakgt contents: projects/dl type: grid fields: [image, title, description] sort: “date desc” image-align: left image-height: 100%\n\n:::"
  },
  {
    "objectID": "projects/projects1.html#statistics",
    "href": "projects/projects1.html#statistics",
    "title": "Projects",
    "section": "📊 Statistics",
    "text": "📊 Statistics\n\n```nctuuakgt contents: projects/statistics type: grid fields: [image, title, description] sort: “date desc” image-align: left image-height: 100%\n\n:::"
  },
  {
    "objectID": "projects/projects1.html#computer-vision",
    "href": "projects/projects1.html#computer-vision",
    "title": "Projects",
    "section": "👁️‍🗨️ Computer Vision",
    "text": "👁️‍🗨️ Computer Vision\n\n```nctuuakgt contents: projects/cv type: grid fields: [image, title, description] sort: “date desc” image-align: left image-height: 100%\n\n:::"
  },
  {
    "objectID": "projects/projects1.html#robotics",
    "href": "projects/projects1.html#robotics",
    "title": "Projects",
    "section": "🤖 Robotics",
    "text": "🤖 Robotics\n\n```nctuuakgt contents: projects/robotics type: grid fields: [image, title, description] sort: “date desc” image-align: left image-height: 100%\n\n:::"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "📄 Resume"
  },
  {
    "objectID": "cv.html#professional-experience",
    "href": "cv.html#professional-experience",
    "title": "Curriculum Vitae",
    "section": " Professional Experience",
    "text": "Professional Experience\n\n\n\n\n\nNational Renewable Energy Laboratory (NREL)\n\n\n\n\n\nMachine Learning Engineer (Intern)\n\n\n May 2024  —  Jan 2025 \n\n\n\n\nDeveloped novel machine learning models for automated network topology inference and resilient control policy optimization for complex distributed systems under extreme scenarios\nDesigned and developed semi‑supervised learning approaches to tackle the challenge of limited labeled data in networks, achieving 98% improvement in model accuracy with varying labeled data.\nPaper ”Advanced Semi‑Supervised Learning with Uncertainty Estimation for Phase Identification in Distribution Systems” accepted at IEEE Power & Energy Society General Meeting (PES GM) 2025.\n\n\n\n\n\n\n\nComcast\n\n\n\n\n\nSoftware Engineer\n\n\n Jul 2019  —  Feb 2020 \n\n\n\n\nDesigned and implemented real‑time data processing pipelines using Amazon Kinesis and RabbitMQ, processing 1TB+ daily data for fraud detection and system monitoring.\nDeveloped machine learning models for anomaly detection and user behavior analysis, reducing fraudulent activities by 70% through predictive analytics.\nBuilt scalable Spring Boot microservices handling 10K+ concurrent requests, achieving 99.9% uptime for critical system components.\nCreated interactive dashboards using Presto DB and Python visualization tools, enabling real‑time monitoring of network performance metrics and fraud patterns.\n\n\n\n\n\n\n\nIBM\n\n\n\n\n\nSoftware Engineer\n\n\n Jan 2019  —  Jun 2019 \n\n\n\n\nLed cloud infrastructure optimization using OpenShift, implementing auto‑scaling solutions that reduced operational costs by 30%.\nDeveloped a comprehensive monitoring system using Grafana and Flask, providing real‑time visibility into 100+ cloud servers.\nImplemented automated performance monitoring and alerting system, reducing incident response time by 60%.\n\n\n\n\n\n\n\nHewlett Packard Enterprise (HPE)\n\n\n\n\n\nSoftware Engineer\n\n\n Apr 2017  —  Dec 2018 \n\n\n\n\n\nSpearheaded migration of critical applications from HPI to HPE domain, ensuring zero downtime during transition.\nImplemented OAuth 2.0 authentication system and RESTful services using Spring Boot, securing applications serving 50K+ users.\nDesigned and deployed microservices architecture on Apache/WebLogic servers, improving system response time by 40%.\n\n\n\n\n\n\n\nTata Consultancy Services (TCS)\n\n\n\n\n\nSystem Engineer\n\n\n Jul 2012  —  Dec 2015 \n\n\n\n\n\nEngineered high‑performance ETL pipelines for data warehouse integration, processing 100GB+ daily data volumes.\nOptimized database performance through advanced SQL tuning and indexing strategies, reducing query execution time by 70%.\nReceived excellence award for achieving $100K cost savings through database optimization initiatives."
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": " Education",
    "text": "Education\n\n\n\n\n\nIowa State University\n\n\n\n\n\nPh.D. in Computer Science (Minor: Statistics)\n\n\n 2020  —  2025 (Expected) \n\n\n\n\n\nResearch: Deep RL, Physics-Informed AI, Uncertainty Quantification\nCourses: Deep Learning, NLP, Statistical Theory, Empirical Methods, Algorithms\n\n\n\n\nMS in Computer Science\n\n\n Jan 2015  —  Dec 2016 \n\n\n\n\nFocus: Algorithms, Databases, Network Programming"
  },
  {
    "objectID": "cv.html#communities-and-organizations",
    "href": "cv.html#communities-and-organizations",
    "title": "Curriculum Vitae",
    "section": "{{< fa user-group >}} Communities and Organizations",
    "text": "{{&lt; fa user-group &gt;}} Communities and Organizations\n Posit Data Science Hangout\n R4DS Learning Community\n Women in GovTech\n R-Ladies Seattle\n Seattle useR Group"
  },
  {
    "objectID": "cv.html#extension-scientific-writing",
    "href": "cv.html#extension-scientific-writing",
    "title": "Curriculum Vitae",
    "section": "{{< fa book >}} Extension & Scientific Writing",
    "text": "{{&lt; fa book &gt;}} Extension & Scientific Writing\nRyan, J.N., Michel, L., Gelardi, D.G. 2023. Standard Operating Procedure: Soil Health Monitoring in Washington State. Washington State Department of Agriculture, Natural Resources Assessment Section: Olympia, WA. Publication No. 102-923.\nNoland, K., Nickelson, A., Ryan, J.N., Drennan, M. 2021. Ambient Monitoring for Pesticides in Washington State Surface Water: 2019 Technical Report. Washington State Department of Agriculture, Natural Resources Assessment Section: Olympia, WA. Publication No. 102-629.\n\nAlso co-author for 2018 and 2017 Technical Reports.\n\nHancock, J., Ryan, J.N. 2021. Pesticide Groundwater Sampling in the Sumas-Blaine Surficial Aquifer. Washington State Department of Agriculture, Natural Resources Assessment Section: Olympia, WA. Publication No. 103-895.\nTuttle, G., Noland, K., Ryan, J.N. 2020. Glyphosate: Ecological Fate and Effects and Human Health Summary. Washington State Department of Agriculture, Natural Resources Assessment Section: Olympia, WA. Publication No. 809-817.\nRyan, J.N. 2020. Understanding Pesticide Leaching Potential and Protecting Groundwater. Washington State Department of Agriculture, Natural Resources Assessment Section: Olympia, WA. Publication No. 809-865.\nRyan, J.N. 2020. Pesticide Application and Water Quality. Washington State Department of Agriculture, Natural Resources Assessment Section: Olympia, WA. Publication No. 102-850.\nRyan, J.N. 2020. Understanding Pesticide Product Labels. Washington State Department of Agriculture, Natural Resources Assessment Section: Olympia, WA. Publication No. 102-849."
  },
  {
    "objectID": "cv.html#presentations",
    "href": "cv.html#presentations",
    "title": "Curriculum Vitae",
    "section": " Presentations",
    "text": "Presentations\n\nUpcoming\n\nParameterized Reporting using Quarto (Invited) — R-Ladies DC (Jan 2024)\n\n\n\nPast\n\nParameterized Quarto Reports — posit::conf(2023), Chicago"
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "Curriculum Vitae",
    "section": " Skills",
    "text": "Skills\n\n\n\nProgramming Languages\n\n\nPython, R, Java, C++, SAS, MATLAB, SQL, HTML, JS, Node.js, React.js\n\n\n\n\nML & Data Analysis\n\n\nscikit-learn, TensorFlow, PyTorch, Pandas, Matplotlib, Seaborn, Gym, RLlib\n\n\n\n\nLLMs & NLP\n\n\nHugging Face Transformers, LangChain, RAG, Prompt Engineering\n\n\n\n\nHPC & Big Data\n\n\nHadoop, Hive, Spark, Kafka, Kinesis, SLURM, MPI, OpenMP\n\n\n\n\nSimulation & Modeling\n\n\nOpal-RT, OpenDSS (Power), Carla (Autonomous Driving)\n\n\n\n\nOptimization\n\n\nGurobi, Pyomo, BoTorch, Optuna, Hyperopt\n\n\n\n\nVisualization & GIS\n\n\nTableau, ArcGIS, Leaflet\n\n\n\n\nCloud & DevOps\n\n\nAWS (EC2, S3, Lambda), GCP, Docker, Kubernetes, Git, Terraform, Jenkins, CircleCI"
  },
  {
    "objectID": "cv.html#footnotes",
    "href": "cv.html#footnotes",
    "title": "Curriculum Vitae",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI completed half of my classes at high school and the other half at community college.↩︎"
  },
  {
    "objectID": "cv.html#communities-organizations",
    "href": "cv.html#communities-organizations",
    "title": "\nCurriculum Vitae\n",
    "section": " Communities & Organizations",
    "text": "Communities & Organizations\n\n R4DS Learning Community\n Women in GovTech\n R-Ladies Seattle"
  },
  {
    "objectID": "cv.html#scientific-writing",
    "href": "cv.html#scientific-writing",
    "title": "\nCurriculum Vitae\n",
    "section": " Scientific Writing",
    "text": "Scientific Writing\n\nRyan, J.N., Michel, L., et al. Soil Health Monitoring SOP. Publication 102-923\nRyan, J.N. 2020. Pesticide Application and Water Quality. Publication 102-850"
  },
  {
    "objectID": "about_me.html#hi-there",
    "href": "about_me.html#hi-there",
    "title": "Kundan Kumar",
    "section": "Hi there!",
    "text": "Hi there!\nI am a researcher focused on developing intelligent, secure, and adaptable AI systems for next-generation cyber-physical infrastructure. My work spans deep reinforcement learning (DRL), multi-agent systems, large language models (LLMs), safe and explainable AI, computer vision, and robotics, with real-world applications in smart energy systems, autonomous vehicles, and critical infrastructure.\nThe core of my Ph.D. research centers on developing physics-informed and safety-critical deep reinforcement learning (DRL) frameworks that embed domain knowledge and system constraints directly into the learning process. By incorporating physical laws, safety boundaries, and system dynamics into policy optimization, I design agents capable of making robust, interpretable, and reliable decisions in dynamic, high-stakes environments. My work addresses challenges such as uncertainty quantification, adversarial resilience, and safe exploration, while enabling agents to generalize across diverse network topologies, environmental conditions, and task distributions through advanced transfer learning and meta-learning techniques.\nI also leverage the CARLA simulator for autonomous driving research, combining computer vision, trajectory planning, and policy learning in complex traffic environments. My work integrates vision-based perception models for object detection, semantic segmentation, and sensor fusion, enabling robust situational awareness for autonomous agents. In parallel, I integrate LLM-based reasoning into simulation and control frameworks to support high-level planning, adaptive decision-making, and interactive human-AI collaboration for robotics and safety-critical control.\nBeyond autonomous and energy systems, my broader research interests include probabilistic modeling, statistical machine learning, and developing AI systems that are robust, trustworthy, and deployable in real-world complex environments.\nOther Research Interests:\n\nComputer Vision: Visual perception, object detection, semantic segmentation, sensor fusion for autonomous systems.\nSoftware Systems: Scalable software engineering, simulation framework development, real-time systems integration.\nStatistical Machine Learning: Uncertainty quantification, probabilistic modeling, data-driven inference in dynamic environments.\nRobotics: Learning-based control, adaptive planning, safe human-robot interaction, multi-modal robotic systems.\n\n\n\nNews\n\n\n\n\n\n\n[Jan 2025]\n\n\nOur paper on Transfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids has been accepted to IEEE PES Grid Edge Technologies Conference & Exposition 2025.\n\n\n\n\n[Aug 2024]\n\n\nOur paper on Workshop for High Performance Computing has been accepted at Pittsburgh Supercomputing Center 2024.\n\n\n\n\n[Jul 2024]\n\n\nOur paper on Bayesian Optimization for Deep Reinforcement Learning in Robust Volt-Var Control has been accepted to IEEE PES General Meeting 2024.\n\n\n\n\n[Nov 2023]\n\n\nOur paper Deep RL-based Volt-VAR Control and Attack Resiliency for DER-Integrated Distribution Grids was accepted to IEEE ISGT 2024.\n\n\n\n\n[Aug 2022]\n\n\nParticipated in the Oxford Machine Learning Summer School, completing tracks in MLx Health and MLx Finance.\n\n\n\n\n[Apr 2022]\n\n\nOur paper on Pattern-Based Multivariate Regression using Deep Learning (PBMR-DP) was accepted to ICMLA 2022."
  },
  {
    "objectID": "cv.html#honors-awards",
    "href": "cv.html#honors-awards",
    "title": "Curriculum Vitae",
    "section": " Honors & Awards",
    "text": "Honors & Awards\n\nSelected, Seventh Workshop on Autonomous Energy Systems @ NREL (2024)\nSelected, ByteBoost Workshop on Accelerating HPC Research Skills (2024)\nSelected, Oxford Machine Learning Summer School (OxML) (2022)\nExcellence Award, Database Optimization @ TCS\n2nd Place, BAJA SAE India (Safest Terrain Vehicle Category, National Level)"
  },
  {
    "objectID": "cv.html#service",
    "href": "cv.html#service",
    "title": "Curriculum Vitae",
    "section": " Service",
    "text": "Service\n\nReviewer:\n\nIEEE Transactions on Industrial Informatics (2025)\nConference on Neural Information Processing Systems (Ethics)(2025)\nIEEE Transactions on Neural Networks and Learning Systems (2024)\nIEEE PES GM, Grid Edge & ISGT (2023, 2024)\n\nMock Interviewer: Supporting underrepresented minorities in tech.\nVolunteer, Prayaas India (BIT): NGO providing quality education to underprivileged children in slums and villages."
  },
  {
    "objectID": "cv.html#teaching-experience",
    "href": "cv.html#teaching-experience",
    "title": "Curriculum Vitae",
    "section": " Teaching Experience",
    "text": "Teaching Experience\n\n\n\n\n\nIowa State University\n\n\n\n\n\nTeaching Assistant\n\n\n 2020  —  2025 \n\n\n\nDepartment of Computer Science\n\nSupported undergraduate/graduate courses including Software Development Practices, Database Systems, and Spreadsheets.\nLed weekly lab sessions, assisted students with debugging and conceptual challenges, and held office hours.\nDesigned assignments and quizzes aligned with real-world workflows and agile development practices.\nMentored students on semester-long capstone projects simulating software engineering team experiences."
  },
  {
    "objectID": "cv.html#research-experience",
    "href": "cv.html#research-experience",
    "title": "Curriculum Vitae",
    "section": " Research Experience",
    "text": "Research Experience\n\n\n\n\n\nIowa State University\n\n\n\n\n\nResearch Assistant\n\n\n Aug 2022  —  Jul 2025 \n\n\n\n\nResearch on Physics‑Informed Deep Reinforcement Learning for Critical Infrastructure Systems, focusing on Intelligent Resource Management and Security in Large‑Scale Distributed Networks.\nApplied computational deep reinforcement learning algorithms in a Smart Energy System to analyze power simulation data, minimizing voltage violations, power loss, and control errors.\nDeveloped physics‑informed DRL algorithms incorporating domain‑specific physical constraints, achieving 30% improvement in resource allocation efficiency and reducing system violations in complex distributed networks.\nDesigned and implemented adversarial attack detection and mitigation frameworks for AI models in critical systems, enhancing robustness against security threats through systematic testing and defensive techniques.\nCreated novel transfer learning methodologies enabling DRL models to adapt across varying network sizes and topologies, reducing training time by 40% for new configurations.\nDeveloped Python‑based simulation and control framework integrating real‑time hardware (OPAL‑RT and OpenDSS) with distributed systems.\nLeveraged LLM‑driven reasoning and contextual understanding within simulation environments to support real‑time adaptive control, human‑AI collaboration, and predictive system optimization.\n\n\n\n\nResearch Assistant\n\n\n Aug 2020  —  Jul 2022 \n\n\n\n\nResearch on Deep Reinforcement Learning (DRL) and Safety‑Critical Learning for Autonomous Systems, with focus on perception, control, and decision‑making in high‑stakes environments.\nUtilized CARLA simulator for vision‑based autonomous driving tasks, including perception, object detection, trajectory planning, and policy learning in complex traffic scenarios.\nApplied deep computer vision models for object recognition, semantic segmentation, and sensor fusion, enabling robust situational awareness in autonomous driving and robotics."
  },
  {
    "objectID": "cv.html#teaching-experience-1",
    "href": "cv.html#teaching-experience-1",
    "title": "Curriculum Vitae",
    "section": " Teaching Experience",
    "text": "Teaching Experience\n\n\n\n\n\nIowa State University – Department of Computer ScienceAug 2020 – Present\n\n\nGraduate Teaching Assistant\n\nSupported undergraduate/graduate courses including Software Development Practices, Database Systems, and Spreadsheets.\nLed weekly lab sessions, assisted students with debugging and conceptual challenges, and held office hours.\nDesigned assignments and quizzes aligned with real-world workflows and agile development practices.\nMentored students on semester-long capstone projects simulating software engineering team experiences."
  },
  {
    "objectID": "cv.html#projects",
    "href": "cv.html#projects",
    "title": "Curriculum Vitae",
    "section": " Projects",
    "text": "Projects\n\n\n\n\n\n\n   Fast Mixed‑Logit Estimation Fast estimation of mixed logit models with preference‑space utility.  \n   cbcTools Suite Designing choice‑based conjoint experiments and power and system analyses.  \n   LLM‑Powered Energy Optimizer Multi‑building energy optimization in CityLearn with LLM guidance.  \n\n\n 📁 Check My Projects"
  },
  {
    "objectID": "cv.html#projects-1",
    "href": "cv.html#projects-1",
    "title": "Curriculum Vitae",
    "section": " Projects",
    "text": "Projects\n\n\n\n\n\n\n   Fast Mixed‑Logit Estimation Fast estimation of mixed logit models with preference‑space utility.  \n   cbcTools Suite Designing choice‑based conjoint experiments and power analyses.  \n   LLM‑Powered Energy Optimizer Multi‑building energy optimization in CityLearn with LLM guidance."
  },
  {
    "objectID": "cv.html#projects-2",
    "href": "cv.html#projects-2",
    "title": "Curriculum Vitae",
    "section": " Projects",
    "text": "Projects\n\n\n   Fast estimation of mixed logit models with preference‑space utility \n   Tools for designing choice‑based conjoint experiments and power analyses. \n   LLM-Powered Multi-Building Energy Optimization in CityLearn LLM-Powered Multi-Building Energy Optimization in CityLearn"
  },
  {
    "objectID": "cv.html#projects-3",
    "href": "cv.html#projects-3",
    "title": "Curriculum Vitae",
    "section": " Projects",
    "text": "Projects\n\n\n\n\n&lt;img src=\"assets/projects/fed‑drl.jpg\" alt=\"Federated DRL thumbnail\"&gt;\n&lt;div class=\"proj-body\"&gt;\n  &lt;h3&gt;project 1&lt;/h3&gt;\n  &lt;p&gt;with background image&lt;/p&gt;\n&lt;/div&gt;\n\n\n\n&lt;img src=\"assets/projects/sunset.jpg\" alt=\"Sunset thumbnail\"&gt;\n&lt;div class=\"proj-body\"&gt;\n  &lt;h3&gt;project 7&lt;/h3&gt;\n  &lt;p&gt;with background image&lt;/p&gt;\n&lt;/div&gt;\n\n\n\n&lt;img src=\"assets/projects/needles.jpg\" alt=\"Needles thumbnail\"&gt;\n&lt;div class=\"proj-body\"&gt;\n  &lt;h3&gt;project 2&lt;/h3&gt;\n  &lt;p&gt;a project with a background image and giscus comments&lt;/p&gt;\n&lt;/div&gt;"
  },
  {
    "objectID": "cv.html#featured-projects",
    "href": "cv.html#featured-projects",
    "title": "Projects",
    "section": "Featured Projects",
    "text": "Featured Projects\n\n\n\n\n🌱\n\n\nwashi\n\nR PACKAGE\n\n\n\nPalette and themes for Washington Soil Health Initiative branding\n\nSEP 7, 2023\n\n\n\n\n\n🗺️\n\n\nWaSHI Soil Health Roadmap\n\nARCGIS PYTHON\n\n\n\nWashington Soil Health Initiative Roadmap created with Python and ArcGIS Dashboards & Experience Builder\n\nAUG 4, 2023\n\n\n\n\n\n🐋\n\n\norcas\n\nR PACKAGE\n\n\n\nMy first personal project to learn web scraping with {rvest} and mapping with {leaflet}\n\nAPR 20, 2023"
  },
  {
    "objectID": "projects/prohect2.html",
    "href": "projects/prohect2.html",
    "title": "Projects",
    "section": "",
    "text": "Data Apothecary’s Notes\n\n\n\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\n\n\n\n\n\n\n\n\n\n\n\nMortality Surveillance in Norway\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoreden\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Kundan Kumar",
    "section": "",
    "text": "Home\n    Contact"
  },
  {
    "objectID": "contact.html#visit-us",
    "href": "contact.html#visit-us",
    "title": "Kundan Kumar",
    "section": "Visit Us",
    "text": "Visit Us\nThe Lab is located in Atanasoff Hall at the Iowa State University.\n\nStreet Address: 2434 Osborn Dr , Ames, IA 50011"
  },
  {
    "objectID": "contact.html#email-us",
    "href": "contact.html#email-us",
    "title": "Kundan Kumar",
    "section": "Email Us",
    "text": "Email Us\n\n\n\n\n\n  \nName *  \nEmail *  \nSubject  — Select a topic — Research Resources Opportunities  \nMessage\n\n\n\nSubmit"
  },
  {
    "objectID": "contact.html#kundan-kumar",
    "href": "contact.html#kundan-kumar",
    "title": "Contact",
    "section": "",
    "text": "Iowa State University\nAtanasoff Hall\n2434 Osborn Dr\nAmes, IA 50011\nYou can reach me at\n📧 kkumar@iastate.edu  | \n📧 cs.kundann@gmail.com"
  },
  {
    "objectID": "contact.html#visit-us-1",
    "href": "contact.html#visit-us-1",
    "title": "Kundan Kumar",
    "section": "Visit Us",
    "text": "Visit Us\nThe AffCom Lab is located in Fraser Hall Room #455 at the University of Kansas.\n\nStreet Address: 1415 Jayhawk Blvd, Lawrence, KS 66044\nVisitor Parking: 1218 Mississippi St, Lawrence, KS 66045"
  },
  {
    "objectID": "contact.html#email-us-1",
    "href": "contact.html#email-us-1",
    "title": "Kundan Kumar",
    "section": "Email Us",
    "text": "Email Us\n\n\n\n\n\n  \nName *  \nEmail *  \nSubject  — Select a topic — Research Resources Opportunities  \nMessage\n\n\n\nSubmit"
  },
  {
    "objectID": "projects/projetcs.html",
    "href": "projects/projetcs.html",
    "title": "Projects",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Deep Reinforcement Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProjects\n\n\nThis porfolio includes openly available educational material I have created, along with works, software, and tools I have contributed to or designed.\n\n\n\n\n\n\n\n\n\n\n\n\n\nMortality Surveillance in Norway\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\nDeep Reinforcement Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Reinforcement Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\n\n\n\n\n\n\nProjects\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoreden\n\n\nR tools to faciliate sustainable nutrition research\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\nggplot2 extension to EHR data\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Reinforcement Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProjects\n\n\nThis porfolio includes openly available educational material I have created, along with works, software, and tools I have contributed to or designed.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects1/cv.html",
    "href": "projects1/cv.html",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package ggehr to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\nggplot2 extension to EHR data\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “Deep Reinforcement Learning.” https://kundan-kumarr.github.io/projects1/cv.html."
  },
  {
    "objectID": "projects1/phuse/index.html",
    "href": "projects1/phuse/index.html",
    "title": "PHUSE - CAMIS",
    "section": "",
    "text": "I am contributing to two working groups at PHUSE: CAMIS, and RWD - Real World Data Guideline (early stage).\n\nCAMIS: Comparing Analysis Method Implementations in Software\nCAMIS is a cross-industry group formed of members from PHUSE, PSI and ASA.\nSeveral discrepancies have been discovered in statistical analysis results between different programming languages, even in fully qualified statistical computing environments. Observing differences across languages can reduce the analyst’s confidence in reliability and, by understanding the source of any discrepancies, one can reinstate confidence in reliability.\nThe goal of this project is to demystify conflict when doing QC and to help ease the transitions to new languages and techniques with comparison and comprehensive explanations.\n\n\nRWD Working Group\nThis is a newly formed working group, working on statistical programming guidelines while working on RWD (read world data).\n\n\n\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “PHUSE - CAMIS.” https://kundan-kumarr.github.io/projects1/phuse/."
  },
  {
    "objectID": "projects1/sykdomspulsen/index.html",
    "href": "projects1/sykdomspulsen/index.html",
    "title": "CSIDS",
    "section": "",
    "text": "CSIDS: the Consortium for Statistics in Disease Surveillance (previously Sykdomspulsen) is a real-time analysis and disease surveillance system designed and developed at the Norwegian Institute of Public Health (NIPH/FHI). It is a unique project that processes new data (e.g. covid-19 cases) shortly after it is available. Complex statistical analyses are automatically run for all locations in Norway, producing reports and alerting various stakeholders. This provides the health authorities the ability to make proactive strategic decisions with the most up-to-date information.\n\n\ncsverse is a number of R packages that have been developed by the Sykdomspulsen team to help with infectious diseases surveillance.\n\n\n\nIn November 2022, the core components of Sykdomspulsen and splverse R packages have been migrated to CSIDS: the Consortium for Statistics in Disease Surveillance. Please refer to CSIDS for ongoing developments.\n\n\n\n\nWe receive data from more than 15 data sources every day\n2 000 000 000+ rows of data and results (1TB)\n1 000+ database tables\n1 000 000+ analyses per day\n1 000+ automatic reports per day\n\n\n\nDownload poster (Norwegian)"
  },
  {
    "objectID": "projects1/sykdomspulsen/index.html#overview",
    "href": "projects1/sykdomspulsen/index.html#overview",
    "title": "CSIDS",
    "section": "",
    "text": "CSIDS: the Consortium for Statistics in Disease Surveillance (previously Sykdomspulsen) is a real-time analysis and disease surveillance system designed and developed at the Norwegian Institute of Public Health (NIPH/FHI). It is a unique project that processes new data (e.g. covid-19 cases) shortly after it is available. Complex statistical analyses are automatically run for all locations in Norway, producing reports and alerting various stakeholders. This provides the health authorities the ability to make proactive strategic decisions with the most up-to-date information.\n\n\ncsverse is a number of R packages that have been developed by the Sykdomspulsen team to help with infectious diseases surveillance.\n\n\n\nIn November 2022, the core components of Sykdomspulsen and splverse R packages have been migrated to CSIDS: the Consortium for Statistics in Disease Surveillance. Please refer to CSIDS for ongoing developments.\n\n\n\n\nWe receive data from more than 15 data sources every day\n2 000 000 000+ rows of data and results (1TB)\n1 000+ database tables\n1 000 000+ analyses per day\n1 000+ automatic reports per day\n\n\n\nDownload poster (Norwegian)"
  },
  {
    "objectID": "projects1/projects.html",
    "href": "projects1/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package ggehr to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\nggplot2 extension to EHR data\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “Projects.” https://kundan-kumarr.github.io/projects1/projects.html."
  },
  {
    "objectID": "projects1/dan/index.html",
    "href": "projects1/dan/index.html",
    "title": "Data Apothecary’s Notes",
    "section": "",
    "text": "(This is my own note-taking system using quarto)\n\nAbout the notes\nData Apothecary’s Notes is a note-taking repository for modern data science skills with a focus on drug development and clinical trials. Content will be gradually added while I learn the topics. Therefore, it is by no means a complete guide by the time you read it!\nI try to organize the content in a modular way. I think these should cover the important aspects in which a data scientist / modern statistician should know.\n\nstudy design\ninference\nmodels\nreporting\nprogramming\n\n\n\nWhy quarto\nIn short, quarto has the advantage of making a very well structured website with code chunks easy. No more worry about scattered notes in different folders - put them together, render it so you can find your notes quickly!\n\n\n\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “Data Apothecary’s Notes.” https://kundan-kumarr.github.io/projects1/dan/."
  },
  {
    "objectID": "projects1/dl.html",
    "href": "projects1/dl.html",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package ggehr to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\nggplot2 extension to EHR data\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “Deep Reinforcement Learning.” https://kundan-kumarr.github.io/projects1/dl.html."
  },
  {
    "objectID": "projects1/nor_mortality/index.html",
    "href": "projects1/nor_mortality/index.html",
    "title": "Mortality Surveillance in Norway",
    "section": "",
    "text": "NorMOMO"
  },
  {
    "objectID": "projects1/nor_mortality/index.html#norwegian-surveillance-system-for-excess-mortality",
    "href": "projects1/nor_mortality/index.html#norwegian-surveillance-system-for-excess-mortality",
    "title": "Mortality Surveillance in Norway",
    "section": "",
    "text": "NorMOMO"
  },
  {
    "objectID": "projects1/nor_mortality/index.html#r-packages-for-mortality-surveillance",
    "href": "projects1/nor_mortality/index.html#r-packages-for-mortality-surveillance",
    "title": "Mortality Surveillance in Norway",
    "section": "R packages for mortality surveillance",
    "text": "R packages for mortality surveillance\n(continue…)\n\nnowcast\nsplalert\nmortanor"
  },
  {
    "objectID": "projects1/nor_mortality/index.html#collaboration-with-cause-of-death-registry",
    "href": "projects1/nor_mortality/index.html#collaboration-with-cause-of-death-registry",
    "title": "Mortality Surveillance in Norway",
    "section": "Collaboration with Cause of Death Registry",
    "text": "Collaboration with Cause of Death Registry"
  },
  {
    "objectID": "projects1/drl.html",
    "href": "projects1/drl.html",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package ggehr to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\nggplot2 extension to EHR data\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “Deep Reinforcement Learning.” https://kundan-kumarr.github.io/projects1/drl.html."
  },
  {
    "objectID": "projects/orcas.html",
    "href": "projects/orcas.html",
    "title": "orcas",
    "section": "",
    "text": "{{&lt; fa brands github &gt;}} Code \nThe goal of orcas is to scrape orca sighting data from the web and visualize it in maps and tables.\nI’ve always had an affinity for the Southern Resident Killer Whales in the Salish Sea. The Center for Whale Research does a lot of really fascinating and important work monitoring their population. They post their survey data on their website; each encounter with the orcas is a separate webpage. I was both curious and intimidated by web scraping so I decided this would make a great case study and personal project. I also learned how to use custom icons in leaflet maps! 🐋"
  },
  {
    "objectID": "projects/roadmap.html",
    "href": "projects/roadmap.html",
    "title": "WaSHI Soil Health Roadmap",
    "section": "",
    "text": "{{&lt; fa laptop &gt;}} App {{&lt; fa brands github &gt;}} Code \nThe Soil Health Roadmap is a science-based guide to maintaining and improving soil health in eight focus areas across Washington state.\nFor each focus area, I created crop and soil maps using the arcpy Python package. I then summarized the crop acreage and soil properties in interactive ArcGIS dashboards to complement the 117-page report."
  },
  {
    "objectID": "projects/washi.html",
    "href": "projects/washi.html",
    "title": "washi",
    "section": "",
    "text": "{{&lt; fa box &gt;}} {pkgdown} site {{&lt; fa brands github &gt;}} Code {{&lt; fa brands r-project &gt;}} CRAN \nInspired by other branding R packages such as glitr, ratlas, and nmfspalette, washi provides color palettes and themes consistent with Washington Soil Health Initiative (WaSHI) branding. This package is to be used only by direct collaborators within WaSHI, though you are welcome to adapt the package to suit your own organization’s branding."
  },
  {
    "objectID": "projects/wacse.html",
    "href": "projects/wacse.html",
    "title": "WaCSE",
    "section": "",
    "text": "{{&lt; fa laptop &gt;}} App {{&lt; fa brands github &gt;}} Code \nThe Washington State Department of Agriculture developed WaCSE for the Washington State Conservation Commission to use in the Sustainable Farms and Fields (SFF) program. Intended users are the Conservation Commission, conservation districts, growers, and anyone interested in reducing agricultural greenhouse gas (GHG) emissions. This interactive tool estimates the reduction of GHG emissions from different conservation practices across Washington’s diverse counties."
  },
  {
    "objectID": "projects1/prohect2.html",
    "href": "projects1/prohect2.html",
    "title": "Projects",
    "section": "",
    "text": "Data Apothecary’s Notes\n\n\n\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\n\n\n\n\n\n\n\n\n\n\n\nMortality Surveillance in Norway\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoreden\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects1/os_teaching/index.html",
    "href": "projects1/os_teaching/index.html",
    "title": "Teach in R and Quarto",
    "section": "",
    "text": "MF9130E - Introductory course in statistics\n8-day intensive course on introductory statistics. April 2023 we made it with R rather than propriety software, coupled with live-coding sessions to enhance understanding of basic concepts such as distribution and hypothesis tests.\nRepository\nCourse website\nRead more about the experience in\n\nblogpost\npresentation\n\n\n\n\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “Teach in R and Quarto.” https://kundan-kumarr.github.io/projects1/os_teaching/."
  },
  {
    "objectID": "projects1/robo.html",
    "href": "projects1/robo.html",
    "title": "Deep Reinforcement Learning",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package ggehr to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\nggplot2 extension to EHR data\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “Deep Reinforcement Learning.” https://kundan-kumarr.github.io/projects1/robo.html."
  },
  {
    "objectID": "projects1/projetcs.html",
    "href": "projects1/projetcs.html",
    "title": "Projects",
    "section": "",
    "text": "No matching items\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “Projects.” https://kundan-kumarr.github.io/projects1/projetcs.html."
  },
  {
    "objectID": "projects1/noreden/index.html",
    "href": "projects1/noreden/index.html",
    "title": "Noreden",
    "section": "",
    "text": "Noreden\n\n\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “Noreden.” https://kundan-kumarr.github.io/projects1/noreden/."
  },
  {
    "objectID": "projects1/ehr/index.html",
    "href": "projects1/ehr/index.html",
    "title": "ggehr",
    "section": "",
    "text": "ggehr (read: gg E-H-R) stands for ggplot2 extension for EHR data, which provides a set of tools to facilitate EHR (Electronic Health Records) visualization.\nggehr package helps you make visualize EHR data, so that you can\n\nhave an overview of the mixed type information related to a patient;\nvisually identify the errors in data recording.\n\nLearn more about ggehr\n\n\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “Ggehr.” https://kundan-kumarr.github.io/projects1/ehr/."
  },
  {
    "objectID": "projects1/stat.html",
    "href": "projects1/stat.html",
    "title": "Statistics",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. My PhD (completed in 2022) focused on representation and utilization of hospital EHR data. I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting, and an R package ggehr to facilitate the exploration of EHR data.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design, noreden.\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\nggplot2 extension to EHR data\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoreden\n\n\nR tools to faciliate sustainable nutrition research\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. I am the developer of MF9310E quarto website, leading the transition of teaching university biostatistics courses using R and Quarto. I’m also a co-lead of the PHUSE CAMIS project.\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nCitationFor attribution, please cite this work as:\nKundan, Kundan. n.d. “Statistics.” https://kundan-kumarr.github.io/projects1/stat.html."
  },
  {
    "objectID": "projects1/projects1.html",
    "href": "projects1/projects1.html",
    "title": "Projects",
    "section": "",
    "text": "Data Apothecary’s Notes\n\n\n\n\n\n\n\n\n\n\n\n\n\nggehr\n\n\n\n\n\n\n\n\n\n\n\n\n\nMortality Surveillance in Norway\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoreden\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeach in R and Quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE - CAMIS\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "rpkg/rpkg2.html",
    "href": "rpkg/rpkg2.html",
    "title": "Research",
    "section": "",
    "text": "I aim to advance the frontier of safe, interpretable, and adaptive AI for cyber-physical systems operating under uncertainty and dynamic constraints. My research sits at the intersection of machine learning, optimization, and control theory, with a particular focus on:\n\nPhysics-informed Deep Reinforcement Learning (DRL)\nProbabilistic & Bayesian Modeling\nLarge Language Models (LLMs) for autonomous reasoning\nVision-based simulation environments\n\nBy tightly integrating domain knowledge into learning frameworks, I design agents capable of robust decision-making in real-world, high-stakes environments such as smart grids, robotics, and intelligent infrastructure."
  },
  {
    "objectID": "rpkg/rpkg2.html#research-vision",
    "href": "rpkg/rpkg2.html#research-vision",
    "title": "Research",
    "section": "",
    "text": "I aim to advance the frontier of safe, interpretable, and adaptive AI for cyber-physical systems operating under uncertainty and dynamic constraints. My research sits at the intersection of machine learning, optimization, and control theory, with a particular focus on:\n\nPhysics-informed Deep Reinforcement Learning (DRL)\nProbabilistic & Bayesian Modeling\nLarge Language Models (LLMs) for autonomous reasoning\nVision-based simulation environments\n\nBy tightly integrating domain knowledge into learning frameworks, I design agents capable of robust decision-making in real-world, high-stakes environments such as smart grids, robotics, and intelligent infrastructure."
  },
  {
    "objectID": "rpkg/rpkg2.html#research-themes",
    "href": "rpkg/rpkg2.html#research-themes",
    "title": "Research",
    "section": "🧠 Research Themes",
    "text": "🧠 Research Themes\n\n🔒 Safe & Trustworthy Reinforcement Learning\nDeveloping control agents that ensure system safety while learning in uncertain and partially observable environments. - Constrained policy optimization and reward shaping - Physics-based priors in DRL - Adversarial resilience and anomaly detection - Epistemic and aleatoric uncertainty quantification\n\n\n🧬 Transfer Learning & Meta-Adaptation\nEnabling rapid generalization across distribution shifts in topology, weather, or load profiles. - Transferable actor-critic architectures - Simulation-to-real (Sim2Real) adaptation - Meta-RL for sample efficiency\n\n\n🧿 Vision-Simulation Integration\nCombining synthetic perception and sensor data with RL agents. - Perception-action loops using CARLA, AirSim - Multi-modal representation learning - End-to-end autonomous control pipelines\n\n\n🧠 LLM-Augmented Decision Systems\nLeveraging large language models to assist or guide RL agents with contextual reasoning. - LLMs for environment summarization & trajectory guidance - Text-to-policy translation - Human-AI collaborative control"
  },
  {
    "objectID": "rpkg/rpkg2.html#domains-of-application",
    "href": "rpkg/rpkg2.html#domains-of-application",
    "title": "Research",
    "section": "🔬 Domains of Application",
    "text": "🔬 Domains of Application\n\n\n\n\n\n\n\nDomain\nDescription\n\n\n\n\n⚡ Smart Energy Systems\nVolt-VAR control, DER coordination, and federated DRL for power grid stability\n\n\n🚘 Autonomous Systems\nSafe navigation, adaptive planning, and control in simulation and real-world driving environments\n\n\n🛡 Secure AI for Infrastructure\nResilience against cyber-attacks and adversarial scenarios in safety-critical systems"
  },
  {
    "objectID": "rpkg/rpkg2.html#selected-publications",
    "href": "rpkg/rpkg2.html#selected-publications",
    "title": "Research",
    "section": "📚 Selected Publications",
    "text": "📚 Selected Publications\n\n📝 Journal Papers🎤 Conference Papers\n\n\n\nKundan Kumar, Gelli Ravikumar\nPhysics-based Deep Reinforcement Learning for Grid-Resilient Volt-VAR Control (Under Review)\nIEEE Transactions on Smart Grid, 2025\n Paper Code Poster \n\n\n\n\n\nKundan Kumar, Gelli Ravikumar\nTransfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids\nIEEE PES Grid Edge Technologies Conference & Exposition, 2025\n Paper Code Poster \nKundan Kumar, Aditya Akilesh Mantha, Gelli Ravikumar\nBayesian Optimization for Deep Reinforcement Learning in Robust Volt-Var Control\nIEEE PES General Meeting, 2024\n Paper Code Poster \nKundan Kumar, Gelli Ravikumar\nDeep RL-based Volt-VAR Control and Attack Resiliency for DER-integrated Distribution Grids\nIEEE ISGT, 2024\n Paper Code Poster \nJK Francis, C Kumar, J Herrera-Gerena, Kundan Kumar, MJ Darr\nDeep Learning and Pattern-based Methodology for Multivariable Sensor Data Regression\nIEEE ICMLA, 2022\n Paper Code Poster \nKin Gwn Lore, Nicholas Sweet, Kundan Kumar, N Ahmed, S Sarkar\nDeep Value of Information Estimators for Collaborative Human-Machine Information Gathering\nACM/IEEE ICCPS, 2016\n Paper Code Poster"
  },
  {
    "objectID": "rpkg/rpkg2.html#ongoing-projects",
    "href": "rpkg/rpkg2.html#ongoing-projects",
    "title": "Research",
    "section": "📎 Ongoing Projects",
    "text": "📎 Ongoing Projects\n\nFederated DRL for Cyber-Resilient Volt-VAR Optimization\nExploring decentralized, communication-efficient coordination among DERs using LSTM-enhanced PPO agents.\nOne-Shot Policy Transfer with Physics Priors\nTraining grid agents in simpler environments and adapting to complex grids (IEEE 123-bus, 8500-node) in few iterations.\nLLM-Guided Autonomous Planning for Smart Buildings\nIntegrating OpenAI and Claude with CityLearn to convert user prompts into interpretable policy instructions."
  },
  {
    "objectID": "rpkg/rpkg2.html#research-themes-1",
    "href": "rpkg/rpkg2.html#research-themes-1",
    "title": "Research",
    "section": "🔬 Research Themes",
    "text": "🔬 Research Themes\n\n\n\n&lt;a href=\"safe-rl.qmd\" style=\"text-decoration: none; color: inherit;\"&gt;\n  &lt;div style=\"border: 2px solid #1e567d; border-radius: 10px; padding: 1.2rem; background: #f9f9f9; height: 100%;\"&gt;\n    &lt;h4 style=\"color: #1e567d;\"&gt;&lt;i class=\"fas fa-shield-alt\"&gt;&lt;/i&gt; Safe Reinforcement Learning&lt;/h4&gt;\n    &lt;p&gt;\n      Designing control agents that learn safely and robustly in high-stakes environments. Includes physics-based constraints, uncertainty-aware exploration, and adversarial resilience in AI systems.\n    &lt;/p&gt;\n  &lt;/div&gt;\n&lt;/a&gt;\n\n\n\n&lt;a href=\"transfer-learning.qmd\" style=\"text-decoration: none; color: inherit;\"&gt;\n  &lt;div style=\"border: 2px solid #1e567d; border-radius: 10px; padding: 1.2rem; background: #f9f9f9; height: 100%;\"&gt;\n    &lt;h4 style=\"color: #1e567d;\"&gt;&lt;i class=\"fas fa-sync-alt\"&gt;&lt;/i&gt; Transfer & Meta-Learning&lt;/h4&gt;\n    &lt;p&gt;\n      Enabling policy generalization across grids, seasons, and topologies using simulation-to-real, one-shot learning, and meta-learning strategies.\n    &lt;/p&gt;\n  &lt;/div&gt;\n&lt;/a&gt;\n\n\n\n&lt;a href=\"vision-simulation.qmd\" style=\"text-decoration: none; color: inherit;\"&gt;\n  &lt;div style=\"border: 2px solid #1e567d; border-radius: 10px; padding: 1.2rem; background: #f9f9f9; height: 100%;\"&gt;\n    &lt;h4 style=\"color: #1e567d;\"&gt;&lt;i class=\"fas fa-video\"&gt;&lt;/i&gt; Vision-Based Simulation&lt;/h4&gt;\n    &lt;p&gt;\n      Bridging perception and control with synthetic environments like CARLA. Developing pipelines that combine object detection with decision-making in robotics and autonomous driving.\n    &lt;/p&gt;\n  &lt;/div&gt;\n&lt;/a&gt;\n\n\n\n&lt;a href=\"llm-control.qmd\" style=\"text-decoration: none; color: inherit;\"&gt;\n  &lt;div style=\"border: 2px solid #1e567d; border-radius: 10px; padding: 1.2rem; background: #f9f9f9; height: 100%;\"&gt;\n    &lt;h4 style=\"color: #1e567d;\"&gt;&lt;i class=\"fas fa-robot\"&gt;&lt;/i&gt; LLM-Augmented Control&lt;/h4&gt;\n    &lt;p&gt;\n      Integrating large language models into control frameworks for prompt-driven planning, explainable reasoning, and human-in-the-loop decision-making.\n    &lt;/p&gt;\n  &lt;/div&gt;\n&lt;/a&gt;"
  },
  {
    "objectID": "rpkg/rpkg3.html",
    "href": "rpkg/rpkg3.html",
    "title": "Research",
    "section": "",
    "text": "I aim to advance the frontier of safe, interpretable, and adaptive AI for cyber-physical systems operating under uncertainty and dynamic constraints. My research sits at the intersection of machine learning, optimization, and control theory, with a particular focus on:\n\nPhysics-informed Deep Reinforcement Learning (DRL)\nProbabilistic & Bayesian Modeling\nLarge Language Models (LLMs) for autonomous reasoning\nVision-based simulation environments\n\nBy tightly integrating domain knowledge into learning frameworks, I design agents capable of robust decision-making in real-world, high-stakes environments such as smart grids, robotics, and intelligent infrastructure."
  },
  {
    "objectID": "rpkg/rpkg3.html#research-vision",
    "href": "rpkg/rpkg3.html#research-vision",
    "title": "Research",
    "section": "",
    "text": "I aim to advance the frontier of safe, interpretable, and adaptive AI for cyber-physical systems operating under uncertainty and dynamic constraints. My research sits at the intersection of machine learning, optimization, and control theory, with a particular focus on:\n\nPhysics-informed Deep Reinforcement Learning (DRL)\nProbabilistic & Bayesian Modeling\nLarge Language Models (LLMs) for autonomous reasoning\nVision-based simulation environments\n\nBy tightly integrating domain knowledge into learning frameworks, I design agents capable of robust decision-making in real-world, high-stakes environments such as smart grids, robotics, and intelligent infrastructure."
  },
  {
    "objectID": "rpkg/rpkg3.html#my-research-framework",
    "href": "rpkg/rpkg3.html#my-research-framework",
    "title": "Research",
    "section": "🧭 My Research Framework",
    "text": "🧭 My Research Framework\n\n\nMy Research Framework\n\n\n&lt;!-- Learning Core --&gt;\n&lt;div class=\"col-md-2\" style=\"background: #e6f4ea; border-radius: 8px; padding: 1rem;\"&gt;\n  &lt;h5 style=\"text-align: center;\"&gt;Learning Core&lt;/h5&gt;\n  &lt;ul style=\"list-style: none; padding: 0; text-align: center;\"&gt;\n    &lt;li&gt;Physics-Informed RL&lt;/li&gt;\n    &lt;li&gt;Bayesian Modeling&lt;/li&gt;\n    &lt;li&gt;Uncertainty Estimation&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/div&gt;\n\n&lt;!-- Reasoning & Transfer --&gt;\n&lt;div class=\"col-md-2\" style=\"background: #fff4d9; border-radius: 8px; padding: 1rem;\"&gt;\n  &lt;h5 style=\"text-align: center;\"&gt;Reasoning & Transfer&lt;/h5&gt;\n  &lt;ul style=\"list-style: none; padding: 0; text-align: center;\"&gt;\n    &lt;li&gt;LLM-Augmented Agents&lt;/li&gt;\n    &lt;li&gt;Meta-RL&lt;/li&gt;\n    &lt;li&gt;Transfer Learning&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/div&gt;\n\n&lt;!-- Perception --&gt;\n&lt;div class=\"col-md-2\" style=\"background: #e9ecff; border-radius: 8px; padding: 1rem;\"&gt;\n  &lt;h5 style=\"text-align: center;\"&gt;Perception & Simulation&lt;/h5&gt;\n  &lt;ul style=\"list-style: none; padding: 0; text-align: center;\"&gt;\n    &lt;li&gt;CARLA / OpenDSS&lt;/li&gt;\n    &lt;li&gt;Sensor Fusion&lt;/li&gt;\n    &lt;li&gt;Scene Understanding&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/div&gt;\n\n&lt;!-- Safety --&gt;\n&lt;div class=\"col-md-2\" style=\"background: #fde8ec; border-radius: 8px; padding: 1rem;\"&gt;\n  &lt;h5 style=\"text-align: center;\"&gt;Robustness & Safety&lt;/h5&gt;\n  &lt;ul style=\"list-style: none; padding: 0; text-align: center;\"&gt;\n    &lt;li&gt;Adversarial Defense&lt;/li&gt;\n    &lt;li&gt;Safe Exploration&lt;/li&gt;\n    &lt;li&gt;Cyber-Resilience&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/div&gt;\n\n&lt;!-- Application --&gt;\n&lt;div class=\"col-md-2\" style=\"background: #e0f0f8; border-radius: 8px; padding: 1rem;\"&gt;\n  &lt;h5 style=\"text-align: center;\"&gt;Applications&lt;/h5&gt;\n  &lt;ul style=\"list-style: none; padding: 0; text-align: center;\"&gt;\n    &lt;li&gt;Smart Grids&lt;/li&gt;\n    &lt;li&gt;Autonomous Systems&lt;/li&gt;\n    &lt;li&gt;Human-AI Collaboration&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/div&gt;\n\n\nUnified through Optimization, AI Safety, and Applied Machine Learning"
  },
  {
    "objectID": "rpkg/rpkg3.html#research-themes",
    "href": "rpkg/rpkg3.html#research-themes",
    "title": "Research",
    "section": "🧠 Research Themes",
    "text": "🧠 Research Themes\n\nSafe & Trustworthy Reinforcement Learning\n\n\n\n🎯 Objective\n\n\nDevelop control agents that guarantee system safety, stability, and robust learning in dynamic, uncertain, and partially observable environments.\n\n\n🔍 Core Focus Areas\n\n\n\nConstrained policy optimization and reward shaping\n\n\nPhysics-based priors in DRL\n\n\nAdversarial resilience and anomaly detection\n\n\nEpistemic and aleatoric uncertainty quantification\n\n\n\n\n\n\n\n\n\nTransfer Learning & Meta-Adaptation\n\n\n\n🎯 Objective\n\n\nEnabling rapid generalization across distribution shifts in topology, weather, or load profiles.\n\n\n🔍 Core Focus Areas\n\n\n\nTransferable actor-critic architectures\n\n\nSimulation-to-real (Sim2Real) adaptation\n\n\nMeta-RL for sample efficiency\n\n\n\n\n\n\n\n\n\nVision-Simulation Integration\n\n\n\n🎯 Objective\n\n\nBridge the gap between perception and control by combining synthetic sensors, simulated environments, and end-to-end learning pipelines.\n\n\n🔍 Core Focus Areas\n\n\n\nPerception-action loops with CARLA, AirSim\n\n\nMulti-modal representation fusion (image + state)\n\n\nAutonomous control with embedded perception modules\n\n\nEnd-to-end autonomous control pipelines\n\n\n\n\n\n\n\n\n\nLLM-Augmented Decision Systems\n\n\n\n🎯 Objective\n\n\nDevelop control agents that guarantee system safety, stability, and robust learning in dynamic, uncertain, and partially observable environments.\n\n\n🔍 Core Focus Areas\n\n\n\nLLMs for summarizing environment states and guiding agents\n\n\nTranslating textual inputs into actionable policies\n\n\nFacilitating human-AI collaboration in dynamic tasks"
  },
  {
    "objectID": "rpkg/rpkg3.html#application-domains",
    "href": "rpkg/rpkg3.html#application-domains",
    "title": "Research",
    "section": "🔬 Application Domains",
    "text": "🔬 Application Domains\n\n\n\n\n\n\n\nDomain\nDescription\n\n\n\n\n⚡ Smart Energy Systems\nVolt-VAR control, DER coordination, and federated DRL for power grid stability\n\n\n🚘 Autonomous Systems\nSafe navigation, adaptive planning, and control in simulation and real-world driving environments\n\n\n🛡 Secure AI for Infrastructure\nResilience against cyber-attacks and adversarial scenarios in safety-critical systems"
  },
  {
    "objectID": "rpkg/rpkg3.html#publications",
    "href": "rpkg/rpkg3.html#publications",
    "title": "Research",
    "section": "📚 Selected Publications",
    "text": "📚 Selected Publications\n\n📝 Journal Papers🎤 Conference Papers\n\n\n\nKundan Kumar, Gelli Ravikumar\nPhysics-based Deep Reinforcement Learning for Grid-Resilient Volt-VAR Control (Under Review)\nIEEE Transactions on Smart Grid, 2025\n Paper Code Poster \n\n\n\n\n\nKundan Kumar, Gelli Ravikumar\nTransfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids\nIEEE PES Grid Edge Technologies Conference & Exposition, 2025\n Paper Code Poster \nKundan Kumar, Aditya Akilesh Mantha, Gelli Ravikumar\nBayesian Optimization for Deep Reinforcement Learning in Robust Volt-Var Control\nIEEE PES General Meeting, 2024\n Paper Code Poster \nKundan Kumar, Gelli Ravikumar\nDeep RL-based Volt-VAR Control and Attack Resiliency for DER-integrated Distribution Grids\nIEEE ISGT, 2024\n Paper Code Poster \nJK Francis, C Kumar, J Herrera-Gerena, Kundan Kumar, MJ Darr\nDeep Learning and Pattern-based Methodology for Multivariable Sensor Data Regression\nIEEE ICMLA, 2022\n Paper Code Poster \nKin Gwn Lore, Nicholas Sweet, Kundan Kumar, N Ahmed, S Sarkar\nDeep Value of Information Estimators for Collaborative Human-Machine Information Gathering\nACM/IEEE ICCPS, 2016\n Paper Code Poster"
  },
  {
    "objectID": "rpkg/rpkg3.html#ongoing-projects",
    "href": "rpkg/rpkg3.html#ongoing-projects",
    "title": "Research",
    "section": "🧪 Ongoing Projects",
    "text": "🧪 Ongoing Projects\n\nFederated DRL for Cyber-Resilient Volt-VAR Optimization\nExploring decentralized, communication-efficient coordination among DERs using LSTM-enhanced PPO agents.\nOne-Shot Policy Transfer with Physics Priors\nTraining grid agents in simpler environments and adapting to complex grids (IEEE 123-bus, 8500-node) in few iterations.\nLLM-Guided Autonomous Planning for Smart Buildings\nIntegrating OpenAI and Claude with CityLearn to convert user prompts into interpretable policy instructions."
  },
  {
    "objectID": "rpkg/rpkg.html#selected-publications",
    "href": "rpkg/rpkg.html#selected-publications",
    "title": "Kundan Kumar",
    "section": "",
    "text": "📝 Journal Papers🎤 Conference Papers🎤 Conference Papers\n\n\n\nKundan Kumar, Gelli Ravikumar\nPhysics-based Deep Reinforcement Learning for Grid-Resilient Volt-VAR Control (Under Review)\nIEEE Transactions on Smart Grid, 2025\n Paper Code Poster \n\n\n\n\n\nPage 1Page 2\n\n\n\nKundan Kumar, Gelli Ravikumar\nTransfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids\nIEEE PES Grid Edge Technologies Conference & Exposition, 2025\n Paper Code Poster \nKundan Kumar, Aditya Akilesh Mantha, Gelli Ravikumar\nBayesian Optimization for Deep Reinforcement Learning in Robust Volt-Var Control\nIEEE PES General Meeting, 2024\n Paper Code Poster \nKundan Kumar, Gelli Ravikumar\nDeep RL-based Volt-VAR Control and Attack Resiliency for DER-integrated Distribution Grids\nIEEE ISGT, 2024\n Paper Code Poster \n\n\n\n\nJK Francis, C Kumar, J Herrera-Gerena, Kundan Kumar, MJ Darr\nDeep Learning and Pattern-based Methodology for Multivariable Sensor Data Regression\nIEEE ICMLA, 2022\n Paper Code Poster \nKin Gwn Lore, Nicholas Sweet, Kundan Kumar, N Ahmed, S Sarkar\nDeep Value of Information Estimators for Collaborative Human-Machine Information Gathering\nACM/IEEE ICCPS, 2016\n Paper Code Poster \n\n\n\n\n\n\n\n\n\nKundan Kumar, Gelli Ravikumar\nTransfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids\nIEEE PES Grid Edge Technologies Conference & Exposition, 2025\n Paper Code Poster \nKundan Kumar, Aditya Akilesh Mantha, Gelli Ravikumar\nBayesian Optimization for Deep Reinforcement Learning in Robust Volt-Var Control\nIEEE PES General Meeting, 2024\n Paper Code Poster \nKundan Kumar, Gelli Ravikumar\nDeep RL-based Volt-VAR Control and Attack Resiliency for DER-integrated Distribution Grids\nIEEE ISGT, 2024\n Paper Code Poster \n\n\n1 2\n\n\n\n\n\n\nJK Francis, C Kumar, J Herrera-Gerena, Kundan Kumar, MJ Darr\nDeep Learning and Pattern-based Methodology for Multivariable Sensor Data Regression\nIEEE ICMLA, 2022\n Paper Code Poster \nKin Gwn Lore, Nicholas Sweet, Kundan Kumar, N Ahmed, S Sarkar\nDeep Value of Information Estimators for Collaborative Human-Machine Information Gathering\nACM/IEEE ICCPS, 2016\n Paper Code Poster \n\n\n1 2"
  },
  {
    "objectID": "rpkg/publications.html",
    "href": "rpkg/publications.html",
    "title": "Kundan Kumar",
    "section": "",
    "text": "📝 Journal Papers🎤 Conference Papers🎤 Conference Papers\n\n\n\nKundan Kumar, Gelli Ravikumar\nPhysics-based Deep Reinforcement Learning for Grid-Resilient Volt-VAR Control (Under Review)\nIEEE Transactions on Smart Grid, 2025\n Paper Code Poster \n\n\n\n\n\nPage 1Page 2\n\n\n\nKundan Kumar, Gelli Ravikumar\nTransfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids\nIEEE PES Grid Edge Technologies Conference & Exposition, 2025\n Paper Code Poster \nKundan Kumar, Aditya Akilesh Mantha, Gelli Ravikumar\nBayesian Optimization for Deep Reinforcement Learning in Robust Volt-Var Control\nIEEE PES General Meeting, 2024\n Paper Code Poster \nKundan Kumar, Gelli Ravikumar\nDeep RL-based Volt-VAR Control and Attack Resiliency for DER-integrated Distribution Grids\nIEEE ISGT, 2024\n Paper Code Poster \n\n\n\n\nJK Francis, C Kumar, J Herrera-Gerena, Kundan Kumar, MJ Darr\nDeep Learning and Pattern-based Methodology for Multivariable Sensor Data Regression\nIEEE ICMLA, 2022\n Paper Code Poster \nKin Gwn Lore, Nicholas Sweet, Kundan Kumar, N Ahmed, S Sarkar\nDeep Value of Information Estimators for Collaborative Human-Machine Information Gathering\nACM/IEEE ICCPS, 2016\n Paper Code Poster \n\n\n\n\n\n\n\n\n\nKundan Kumar, Gelli Ravikumar\nTransfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids\nIEEE PES Grid Edge Technologies Conference & Exposition, 2025\n Paper Code Poster \nKundan Kumar, Aditya Akilesh Mantha, Gelli Ravikumar\nBayesian Optimization for Deep Reinforcement Learning in Robust Volt-Var Control\nIEEE PES General Meeting, 2024\n Paper Code Poster \nKundan Kumar, Gelli Ravikumar\nDeep RL-based Volt-VAR Control and Attack Resiliency for DER-integrated Distribution Grids\nIEEE ISGT, 2024\n Paper Code Poster \n\n\n1 2\n\n\n\n\n\n\nJK Francis, C Kumar, J Herrera-Gerena, Kundan Kumar, MJ Darr\nDeep Learning and Pattern-based Methodology for Multivariable Sensor Data Regression\nIEEE ICMLA, 2022\n Paper Code Poster \nKin Gwn Lore, Nicholas Sweet, Kundan Kumar, N Ahmed, S Sarkar\nDeep Value of Information Estimators for Collaborative Human-Machine Information Gathering\nACM/IEEE ICCPS, 2016\n Paper Code Poster \n\n\n1 2"
  },
  {
    "objectID": "rpkg/publications.html#selected-publications",
    "href": "rpkg/publications.html#selected-publications",
    "title": "Kundan Kumar",
    "section": "",
    "text": "📝 Journal Papers🎤 Conference Papers🎤 Conference Papers\n\n\n\nKundan Kumar, Gelli Ravikumar\nPhysics-based Deep Reinforcement Learning for Grid-Resilient Volt-VAR Control (Under Review)\nIEEE Transactions on Smart Grid, 2025\n Paper Code Poster \n\n\n\n\n\nPage 1Page 2\n\n\n\nKundan Kumar, Gelli Ravikumar\nTransfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids\nIEEE PES Grid Edge Technologies Conference & Exposition, 2025\n Paper Code Poster \nKundan Kumar, Aditya Akilesh Mantha, Gelli Ravikumar\nBayesian Optimization for Deep Reinforcement Learning in Robust Volt-Var Control\nIEEE PES General Meeting, 2024\n Paper Code Poster \nKundan Kumar, Gelli Ravikumar\nDeep RL-based Volt-VAR Control and Attack Resiliency for DER-integrated Distribution Grids\nIEEE ISGT, 2024\n Paper Code Poster \n\n\n\n\nJK Francis, C Kumar, J Herrera-Gerena, Kundan Kumar, MJ Darr\nDeep Learning and Pattern-based Methodology for Multivariable Sensor Data Regression\nIEEE ICMLA, 2022\n Paper Code Poster \nKin Gwn Lore, Nicholas Sweet, Kundan Kumar, N Ahmed, S Sarkar\nDeep Value of Information Estimators for Collaborative Human-Machine Information Gathering\nACM/IEEE ICCPS, 2016\n Paper Code Poster \n\n\n\n\n\n\n\n\n\nKundan Kumar, Gelli Ravikumar\nTransfer Learning Enhanced Deep Reinforcement Learning for Volt-Var Control in Smart Grids\nIEEE PES Grid Edge Technologies Conference & Exposition, 2025\n Paper Code Poster \nKundan Kumar, Aditya Akilesh Mantha, Gelli Ravikumar\nBayesian Optimization for Deep Reinforcement Learning in Robust Volt-Var Control\nIEEE PES General Meeting, 2024\n Paper Code Poster \nKundan Kumar, Gelli Ravikumar\nDeep RL-based Volt-VAR Control and Attack Resiliency for DER-integrated Distribution Grids\nIEEE ISGT, 2024\n Paper Code Poster \n\n\n1 2\n\n\n\n\n\n\nJK Francis, C Kumar, J Herrera-Gerena, Kundan Kumar, MJ Darr\nDeep Learning and Pattern-based Methodology for Multivariable Sensor Data Regression\nIEEE ICMLA, 2022\n Paper Code Poster \nKin Gwn Lore, Nicholas Sweet, Kundan Kumar, N Ahmed, S Sarkar\nDeep Value of Information Estimators for Collaborative Human-Machine Information Gathering\nACM/IEEE ICCPS, 2016\n Paper Code Poster \n\n\n1 2"
  },
  {
    "objectID": "rpkg/rpkg.html#research-themes",
    "href": "rpkg/rpkg.html#research-themes",
    "title": "Research",
    "section": "🧠 Research Themes",
    "text": "🧠 Research Themes\n\nSafe & Trustworthy Reinforcement Learning\n\n\n\n🎯 Objective\n\n\nDevelop control agents that guarantee system safety, stability, and robust learning in dynamic, uncertain, and partially observable environments.\n\n\n🔍 Core Focus Areas\n\n\n\nConstrained policy optimization and reward shaping\n\n\nPhysics-based priors in DRL\n\n\nAdversarial resilience and anomaly detection\n\n\nEpistemic and aleatoric uncertainty quantification\n\n\n\n\n\n\n\n\n\nTransfer Learning & Meta-Adaptation\n\n\n\n🎯 Objective\n\n\nEnabling rapid generalization across distribution shifts in topology, weather, or load profiles.\n\n\n🔍 Core Focus Areas\n\n\n\nTransferable actor-critic architectures\n\n\nSimulation-to-real (Sim2Real) adaptation\n\n\nMeta-RL for sample efficiency\n\n\n\n\n\n\n\n\n\nVision-Simulation Integration\n\n\n\n🎯 Objective\n\n\nBridge the gap between perception and control by combining synthetic sensors, simulated environments, and end-to-end learning pipelines.\n\n\n🔍 Core Focus Areas\n\n\n\nPerception-action loops with CARLA, AirSim\n\n\nMulti-modal representation fusion (image + state)\n\n\nAutonomous control with embedded perception modules\n\n\nEnd-to-end autonomous control pipelines\n\n\n\n\n\n\n\n\n\nLLM-Augmented Decision Systems\n\n\n\n🎯 Objective\n\n\nDevelop control agents that guarantee system safety, stability, and robust learning in dynamic, uncertain, and partially observable environments.\n\n\n🔍 Core Focus Areas\n\n\n\nLLMs for summarizing environment states and guiding agents\n\n\nTranslating textual inputs into actionable policies\n\n\nFacilitating human-AI collaboration in dynamic tasks"
  },
  {
    "objectID": "rpkg/rpkg.html#application-domains",
    "href": "rpkg/rpkg.html#application-domains",
    "title": "Research",
    "section": "🔬 Application Domains",
    "text": "🔬 Application Domains\n\n\n\n\n\n\n\nDomain\nDescription\n\n\n\n\n⚡ Smart Energy Systems\nVolt-VAR control, DER coordination, and federated DRL for power grid stability\n\n\n🚘 Autonomous Systems\nSafe navigation, adaptive planning, and control in simulation and real-world driving environments\n\n\n🛡 Secure AI for Infrastructure\nResilience against cyber-attacks and adversarial scenarios in safety-critical systems"
  },
  {
    "objectID": "rpkg/rpkg.html#ongoing-projects",
    "href": "rpkg/rpkg.html#ongoing-projects",
    "title": "Research",
    "section": "🧪 Ongoing Projects",
    "text": "🧪 Ongoing Projects\n\nFederated DRL for Cyber-Resilient Volt-VAR Optimization\nExploring decentralized, communication-efficient coordination among DERs using LSTM-enhanced PPO agents.\nOne-Shot Policy Transfer with Physics Priors\nTraining grid agents in simpler environments and adapting to complex grids (IEEE 123-bus, 8500-node) in few iterations.\nLLM-Guided Autonomous Planning for Smart Buildings\nIntegrating OpenAI and Claude with CityLearn to convert user prompts into interpretable policy instructions."
  },
  {
    "objectID": "blog/blog1.html",
    "href": "blog/blog1.html",
    "title": "Blogs",
    "section": "",
    "text": "Parameterized Reports with Quarto: R-Ladies Abuja Workshop\n\n\n\n\n\n\nR\n\n\nQuarto\n\n\nparameterized reports\n\n\nworkshop\n\n\n\n2-hour code-along workshop on parameterized reports with Quarto\n\n\n\n\n\nFeb 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nParameterized Reports with Quarto: R-Ladies DC Workshop\n\n\n\n\n\n\nR\n\n\nQuarto\n\n\nparameterized reports\n\n\nworkshop\n\n\n\n2-hour code-along workshop on parameterized reports with Quarto\n\n\n\n\n\nJan 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nParameterized Quarto reports improve understanding of soil health\n\n\n\n\n\n\nR\n\n\nQuarto\n\n\nparameterized reports\n\n\nagriculture\n\n\nsoil health\n\n\n\nCreating custom soil health reports with Quarto\n\n\n\n\n\nSep 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nShiny optimization of climate benefits from a statewide agricultural grant program\n\n\n\n\n\n\nR\n\n\nshiny\n\n\nagriculture\n\n\nclimate\n\n\n\nDevelopment process of {WaCSE} shiny app\n\n\n\n\n\nAug 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWashington Soil Health Initative and Climate Smart Estimator\n\n\n\n\n\n\nagriculture\n\n\nsoil health\n\n\nclimate\n\n\n\nOverview of the WA Soil Health Initative & WA Climate Smart Estimator {WaCSE} shiny app\n\n\n\n\n\nJun 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping & Mapping {orcas} Encounters\n\n\n\n\n\n\nR\n\n\nweb scraping\n\n\nleaflet\n\n\n\nWeb scraping with {rvest} & mapping with {leaflet}\n\n\n\n\n\nApr 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWashington Climate Smart Estimator: Using ArcGIS Dashboards and Experience Builder\n\n\n\n\n\n\nArcGIS\n\n\nagriculture\n\n\nclimate\n\n\n\nOverview of WA Climate Smart Estimator ArcGIS Experience web app\n\n\n\n\n\nMay 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nAquatic Risk Assessment: Organophosphate insecticide mixtures in Washington surface waters\n\n\n\n\n\n\nagriculture\n\n\nwater quality\n\n\n\nAquartic risk assessment of insecticide mixtures in WA streams\n\n\n\n\n\nNov 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nAquatic Risk Assessment: Organophosphate insecticide mixtures in Washington surface waters\n\n\n\n\n\n\nagriculture\n\n\nwater quality\n\n\n\nAquartic risk assessment of insecticide mixtures in WA streams\n\n\n\n\n\nNov 9, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/blog1.html#blogs-notes",
    "href": "blog/blog1.html#blogs-notes",
    "title": "Blogs & Notes",
    "section": "",
    "text": "📝 Blogs\n\n\nA collection of articles and personal writings about machine learning, smart energy systems, and academic reflections.\ncontents: - “blog_*/index.qmd” type: table fields: [title, date] sort: “date desc” categories: true grid-item-border: false date-format: short sort-ui: false filter-ui: false"
  },
  {
    "objectID": "blog/talks/2023-08-19_cascadia_shiny-wacse/index.html",
    "href": "blog/talks/2023-08-19_cascadia_shiny-wacse/index.html",
    "title": "Shiny optimization of climate benefits from a statewide agricultural grant program",
    "section": "",
    "text": "{{&lt; fa display &gt;}} Slides {{&lt; fa brands github &gt;}} Code {{&lt; fa play-circle &gt;}} Video \n\nDetails\n📆 August 19, 2023 // 2:05 pm - 2:20 pm PT\n🏨 Seattle, WA\n🌠 Cascadia R Conf\n\n\nAbstract\nWashington’s Sustainable Farms and Fields program provides grants to growers to increase soil carbon or reduce greenhouse gas (GHG) emissions on their farms. To optimize the climate benefits of the program, we developed the Washington Climate Smart Estimator {WaCSE} using R and Shiny.\nIntegrating national climate models and datasets, this intuitive, regionally specific user interface allows farmers and policymakers to compare the climate benefits of different agricultural practices across Washington’s diverse counties and farm sizes. Users can explore GHG estimates in interactive tables and plots, download results in spreadsheets and figures, and generate PDF reports. In this talk, we present the development process of {WaCSE} and discuss the lessons we learned from creating our first ever Shiny app.\n\n\nSlides\n\n\n\n\nRecording\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2023,\n  author = {Ryan, Jadey and Michel, Leslie and Gelardi, Dani},\n  title = {Shiny Optimization of Climate Benefits from a Statewide\n    Agricultural Grant Program},\n  date = {2023-08-19},\n  url = {https://kundan-kumarr.github.io/blog/talks/2023-08-19_cascadia_shiny-wacse/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey, Leslie Michel, and Dani Gelardi. 2023. “Shiny\nOptimization of Climate Benefits from a Statewide Agricultural Grant\nProgram.” August 19, 2023. https://kundan-kumarr.github.io/blog/talks/2023-08-19_cascadia_shiny-wacse/."
  },
  {
    "objectID": "blog/talks/2021-11-14_setac_insecticides_water/index.html",
    "href": "blog/talks/2021-11-14_setac_insecticides_water/index.html",
    "title": "Aquatic Risk Assessment: Organophosphate insecticide mixtures in Washington surface waters",
    "section": "",
    "text": "{{&lt; fa display &gt;}} Slides\n\nDetails\n📆 November 14, 2021 // 12-minute recording\n🏨 Virtual\n🌠 Society of Environmental Toxicology and Chemistry (SETAC) North America 42nd Annual Meeting\n\n\nAbstract\nEcological risk assessments often do not consider potential additive, synergistic, or antagonistic effects from mixtures of chemicals and instead typically base risk on a single chemical. In the last decade, more tools and models have been developed to consider the interactive effects of chemicals within a mixture when conducting risk assessments. Therefore, this study uses actual environmental concentrations measured in 2018 and 2019 from the Washington State Department of Agriculture’s Surface Water Monitoring Program. Aquatic risk from exposure was assessed from chlorpyrifos, diazinon, and malathion (as individual chemicals and as binary and ternary mixtures) using the concentration addition model. These pesticides were selected because they have a common mechanism of toxicity, are frequently detected in surface waters in Washington, and were recently evaluated in a biological opinion by the National Marine Fisheries Service.\nAll detected concentrations of chlorpyrifos and malathion, assessed as individual chemicals, exceeded the predicted no effect concentration, indicating potential for adverse effects on aquatic life. Further, risk quotients for all binary and ternary mixtures were greater than one, also indicative of potential for adverse effects on aquatic life. In all samples containing a mixture, the maximum cumulative ratio suggested that a single insecticide contributed &gt;50% of the overall toxicity of each mixture. Based on the individual and mixture risk quotients, chlorpyrifos and malathion were the primary drivers of the toxicity of each mixture.\n\n\nSlides\n\n\nOops! Your browser doesn’t seem to support embedded PDFs.\n\n\nTry downloading instead.\n\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2021,\n  author = {Ryan, Jadey},\n  title = {Aquatic {Risk} {Assessment:} {Organophosphate} Insecticide\n    Mixtures in {Washington} Surface Waters},\n  date = {2021-11-14},\n  url = {https://kundan-kumarr.github.io/blog/talks/2021-11-14_setac_insecticides_water/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey. 2021. “Aquatic Risk Assessment: Organophosphate\nInsecticide Mixtures in Washington Surface Waters.” November 14,\n2021. https://kundan-kumarr.github.io/blog/talks/2021-11-14_setac_insecticides_water/."
  },
  {
    "objectID": "blog/talks/2024-02-21_rladies-abuja-quarto-params/index.html",
    "href": "blog/talks/2024-02-21_rladies-abuja-quarto-params/index.html",
    "title": "Parameterized Reports with Quarto: R-Ladies Abuja Workshop",
    "section": "",
    "text": "{{&lt; fa globe &gt;}} Course website {{&lt; fa display &gt;}} Slides {{&lt; fa brands github &gt;}} Code {{&lt; fa play-circle &gt;}} Video \n\nDetails\n📆 February 21, 2024 // 4:30 pm - 6:30 pm WAT\n🏨 Virtual\n🆓 FREE with registration\n🎥 Recording\n🏡 Workshop website\n🔖 Source tag\n\n\nAbstract\nTired of manually adjusting Quarto reports for different regions, time periods, or clients? Dreaming of using just one template to generate both interactive HTML and static Word/PDF versions of your reports?\nJoin our workshop to unlock the power of parameterized reporting with Quarto and leave with your own template and examples to modify for your own projects.\nGet a sneak preview of what you’ll learn by checking out the slides from my posit::conf(2023) talk.\nWe welcome everyone! However, if you’re new to Quarto or functional programming with {purrr}, take a look at the pre-work for some background videos/tutorials.\n\n\nSlides\n\n\n\n\nRecording\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2024,\n  author = {Ryan, Jadey},\n  title = {Parameterized {Reports} with {Quarto:} {R-Ladies} {Abuja}\n    {Workshop}},\n  date = {2024-02-21},\n  url = {https://kundan-kumarr.github.io/blog/talks/2024-02-21_rladies-abuja-quarto-params/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey. 2024. “Parameterized Reports with Quarto: R-Ladies\nAbuja Workshop.” February 21, 2024. https://kundan-kumarr.github.io/blog/talks/2024-02-21_rladies-abuja-quarto-params/."
  },
  {
    "objectID": "blog/talks/2024-01-18_rladies-dc_quarto-params/index.html",
    "href": "blog/talks/2024-01-18_rladies-dc_quarto-params/index.html",
    "title": "Parameterized Reports with Quarto: R-Ladies DC Workshop",
    "section": "",
    "text": "{{&lt; fa globe &gt;}} Course website {{&lt; fa display &gt;}} Slides {{&lt; fa brands github &gt;}} Code {{&lt; fa play-circle &gt;}} Video \n\nDetails\n📆 January 18, 2024 // 6:30 pm - 8:30 pm EDT\n🏨 Virtual\n🆓 FREE with registration\n🏡 Workshop website\n🔖 Source tag\n\n\nAbstract\nTired of manually adjusting Quarto reports for different regions, time periods, or clients? Dreaming of using just one template to generate both interactive HTML and static Word/PDF versions of your reports?\nJoin our workshop to unlock the power of parameterized reporting with Quarto and leave with your own template and examples to modify for your own projects.\nGet a sneak preview of what you’ll learn by checking out the slides for my posit::conf(2023) talk.\nEveryone is welcome to attend. If you’re new to Quarto, we recommend watching Tom Mock’s excellent 2-hour introduction to Quarto.\n\n\nSlides\n\n\n\n\nRecording\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2024,\n  author = {Ryan, Jadey},\n  title = {Parameterized {Reports} with {Quarto:} {R-Ladies} {DC}\n    {Workshop}},\n  date = {2024-01-18},\n  url = {https://kundan-kumarr.github.io/blog/talks/2024-01-18_rladies-dc_quarto-params/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey. 2024. “Parameterized Reports with Quarto: R-Ladies DC\nWorkshop.” January 18, 2024. https://kundan-kumarr.github.io/blog/talks/2024-01-18_rladies-dc_quarto-params/."
  },
  {
    "objectID": "blog/talks/2021-11-09_awra_insecticides_water/index.html",
    "href": "blog/talks/2021-11-09_awra_insecticides_water/index.html",
    "title": "Aquatic Risk Assessment: Organophosphate insecticide mixtures in Washington surface waters",
    "section": "",
    "text": "Slides\n\nDetails\n📆 November 9, 2021 // 1:30 pm - 1:50 pm PT\n🏨 Virtual\n🌠 American Water Resources Association (AWRA)\n\n\nAbstract\nEcological risk assessments often do not consider potential additive, synergistic, or antagonistic effects from mixtures of chemicals and instead typically base risk on a single chemical. In the last decade, more tools and models have been developed to consider the interactive effects of chemicals within a mixture when conducting risk assessments. Therefore, this study uses actual environmental concentrations measured in 2018 and 2019 from the Washington State Department of Agriculture’s Surface Water Monitoring Program. Aquatic risk from exposure was assessed from chlorpyrifos, diazinon, and malathion (as individual chemicals and as binary and ternary mixtures) using the concentration addition model. These pesticides were selected because they have a common mechanism of toxicity, are frequently detected in surface waters in Washington, and were recently evaluated in a biological opinion by the National Marine Fisheries Service.\nAll detected concentrations of chlorpyrifos and malathion, assessed as individual chemicals, exceeded the predicted no effect concentration, indicating potential for adverse effects on aquatic life. Further, risk quotients for all binary and ternary mixtures were greater than one, also indicative of potential for adverse effects on aquatic life. In all samples containing a mixture, the maximum cumulative ratio suggested that a single insecticide contributed &gt;50% of the overall toxicity of each mixture. Based on the individual and mixture risk quotients, chlorpyrifos and malathion were the primary drivers of the toxicity of each mixture.\n\n\nSlides\n\n\nOops! Your browser doesn’t seem to support embedded PDFs.\n\n\nTry downloading instead.\n\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2021,\n  author = {Ryan, Jadey},\n  title = {Aquatic {Risk} {Assessment:} {Organophosphate} Insecticide\n    Mixtures in {Washington} Surface Waters},\n  date = {2021-11-09},\n  url = {https://kundan-kumarr.github.io/blog/talks/2021-11-09_awra_insecticides_water/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey. 2021. “Aquatic Risk Assessment: Organophosphate\nInsecticide Mixtures in Washington Surface Waters.” November 9,\n2021. https://kundan-kumarr.github.io/blog/talks/2021-11-09_awra_insecticides_water/."
  },
  {
    "objectID": "blog/talks/2022-05-25_wagisa_arcgis-wacse/index.html",
    "href": "blog/talks/2022-05-25_wagisa_arcgis-wacse/index.html",
    "title": "Washington Climate Smart Estimator: Using ArcGIS Dashboards and Experience Builder",
    "section": "",
    "text": "{{&lt; fa file-pdf &gt;}} Slides {{&lt; fa play-circle &gt;}} Recording \n\nDetails\n📆 May 25, 2022 // 2:30 pm - 3:00 pm PT\n🏨 Leavenworth, WA\n🌠 Washington GIS Association (WAGISA) conference\n\n\nAbstract\nWashington’s Sustainable Farms and Fields (SFF) program provides financial incentives to growers who implement climate-smart practices that sequester soil carbon or reduce greenhouse gas emissions. The climate change mitigation potential of different on-farm practices depends on many site-specific factors such as geography, climate, soil type, and management history.\nTo maximize the climate change mitigation potential of the SFF program, and to optimize the use of every dollar, a decision support tool is required. This presentation demonstrates how two existing spatial datasets (NRCS’ COMET-Planner and WSDA’s Agricultural Land Use) can be integrated into a decision support tool using ArcGIS Dashboards and Experience Builder. This tool, called the Washington Climate Smart Estimator (WaCSE), allows users to compare the climate benefits of different agricultural practices across different counties in Washington.\nBy utilizing the intuitive user interface of ArcGIS Dashboards, WaCSE enables swift, science-based estimates of climate benefits, while remaining accessible for audiences of varied technical backgrounds.\nSee the old app built with ArcGIS.\n\n\nSlides\n\n\nOops! Your browser doesn’t seem to support embedded PDFs.\n\n\nTry downloading instead.\n\n\n\n\nRecording\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2022,\n  author = {Ryan, Jadey and Michel, Leslie and Gelardi, Dani},\n  title = {Washington {Climate} {Smart} {Estimator:} {Using} {ArcGIS}\n    {Dashboards} and {Experience} {Builder}},\n  date = {2022-05-25},\n  url = {https://kundan-kumarr.github.io/blog/talks/2022-05-25_wagisa_arcgis-wacse/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey, Leslie Michel, and Dani Gelardi. 2022. “Washington\nClimate Smart Estimator: Using ArcGIS Dashboards and Experience\nBuilder.” May 25, 2022. https://kundan-kumarr.github.io/blog/talks/2022-05-25_wagisa_arcgis-wacse/."
  },
  {
    "objectID": "blog/talks/2023-04-20_rladies_orcas-web-scraping/index.html",
    "href": "blog/talks/2023-04-20_rladies_orcas-web-scraping/index.html",
    "title": "Web Scraping & Mapping {orcas} Encounters",
    "section": "",
    "text": "{{&lt; fa display &gt;}} Slides {{&lt; fa brands github &gt;}} Code \n\nDetails\n📆 April 20, 2023 // 15-minute talk\n🏨 Seattle, WA\n🆓 R-Ladies Seattle and Seattle useR Group\n\n\nAbstract\nR-Ladies Seattle invited me to give a talk for the ‘R in the Outdoors’ meetup. This was my first in-person talk of my professional career! I used this as an opportunity to learn new skills through a personal project. I’ve always had an affinity for the Southern Resident Killer Whales in the Salish Sea. The Center for Whale Research does a lot of really fascinating and important work monitoring their population. They post their survey data on their website; each encounter with the orcas is a separate webpage. Lately, I’ve been curious and intimidated by web scraping so I decided this would make a great case study and personal project.\nI ended up also going to the Seattle useR Group lightning talks meetup afterwards and spontaneously gave the same presentation there!\n\n\nSlides\n\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2023,\n  author = {Ryan, Jadey},\n  title = {Web {Scraping} \\& {Mapping} \\{Orcas\\} {Encounters}},\n  date = {2023-04-20},\n  url = {https://kundan-kumarr.github.io/blog/talks/2023-04-20_rladies_orcas-web-scraping/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey. 2023. “Web Scraping & Mapping {Orcas}\nEncounters.” April 20, 2023. https://kundan-kumarr.github.io/blog/talks/2023-04-20_rladies_orcas-web-scraping/."
  },
  {
    "objectID": "blog/talks/2023-09-25_posit_parameterized-quarto/index.html",
    "href": "blog/talks/2023-09-25_posit_parameterized-quarto/index.html",
    "title": "Parameterized Quarto reports improve understanding of soil health",
    "section": "",
    "text": "{{&lt; fa display &gt;}} Slides {{&lt; fa brands github &gt;}} Code {{&lt; fa play-circle &gt;}} Video \n\nDetails\n📆 September 25, 2023 // 5:30 pm - 5:40 pm CDT 🏨 Chicago, IL\n🌠 posit::conf(2023)\n\n\nAbstract\nSoil sampling data are notoriously challenging to tidy and effectively communicate to farmers. We used functional programming with the tidyverse to reproducibly streamline data cleaning and summarization. To improve project outreach, we developed a Quarto project to dynamically create interactive HTML reports and printable PDFs. Custom to every farmer, reports include project goals, measured parameter descriptions, summary statistics, maps, tables, and graphs.\nOur case study presents a workflow for data preparation and parameterized reporting, with best practices for effective data visualization, interpretation, and accessibility.\nSee an example HTML report.\nLearn more about the Washington Soil Health Initiative State of the Soils Assessment.\n\n\nSlides\n\n\n\n\nRecording\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2023,\n  author = {Ryan, Jadey and McIlquham, Molly and Sarpong, Kwabena and\n    Michel, Leslie and Potter, Teal and Griffin LaHue, Deirdre and\n    Gelardi, Dani},\n  title = {Parameterized {Quarto} Reports Improve Understanding of Soil\n    Health},\n  date = {2023-09-25},\n  url = {https://kundan-kumarr.github.io/blog/talks/2023-09-25_posit_parameterized-quarto/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey, Molly McIlquham, Kwabena Sarpong, Leslie Michel, Teal\nPotter, Deirdre Griffin LaHue, and Dani Gelardi. 2023.\n“Parameterized Quarto Reports Improve Understanding of Soil\nHealth.” September 25, 2023. https://kundan-kumarr.github.io/blog/talks/2023-09-25_posit_parameterized-quarto/."
  },
  {
    "objectID": "blog/talks/2023-06-13_wade_washi-wacse/index.html",
    "href": "blog/talks/2023-06-13_wade_washi-wacse/index.html",
    "title": "Washington Soil Health Initative and Climate Smart Estimator",
    "section": "",
    "text": "{{&lt; fa display &gt;}} Slides {{&lt; fa play-circle &gt;}} Video \n\nDetails\n📆 June 13, 2023 // 1:30 pm - 2:20 pm PT\n🏨 Leavenworth, WA\n🌠 Washington Association of District Employees (WADE) conference\n\n\nAbstract\nWashington Soil Health Initiative overview and updates.\nHow to get the most of the Sustainable Farms and Fields Washington Climate Smart Estimator (WaCSE) tool.\n\n\nSlides\n\n\nOops! Your browser doesn’t seem to support embedded PDFs.\n\n\nTry downloading instead.\n\n\n\n\nRecording\n\n\n\n\n\nCitationBibTeX citation:@online{ryan2023,\n  author = {Ryan, Jadey and Michel, Leslie and Gelardi, Dani},\n  title = {Washington {Soil} {Health} {Initative} and {Climate} {Smart}\n    {Estimator}},\n  date = {2023-06-13},\n  url = {https://kundan-kumarr.github.io/blog/talks/2023-06-13_wade_washi-wacse/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRyan, Jadey, Leslie Michel, and Dani Gelardi. 2023. “Washington\nSoil Health Initative and Climate Smart Estimator.” June 13,\n2023. https://kundan-kumarr.github.io/blog/talks/2023-06-13_wade_washi-wacse/."
  },
  {
    "objectID": "blog/blog2.html",
    "href": "blog/blog2.html",
    "title": "Blog and notes",
    "section": "",
    "text": "Blog\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nPersonal Highlights: Positconf 2024\n\n\n11/5/24\n\n\n\n\nUse Quarto, Make Friends: a two-year journey\n\n\n9/23/24\n\n\n\n\nPersonal Highlights: Positconf 2023\n\n\n9/21/23\n\n\n\n\nPersonal Highlights: CEN2023\n\n\n9/6/23\n\n\n\n\nTransforming medical statistics classroom with R and Quarto\n\n\n7/17/23\n\n\n\n\nCourse review: making DS work for clinical reporting\n\n\n3/1/23\n\n\n\n\nOpen source reporting with R: clinical, public health, RSE and embrace the change\n\n\n1/13/23\n\n\n\n\nqtwAcademic: a quick and easy way to start your Quarto website\n\n\n1/5/23\n\n\n\n\nWebsite reboot: switching from Blogdown to Quarto\n\n\n1/3/23\n\n\n\n\n\nNo matching items\n\n\n\n\nTechnical notes\nMost of the technical notes are in the newly built note repository, Data Apothecary’s Notes. Please feel free to reach out if you found any errors!\nI’d be glad if it helps you in some way.\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nResearch Scientist Interview Guide\n\n\n7/3/25\n\n\n\n\nNotes: The Book of OHDSI - Data Analytics\n\n\n5/6/24\n\n\n\n\nStyling your quarto project\n\n\n10/18/23\n\n\n\n\nUse WebR in your existing quarto website\n\n\n10/1/23\n\n\n\n\nR package workflow\n\n\n5/19/23\n\n\n\n\nNotes: Making Data Science work for Clinical Reporting - Part 4\n\n\n3/1/23\n\n\n\n\nNotes: Making Data Science work for Clinical Reporting - Part 3\n\n\n2/27/23\n\n\n\n\nTesting Shiny app and deploy to shinyapps.io\n\n\n2/25/23\n\n\n\n\nNotes: Making Data Science work for Clinical Reporting - Part 2\n\n\n2/22/23\n\n\n\n\nR package website with pkgdown\n\n\n2/20/23\n\n\n\n\nNotes: Making Data Science work for Clinical Reporting - Part 1\n\n\n2/6/23\n\n\n\n\nPublishing Quarto Website with GitHub Pages\n\n\n1/11/23\n\n\n\n\n\nNo matching items\n\n\n\n\nReading notes\nThis section is constantly being updated.\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nBad Pharma: How medicine is broken, and how we can fix it - Ben Goldacre\n\n\n6/6/24\n\n\n\n\nWorking in Public: The Making and Maintenance of Open Source Software - Nadia Eghbal\n\n\n2/18/24\n\n\n\n\nPreventable: How a Pandemic Changed the World & How to Stop the Next One - Devi Sridhar\n\n\n3/17/23\n\n\n\n\nHow to prevent the next pandemic - Bill Gates\n\n\n1/4/23\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/talks/technotes_20230205_clinreport_part1/index.html",
    "href": "blog/talks/technotes_20230205_clinreport_part1/index.html",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 1",
    "section": "",
    "text": "This is a course provided by Genentech (part of Roche) on Coursera.\nCourse link"
  },
  {
    "objectID": "blog/talks/technotes_20230205_clinreport_part1/index.html#introduction-to-clinical-trial",
    "href": "blog/talks/technotes_20230205_clinreport_part1/index.html#introduction-to-clinical-trial",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 1",
    "section": "Introduction to clinical trial",
    "text": "Introduction to clinical trial\nClinical trial: aim to demonstrate that drug is safe and effective (safety, efficacy)\n\nphase 1: 10-20 people, focus on safety (healthy volunteers)\nphase 2: 100, study of side effects, determine best dose\nphase 3: 1000, demonstrate drug efficacy, fuller safety profile (common across multiple regions, ethnicities)\n\ncollecting data from different hospitals, hence important to ensure standards are being followed\n\nevidence must be submitted to health authorities (FDA, EMA European medicines agency)\nhealth authorities determine whether the drug is submitted to market\n\nSubmit the analysis plan in advance"
  },
  {
    "objectID": "blog/talks/technotes_20230205_clinreport_part1/index.html#why-share-data",
    "href": "blog/talks/technotes_20230205_clinreport_part1/index.html#why-share-data",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 1",
    "section": "Why share data",
    "text": "Why share data\n\nregulatory requirements\nscientific community interest\ncompany internal research interest\nmarketing materials\n\n\nData and results sharing\n\nRegulatory req (e.g. EMA require sharing clinical trial results to gain marketing authorization for pharma products, FDA require sharing data)\nscientific community (peer review check accuracy, perform additional analyses, derive new hypothesis)\nCDISC standards\n\nCDASH (clinical data acquisition standards harmonization)\nSDTM (study data tabulation model)\n\nformat for ‘raw’ data, define datasets, structures, contents, variable attributes\n\nSEND (standard for exchange of non clinical data)\nADaM (analysis data model)\n\ndata format for data processed for analysis (e.g. converted, imputed, derived)\n\n\nDictionary\n\ne.g. nose congestion, stuffy nose, … need to be standardized\nMedDRA: standard dictionary for medical conditions, events and procedures\nWHO drug dictionary (for pharma agents)\n\nSAP statistical analysis plan\n\nbased on study protocol, focus on statistical methodology, is regulated\n\nProgramming specification\n\nbased on SAP, provides additional details on datasets and tables, listing and figures (TLFs) required for statistical analysis. focus on programming details. Not regulated\n\n\n\n\nQuality assurance\n\nGood clinical practice (GCP), issued by ICH\npurpose: prevent mistakes, reduce inefficiencies/waste in a process, increase reliability/trustworthiness of the product of a process\nClinical monitoring: performed by a clinical research assistant (CRA) at investigator sites, checks that study protocols are executed as intended, and site processes result in accurate data capture. Focus on trial subjects’ safety. Traditional goal: 100% source data verification\nData quality checks (more relevant for data scientists). Checks data for technical conformance, and data plausibility. Focus on data quality. Traditional goal: 100% accurate and format compliant data\nCode review\nDouble programming\n\n\n\nData access restrictions\n\nreasons\n\ndata collected is very sensitive (health data), need data protection\nclinical trial data is a key asset and revenue predictor for pharma companies, high confidentiality levels\nscientific validity, data is ideally double blinded, no-one should know whether a subjecttreatment is as long as the data is still being collected\n\npseudonymization: data de-identification\n\nuse pseudonym (ID), link is recorded to allow re-identification\n\nanonymization: limit the risk of re-identification\n\nremove variables, remove values, replace more precise values with more general categories, replace personal identifiers with random identifiers\n\nFSP, CRO (out-sourcing), personnels require data access at different levels\nUnblinding"
  },
  {
    "objectID": "blog/talks/readnotes_20240218_open_source/index.html",
    "href": "blog/talks/readnotes_20240218_open_source/index.html",
    "title": "Working in Public: The Making and Maintenance of Open Source Software - Nadia Eghbal",
    "section": "",
    "text": "Github as a platform\n\nOn contribution\nNearly half of all contributors only contributed once; which accounted for less than 2% of total commits.\nThe pattern that one or a few developers do most of the work, followed by many casual contributors and even more passive users is the norm, not exception in open source.\nOn casual contributors: they primarily see themselves as users of the project, rather than a part of a contributor community.\nChallenge for maintainers: not how to get more contributors, but how to manage high volume of frequent, low-touch interactions (directing air traffic)\n\nGithub’s open source developers have more in common with solo creators on Twitter, Instagram, YouTube or Twitch.\n\nComparing early internet and social platform nowadays: the early online communities have mailing lists, online forums, membership groups, operated like villages that have their own culture, history and norms. Nowadays creators have much bigger potential audience but the relationship is one-sided, and can be overwhelming.\n\n\nOn free software and hacker\n“Free” means you are able to do what you want with the software, rather than the cost. Libre rather than gratis. At least at the beginning.\nBravado, showmanship, mischievousness, deep mistrust of authority. This culture in the 1980s and 90s was closely linked to the early open source software.\n“Bazaar”: highly participatory, versus “Cathedral”: restricted to a smaller group\nToday’s developer hardly even notice “open source” as a concept anymore, they just want to write and publish their code. They prioritize convenience over freedom or openness.\n\n\nOn licensing\nThe widespread use of permissive licensing is popularized by GH.\nCopyleft licensing (e.g. GNU General Public License GPL) is not commercial friendly as it requires companies to license their software that depend on open source GPL software to have the same license. However GPL gives developers more control over how others use their code in the long run.\n\nAs with any other online content today, sharing is the default.\n\n\n\n\nThe structure of an open source software\n\nOn how projects evolve\nCreate -&gt; Promote and distribute -&gt; Grow\nProjects are promoted like a founder would promote a startup: share on the relevant channels online, give talks at conference and meetups, encourage others to write and talk about it\nA sign that the software is used widely: when the maintainer starts doing more non-code (triage issues, review pull requests) rather than code work.\n\n\nContributor and users\nDepends on technical scope (whether there is much to do), support required (code and admin work), ease of participation (whether on Github) and user adoption (potential contributor base).\nFour types of projects\n\nhigh user growth, high contributor growth: federations. Rare, impactful, the ‘ideal’ of open source project. Roughly 3% of open source projects. Examples: Rust, Node.js, Linux\nhigh user growth, low contributor growth: stadiums: powered by one or a few developers. Centralized.\nlow user growth, high contributor growth: club. Similar to meetup or hobby groups, do not have a wide reach but are loved and built by enthusiasts.\nlow user growth, low contributor growth: toys. Personal project, isn’t trying to grow its user base. Projects on Github with less than 10 stars. Authors do not expect to receive contributions nor do they care about whether people are watching.\n\nDecentralized communities (clubs and federations) have the potential for high user growth - recruit new contributors, reduce contribution friction.\nCentralized communities (stadium) depends on the creators to manage user demand - automation, elimination of noise\n\n\n\nRoles, incentives and relationships\n\nFirms or communities\nFirms (companies, organizations): centralized resources; from a coordination standpoint, managing resources would be more efficient within the same organization - which does not explain why open source developers make software together without formal contracts and financial compensation.\n\n\nThe commons and peer production\nTragedy of the commons: resources depleted by people acting in their own self-interest rather than in the collective interest.\n(One of the 8 design principles by Ostrom on) successful commons:\n\nThose who are affecteed by the rules can participate in modifying them.\n\nStrong sense of group identity maks rules, dispute resolution more meaningful.\nCoordination cost is lower when self-organized based on who wants to do the work most, anyone can do the advertised work and volunteer.\nIn contrast, in companies - solicit, evaluate, hire, manage employees; only employees can do the work limited by their job functions.\nPeople collaborating online for no obvious reason beyond personal satisfaction (intrinsic motivation)\nModular and granular tasks: how tasks are organized, and how big each task is.\nLow coordination costs: quality control over thee modules, integrate the contributions into the finished product\n\n\nContribution beyond code\nSome users do not consider them a contributor, but do actually contribute by education, spreading the word, support (forum), bug reports and more.\nThese active users are similar to contributors but operate independently from project’s contributor community.\n\n\n\n\n\nCitationBibTeX citation:@online{zhang2024,\n  author = {Zhang, Chi},\n  title = {Working in {Public:} {The} {Making} and {Maintenance} of\n    {Open} {Source} {Software} - {Nadia} {Eghbal}},\n  date = {2024-02-18},\n  url = {https://kundan-kumarr.github.io/blog/talks/readnotes_20240218_open_source/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nZhang, Chi. 2024. “Working in Public: The Making and Maintenance\nof Open Source Software - Nadia Eghbal.” February 18, 2024. https://kundan-kumarr.github.io/blog/talks/readnotes_20240218_open_source/."
  },
  {
    "objectID": "blog/talks/blog_20241105_positconf2024/index.html",
    "href": "blog/talks/blog_20241105_positconf2024/index.html",
    "title": "Personal Highlights: Positconf 2024",
    "section": "",
    "text": "Tyler Morgan-wall: Quarto, AI, and the Art of Getting Your Left Back\nTransform old blog (Wordpress) to Quarto. Quarto can render pre-exisiting HTML\nCustomize website with AI, with css\nImage carousel, transform some grid images into an auto rotating carousel (JS). Can also add screenshot; explain what the code is doing\n\n\nAlenka Frim and Nic Crane: Mixing R, Python, and Quarto: Crafting the Perfect Open Source Cocktail\nComparing the tools to make dashboards:\n\n\n\n\n\n\n\n\n\n\n\nDashboard aesthetic\nMarkdown syntax\nDeploy with GH actions\nBoth R and python\n\n\n\n\nShiny\nYes\nNo\nNo / Yes with Shinylive\nYes\n\n\nQuarto doc\nNo\nYes\nYes\nYes\n\n\nQuarto dashboard\nYes\nYes\nYes\nYes\n\n\n\nEngine: knitr and jupyter\nSean Nguyen: Beyond Dashboards: Dynamic Data Storytelling with Python, R, and Quarto Emails\nRemove friction of too much dashboards\n\nlogging in can create barriers\nmeeting executives where they are (email, for example)\n“no-click” insights (e.g. add key metric/alerts in the subject line or notification)\n\nTools: quarto emails, pins (a package), posit connect\n\ndata sources -&gt; data warehouse (big query) -&gt; quarto (create pins.qmd) -&gt; pins (marketing data.csv pin, for example)\nchange quarto format to email\ngenerate multiple using purrr"
  },
  {
    "objectID": "blog/talks/blog_20241105_positconf2024/index.html#quarto",
    "href": "blog/talks/blog_20241105_positconf2024/index.html#quarto",
    "title": "Personal Highlights: Positconf 2024",
    "section": "",
    "text": "Tyler Morgan-wall: Quarto, AI, and the Art of Getting Your Left Back\nTransform old blog (Wordpress) to Quarto. Quarto can render pre-exisiting HTML\nCustomize website with AI, with css\nImage carousel, transform some grid images into an auto rotating carousel (JS). Can also add screenshot; explain what the code is doing\n\n\nAlenka Frim and Nic Crane: Mixing R, Python, and Quarto: Crafting the Perfect Open Source Cocktail\nComparing the tools to make dashboards:\n\n\n\n\n\n\n\n\n\n\n\nDashboard aesthetic\nMarkdown syntax\nDeploy with GH actions\nBoth R and python\n\n\n\n\nShiny\nYes\nNo\nNo / Yes with Shinylive\nYes\n\n\nQuarto doc\nNo\nYes\nYes\nYes\n\n\nQuarto dashboard\nYes\nYes\nYes\nYes\n\n\n\nEngine: knitr and jupyter\nSean Nguyen: Beyond Dashboards: Dynamic Data Storytelling with Python, R, and Quarto Emails\nRemove friction of too much dashboards\n\nlogging in can create barriers\nmeeting executives where they are (email, for example)\n“no-click” insights (e.g. add key metric/alerts in the subject line or notification)\n\nTools: quarto emails, pins (a package), posit connect\n\ndata sources -&gt; data warehouse (big query) -&gt; quarto (create pins.qmd) -&gt; pins (marketing data.csv pin, for example)\nchange quarto format to email\ngenerate multiple using purrr"
  },
  {
    "objectID": "blog/talks/blog_20241105_positconf2024/index.html#python",
    "href": "blog/talks/blog_20241105_positconf2024/index.html#python",
    "title": "Personal Highlights: Positconf 2024",
    "section": "Python",
    "text": "Python\nEmily Riederer: Python Rgonomics\nPython alternatives to R. Worth rewatching!"
  },
  {
    "objectID": "blog/talks/blog_20241105_positconf2024/index.html#teaching-and-education",
    "href": "blog/talks/blog_20241105_positconf2024/index.html#teaching-and-education",
    "title": "Personal Highlights: Positconf 2024",
    "section": "Teaching and education",
    "text": "Teaching and education\nAndrew Gard: Teaching and learning data science in the era of AI\nStudents don’t know enough to be able to edit the prompt to reach a sensible code chunk, AI guessed and guessed wrong. We should not expect AI to guess information that we do not provide!\nStudents should still learn to code, and teachers should ask better questions - instead of asking for the final result (create a bar plot), ask students to critically think: why doesn’t the AI-generated code work? what information is missing? how do you improve the prompt?\nJames Wade: Posit Academy in the Age of Generative AI - Lessons from the Frontlines\nchattr, gptstudio, github copilot\nPosit Academy learners (over half) give AI code assistants 2 star rating or less\nRewarding, high-growth period. Threshold concepts: once understood, transforms your perception and approach of a discipline, and these must be encountered not told.\nTC in DS:\n\ntidy data enables efficient analysis\nmodular code enhances re-usablity and clarity\nvisualization as a tool for exploration and communication\n\nHow to incorporate AI code assistants (in DS class)\n\nearly stage: explain this code piece by piece\nmid stage: add a roxygen skeleton to my code\nlate stage: try code assistants in the IDE\n\nTC for code assistants:\n\ndrive faster but don’t forget to steer\nprompting matters, learning how to use these tools is a skill"
  },
  {
    "objectID": "blog/talks/blog_20241105_positconf2024/index.html#statistics",
    "href": "blog/talks/blog_20241105_positconf2024/index.html#statistics",
    "title": "Personal Highlights: Positconf 2024",
    "section": "Statistics",
    "text": "Statistics\nHannah Frick: tidymodels for time-to-event data\nMax Kuhn: Evaluating time-to-event models is hard\nDemetri Pananos - Making sense of marginal effects"
  },
  {
    "objectID": "blog/talks/technotes_20230301_clinreport_part4/index.html",
    "href": "blog/talks/technotes_20230301_clinreport_part4/index.html",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "",
    "text": "This is a course provided by Genentech (part of Roche) on Coursera.\nCourse link"
  },
  {
    "objectID": "blog/talks/technotes_20230301_clinreport_part4/index.html#open-source-packages",
    "href": "blog/talks/technotes_20230301_clinreport_part4/index.html#open-source-packages",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "Open source packages",
    "text": "Open source packages\nExmample:\n\nsurvival: 8 developers, &gt;18 years\nadmiral: 25 developers, &gt;1 year\ntern: 77 developers, 5 years\nrtables: 21 developers, 4 years\n\nEngagement across these packages is different, some receive more issues and comments, some receive more code contributions.\nStale: stable? abandoned?\nContribution is highly skewed, a few contributors write the majority of the code.\nR package life cycles (indicative, not guaranteed)\n\nexperimental (ready to use?)\nstable (safe to use?)\ndeprecated, no longer maintained\nsuperseded, something better exists\n&lt;1.0: big changes likely; &gt;=v1.0: is it safe to use?"
  },
  {
    "objectID": "blog/talks/technotes_20230301_clinreport_part4/index.html#risk-mitigation-for-r-packages",
    "href": "blog/talks/technotes_20230301_clinreport_part4/index.html#risk-mitigation-for-r-packages",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "Risk mitigation for R packages",
    "text": "Risk mitigation for R packages\nCombine external and internal packages (CI/CD release)\n-&gt; automated package data collection\n-&gt; automated quality checks: if not pass, assess\n-&gt; package repo integration tests\n-&gt; publish to package repo, generate package validation report"
  },
  {
    "objectID": "blog/talks/technotes_20230301_clinreport_part4/index.html#assess-external-packages-for-statistical-methods",
    "href": "blog/talks/technotes_20230301_clinreport_part4/index.html#assess-external-packages-for-statistical-methods",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "Assess external packages for statistical methods",
    "text": "Assess external packages for statistical methods\nDoes it provide the required functionality?\n\nCorrect statistical method?\nCould you extend it?\nCorrect results? (compare with another software)\nDo you understand the method? (check the paper linked with package)\n\nDoes it work reliably?\n\nPublished? (e.g. on CRAN)\nDifferent inputs?\nFast?\nDo other people use it? (downloads)\nDoes other software use it? (reverse dependencies)\n\nDoes the code look robust and well tested?\n\nHow are the functions implemented\nIs the source code readable\nCoverage with unit tests\nMature package?\n\nIs it well documented?\n\nDocumented functions?\nVignettes?\nPublished?\nInformative NEWS entry?\n\nWho are the authors, are they responsive?\n\nDid they publish statistics papers on this topic\nIs a github site with issues available"
  },
  {
    "objectID": "blog/talks/technotes_20230301_clinreport_part4/index.html#tools",
    "href": "blog/talks/technotes_20230301_clinreport_part4/index.html#tools",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "Tools",
    "text": "Tools\ncovr and unit tests\nriskmetric and the R Validation Hub\npharmaverse.org, with end-to-end examples"
  },
  {
    "objectID": "blog/talks/blog_20230904_cen2023/index.html",
    "href": "blog/talks/blog_20230904_cen2023/index.html",
    "title": "Personal Highlights: CEN2023",
    "section": "",
    "text": "The IBS (International Biometric Society) conference of the Central European Network, CEN2023 has been a great opportunity to keep myself up to date with the latest development of biostatistics, both in academia and industry. Thanks to the great effort made by the organizing committee and almighty Google Meet/Zoom, I have been able to follow the talks without any issue, and have definitely learned a lot.\nGiven my background, I paid more attention on talks and workshops on\n\nStatistical software, R programming and simulation\nCausal inference\n\nThere were also two topics that drew my attention: one is on statistical education towards medial professionals, the other is on a Data Challenge using RCT data.\n\nStatistical software\n\nSoftware Engineering Working Group (SWE WG), MMRM\n\n\n\n\n\n\nTalk information\n\n\n\n\n\n\nDaniel Sabanes Bove (Roche). First year of the Software Engineering working group - working together across organizations\nGonzalo Duran-Pacheco (Roche). Comparing R libraries with SAS’s PROC MIXED for the analysis of longitudinal continuous endpoints using MMRM\n\n\n\n\nThe ASA Biopharmaceutical Section (BIOP) Software Engineering Working Group SWE WG was established in 2022. Currently they have 3 work streams:\n\nmmrm implements MMRM (mixed models with repeated measures)\nbrms.mmrm, the Bayesian version of MMRM\nHealth Technology Assessment with R\n\nAt a later talk, mmrm was compared with SAS’s PROC MIXED and R’s nlme, glmmTMB for analyses of longitudinal continuous endpoints. In terms of speed and convergence, mmrm is superior than others; while the estimate prodouced by mmrm is very close to PROC MIXED and glmmTMB.\nThis looks like a very interesting tool to try out! Vignette\n\n\nSimulation tools and RWD\n\n\n\n\n\n\nTalk information\n\n\n\n\n\n\nMichael Kammer (Medical University of Vienna). An overview of R software tools to support simulation studies: towards standardizing coding practices.\n\n\n\n\nKammer and colleagues did a review on R packages for simulation, and selected 14 top simulation packages, including simstudy, simdata, synthpop, bigsimr and others. The full list is made available here.\nA real-world dataset, NHANES was also introduced here. The data can be accessed with R package nhanesA.\n\n\n\nCausal Inference\n\n\n\n\n\n\nInformation\n\n\n\n\n\nWorkshop: Implementing the estimand framework in global drug development: Application of causal inference approaches (Mouna Akacha, Björn Bornkamp, Alex Ocampo, Jiawei Wei at Novartis)\nKeynote: Ruth Keogh (LSHTM). Causal inference with observational data: A survival guide\n\n\n\nThese two workshop / talk cover slightly different scenarios: one in RCT, one for observational data. It deserves a whole article or more to elaborate on this topic, so I’m only putting some resources here.\nCausal inference is definitely gaining traction in recent years in both academia and industry. Techniques such as g-computation, IPW and doubly robust estimation are starting to become mainstream. It is fascinating that these techniques themselves are not bound to a fixed model.\nResources:\n\nWorkshop repository Causal-inference-in-RCTs\nBook: Causal Inference: What If by Hernán and Robins (2020)\nPrincipal stratum strategy, Bornkamp et al. (2021)\nTime-dependent covariates, Keogh et al. (2023)\nTarget Trial Emulation (TTE), Hernán and Robins (2016)\n\n\n\nCovariate adjustment and data challenge with RCT data\n\n\n\n\n\n\nTalk information\n\n\n\n\n\n\nKelly Van Lancker (Ghent University). Improving Power in Randomized Trials by Leveraging Baseline Variables\nDominic Magirr (Novartis). Organizing a Data Challenge on Covariate Adjustment in RCTs\nCraig Wang (Novartis). Participating in a Data Challenge on Covariate Adjustment in RCTs\n\nPanel discussion: Jonathan Bartlett (LSHTM)\n\n\n\n23 teams at Novartis participated in a Data Challenge on Covariate Adjustment. They were given a fixed outcome model, and 5 prior studies trial data, and their task was to create the design matrices that improve the precision compared to unadjusted data.\nIf I were to select talks based on the category titles, I would probably missed the whole session. However, it is surprisingly similar to using not trial, but real-world data (such as EHR) to make predictions. The conclusion were similar as well: using “supercovariates” created by ML isn’t gaining much compared to simple models such as ANCOVA. Possible reasons:\n\nsmall to moderate data size\nlinear relationship between covariates and outcome\ngood enough prognostic variables\n\nIt was also mentioned that the winning team did some trick to reduce the variance among the covariates. Would be interesting to read about it.\nSome resources:\n\nLancker et al. The use of covariate adjustment in randomized controlled trials: an overview link\nCovariate adjustment tutorial, link\n\n\n\nStatistical education\n\n\n\n\n\n\nTalk information\n\n\n\n\n\n\nMaren Vens et al (University of Lübeck). Biostatistics/Biometrics for physicians – essential or unnecessary? How do practicing physicians and dentists evaluate biostatistics? A cross-sectional survey\n\n\n\n\nStatistical education to students / professionals who are not used to working with data has always been tricky. Students generally think statistics is difficult, and need help from a statistician. However there are only limited number of statisticians. The talk by Vens and colleagues confirms what practicing statisticians know, but can’t do much about: most (87%) physicians and dentists in the survey need a statistician to help with their work.\nHow to improve the statistical competency is an important and relevant topic for discussion, and might require systematic changes in how it is taught. Use of modern technology can help, yet it’s only helpful when students start to not fear, or not find math and technology boring.\n\n\n\n\nCitationBibTeX citation:@online{zhang2023,\n  author = {Zhang, Chi},\n  title = {Personal {Highlights:} {CEN2023}},\n  date = {2023-09-06},\n  url = {https://kundan-kumarr.github.io/blog/talks/blog_20230904_cen2023/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nZhang, Chi. 2023. “Personal Highlights: CEN2023.” September\n6, 2023. https://kundan-kumarr.github.io/blog/talks/blog_20230904_cen2023/."
  },
  {
    "objectID": "blog/talks/blog_20230301_ds_clinreport/index.html",
    "href": "blog/talks/blog_20230301_ds_clinreport/index.html",
    "title": "Course review: making DS work for clinical reporting",
    "section": "",
    "text": "This is a course provided by Genentech (part of Roche) on Coursera (course link). It is not necessary to have a paid coursera membership to view the course, everyone could access it.\nIt is a 4 part course released one month ago (Jan/Feb 2023), and it seems that a follow-up will be released in the future.\nOverall I think it strikes a good balance between high-level introduction of the good practices, and examples with how they are implemented. Even though the course focuses on clinical reporting in the pharmaceutical industry, the practices are highly relevant in other sectors as well (e.g. public health, academia, other industries that use open-source software).\nSpecific statistical methods, packages are introduced only at a high-level; which means the course is not for learning how to use this or that packages; but good practice guidelines.\nIn my opinion,\n\nit would be useful if the learner has some experience with software development and/or statistics; otherwise learners might not know how to practice them.\nmost of the examples are related to R packages (understandable), so some experience with R package (use or develop) is useful.\nit could be a very good study material for university students in related subjects.\n\n\n\n\nModule 1 (notes): what the requirements are regarding clinical reporting, what should be done to meet the quality standards;\nModule 2 (notes): DevOps and Agile\nModule 3 (notes): version Control, git workflows, reproducible clinical reporting\nModule 4 (notes): code quality, robust and reusable code, R packages\nModule 5 (notes): risk management with open source software\n\n\n\n\nI have a few years of experience as an R developer and academic researcher in related fields, so not all concepts are new to me. Nevertheless, I still learned quite a bit. For example,\n\n(Module 1) Data and results sharing needs to follow certain standards, such as CDISC; there are different industry standards to follow when it comes to data acquisition, tabulation and analysis (e.g. ADaM)\n(Module 2) Data scientists not only need hard skills, but also soft skills - they need to be able to wear many hats, and be more flexible and resilient.\n(Module 4, 5) Tests are extremely important. Think afar, develop your package so that they can be extended in the future. Design your package first, don’t start making your package immediately."
  },
  {
    "objectID": "blog/talks/blog_20230301_ds_clinreport/index.html#each-module",
    "href": "blog/talks/blog_20230301_ds_clinreport/index.html#each-module",
    "title": "Course review: making DS work for clinical reporting",
    "section": "",
    "text": "Module 1 (notes): what the requirements are regarding clinical reporting, what should be done to meet the quality standards;\nModule 2 (notes): DevOps and Agile\nModule 3 (notes): version Control, git workflows, reproducible clinical reporting\nModule 4 (notes): code quality, robust and reusable code, R packages\nModule 5 (notes): risk management with open source software"
  },
  {
    "objectID": "blog/talks/blog_20230301_ds_clinreport/index.html#highlight",
    "href": "blog/talks/blog_20230301_ds_clinreport/index.html#highlight",
    "title": "Course review: making DS work for clinical reporting",
    "section": "",
    "text": "I have a few years of experience as an R developer and academic researcher in related fields, so not all concepts are new to me. Nevertheless, I still learned quite a bit. For example,\n\n(Module 1) Data and results sharing needs to follow certain standards, such as CDISC; there are different industry standards to follow when it comes to data acquisition, tabulation and analysis (e.g. ADaM)\n(Module 2) Data scientists not only need hard skills, but also soft skills - they need to be able to wear many hats, and be more flexible and resilient.\n(Module 4, 5) Tests are extremely important. Think afar, develop your package so that they can be extended in the future. Design your package first, don’t start making your package immediately."
  },
  {
    "objectID": "blog/talks/blog_20240923_quartofriends/index.html",
    "href": "blog/talks/blog_20240923_quartofriends/index.html",
    "title": "Use Quarto, Make Friends: a two-year journey",
    "section": "",
    "text": "I wrote a blog back in early 2023 when I first switched from blogdown to Quarto on my initial impression (read here), and this is a two-year follow-up on my journey since I started using Quarto, for my personal website, teaching, scientific works and collaborative community projects."
  },
  {
    "objectID": "blog/talks/blog_20240923_quartofriends/index.html#quarto-as-a-teaching-tool",
    "href": "blog/talks/blog_20240923_quartofriends/index.html#quarto-as-a-teaching-tool",
    "title": "Use Quarto, Make Friends: a two-year journey",
    "section": "Quarto as a teaching tool",
    "text": "Quarto as a teaching tool\n\nFrom personal to workshop website\nI switched from blogdown to Quarto in late 2022, right after my PhD. It was initially a cure for a severe burnout from a combination of work-related stressors, when I desperately needed something other than research. My mental state was like the famous painting by Norwegian artist Edvard Munch:\n\n\n\n\n\nThe experience of the switch was explained in the previously mentioned blog. Briefly, it was light like a feather. Since I was quite satisfied, I thought, why don’t I make a workshop website? So I did.\nThe result was quite good, I made the (as far as I knew) first quarto workshop website at University of Oslo for the Oslo Bioinformatics Workshop Week 2022. Feedback from students were positive, and the instructor team thought it hosts the material in a more organized way.\n\n\nSingle day workshop -&gt; two week course\nI was greatly encouraged by the experience, so when I got a 50% position at University of Oslo as biostatistics lecturer, I thought, why don’t we have the same thing for the course?\n\n\n\n\n\nOh well, the workload is crushing. There were a few key differences:\n\nR scripts and material were unavailable since the course was originally in STATA. Everything need to be done from scratch, for at least 12 lab sessions;\nThe students generally have little IT skills, which means more effort need to be done to guide them through the ‘get started’ part.\n\n\n\n\n\n\nIt took one month to create the first version of the website. More details about the experience can be read here.\n\n\nAdding WebR to the course\nOne year later, as technology advances, we added new content to some parts of the website. Most notably is the interactivity achieved through WebR. For example, I made this page on randomness and statistical distribution where students can interactively modify code chunks in a web browser."
  },
  {
    "objectID": "blog/talks/blog_20240923_quartofriends/index.html#quarto-as-a-collaboration-tool",
    "href": "blog/talks/blog_20240923_quartofriends/index.html#quarto-as-a-collaboration-tool",
    "title": "Use Quarto, Make Friends: a two-year journey",
    "section": "Quarto as a collaboration tool",
    "text": "Quarto as a collaboration tool\nA static (or even interactive) website is not exactly what you call ‘collaborative tool’. However, if you work as a group towards something cool, Quarto might just be the tool you need. Check out the CAMIS project to find out what I mean by this!"
  },
  {
    "objectID": "blog/talks/blog_20240923_quartofriends/index.html#what-else",
    "href": "blog/talks/blog_20240923_quartofriends/index.html#what-else",
    "title": "Use Quarto, Make Friends: a two-year journey",
    "section": "What else?",
    "text": "What else?\nThe associated talk is available on YouTube, check it out!"
  },
  {
    "objectID": "blog/talks/blog_20230921_positconf2023/index.html",
    "href": "blog/talks/blog_20230921_positconf2023/index.html",
    "title": "Personal Highlights: Positconf 2023",
    "section": "",
    "text": "The yearly party of Positconf (formerly Rstudio conf) has come to an end. I joined the virtual experience at home, it is of course not the same as attending in-person, yet the atmosphere in discord was still great!\nIt’s hard to choose which talks to watch since multiple were scheduled at the same time, so one has to prioritize. I definitely will re-visit some of the talks at a later point, so this blog acts as a placeholder for links so that I can find them in the future."
  },
  {
    "objectID": "blog/talks/blog_20230921_positconf2023/index.html#make-interactive-things",
    "href": "blog/talks/blog_20230921_positconf2023/index.html#make-interactive-things",
    "title": "Personal Highlights: Positconf 2023",
    "section": "Make interactive things",
    "text": "Make interactive things\nWebDev is definitely a big thing at this year’s positconf. If I’m learning one thing from the conference, I’d check out webR.\nI still remember when R was mainly for statistical analysis and computing back when I learned it. Now it’s become much more fun! Strictly speaking, webRand quarto are not R per se. However, they’ve become the gateway drugs for R programmers to dabble in WebDev. With web assembly (wasm), now one can execute R code in a browser and even run shiny app.\nUnlock the power of dataViz animation and interactivity in quarto by Deepsha Menghani used a super fun example (F-bomb) to demonstrate how to add interactivity to your barplot (or other plots) with Crosstalk. Check out the talk here. The presentation was as interactive as the quarto slides, good job Deepsha!\nRunning shiny without a server by Joe Cheng (repo): this was a big announcement. I used shiny at work, but for my own projects or smaller teaching projects I tried to stay away from shiny - I was concerned about the fee. This looks like a promising thing to try out once it’s stable, although I’d probably do webR first."
  },
  {
    "objectID": "blog/talks/blog_20230921_positconf2023/index.html#make-pretty-things",
    "href": "blog/talks/blog_20230921_positconf2023/index.html#make-pretty-things",
    "title": "Personal Highlights: Positconf 2023",
    "section": "Make pretty things",
    "text": "Make pretty things\nIt is fascinating to see so many organizations and individual R developers make their own themes for better branding, recognition and storytelling. More and more peple have realized that making beautiful plots is important, and totally possible as well. Work on layout, color, font and sizes!\n\nThemes\nAdding a touch of glitr: Developing a package of themes on top of ggplot by Aaron Chafetz, Karishma Srikanth and colleagues at USAID. repo\n\n\nTables\nMaking tables with gt has been on my to-do list for a while now. It is very inspiring to see so many cool tables that makes you wonder, “is it really JUST a table?” For example, check out this gallery by Posit community.\nThe book Creating beautiful tables in R with gt by Albert Rapp would be a good place to learn how to make nice tables. Actually the reason why I wanted to use gt is that it seems to be the mainsteam in clinical reporting in pharma. I bumped into this blog post some time ago, and this would be my starting point.\n\n\nQuarto\nIf you want to go one step further and start making your quarto project pretty, there are a few things to try out.\nAlbert Rapp in his talk HTML and CSS for R Users stated that quarto is a gateway drug to WebDev. It reminds me of my very first presentation at my local R users community (2019) was about building a website with blogdown, and when I really spent a lot of time to make my markdown documentation colorful with span style - and that was about everything I knew.\nNow I want more. Learning HTML and CSS can make your dataviz, tables, slides and dashboards look not only professional but also special. I’m going to check out the scss variables in quarto which defines the theme, theme_file.scss. Emil Hvitfeldt (Styling and templating quarto documents) showed us how to make really pretty and animated (!) quarto sldies themes, and shared this template with us, quarto-revealjs-earth. I really like how revealjs slides look like, just that the MacOS Keynote (or MS ppt) drag-and-drop seems more flexible to me (?) Guess it’s something I should get used to over time.\nRichard Iannone (Extending quarto) introduced quarto shortcode extensions to add a bunch of fancy-looking icons to quarto files. To create extensions in general: https://quarto.org/docs/extensions/creating. This is for more pro-users since you needs to learn lua."
  },
  {
    "objectID": "blog/talks/blog_20230921_positconf2023/index.html#quarto-updates",
    "href": "blog/talks/blog_20230921_positconf2023/index.html#quarto-updates",
    "title": "Personal Highlights: Positconf 2023",
    "section": "Quarto updates",
    "text": "Quarto updates\nQuarto is definitely one of the most discussed topics in the year 2022-2023 in the R community. For good reasons. I need to catch up the the latest developments annd use-cases:\n\nWhat’s new in quarto? by Charlotte Wickham\nReproducible manuscripts with Quarto by Mine Çetinkaya-Rundel\nParametrized quarto reports improves understanding of soil health by Jadey Ryan\n\nand so many more. I couldn’t follow all the talks and I’m sure there are lots of great examples of how quarto is better than traditional ways of reporting."
  },
  {
    "objectID": "blog/talks/blog_20230921_positconf2023/index.html#a-few-other-things-to-check-out",
    "href": "blog/talks/blog_20230921_positconf2023/index.html#a-few-other-things-to-check-out",
    "title": "Personal Highlights: Positconf 2023",
    "section": "A few other things to check out",
    "text": "A few other things to check out\nBeyond the web and quarto topics, I think there are some existing and new tools that can be useful for my work. For example,\n\nI should review Hadley and Jenny’s R package book (2e).\nthis package targets for pipeline automation and management look like something that can be used for my analysis\n…\n\nIt will take a while to digest the latest developments. But little by little, we’ll get there! People in the R community are doing great things."
  },
  {
    "objectID": "blog/talks/technotes_20230220_pkgdown/index.html",
    "href": "blog/talks/technotes_20230220_pkgdown/index.html",
    "title": "R package website with pkgdown",
    "section": "",
    "text": "1. Create the website skeleton.\nBefore editing the details, we need to create the skeleton for the website. It can be done with usethis and pkgdown packages.\nIn R, run this:\nusethis::use_pkgdown()\nThis creates the _pkgdown.yml file, which is the place you configure your site.\nTo view the initial package website, use the following command:\npkgdown::build_site()\nThis creates docs/ directory containing a website\n\nREADME.md becomes the homepage,\ndocumentation in man/ generates a function reference,\nvignettes are rendered into articles/.\n\n\n\n2. Edit the vignette documentation\nMake sure that the vignette index is consistent with Title, otherwise it will not render.\n\n\n3. Build and preview your site\nNow check if the site looks good, and contents are correctly positioned.\npkgdown::preview_site()\npkgdown::build_site()\nYou can also do this to build the site.\npkgdown::build_site_github_pages()\n\n\n4. Deploy site with GitHub Pages\nThere seems to be two options:\n\nusethis::use_pkgdown_github_pages(), this function should take care of everything after pushing changes to GH.\nif you used pkgdown::build_site_github_pages() and pushed everything to GitHub, it might not automatically deploy your site to GH pages. I tried to go to Settings -&gt; Pages -&gt; Deploy from a branch -&gt; main -&gt; /docs, this makes Action deploy your site from the docs folder.\n\ndouble check if you have .nojekyll file\nif a website does not show, check whether you have docs in the .gitignore file; since you are deploying from that folder.\n\n\n\n\n\n\nCitationBibTeX citation:@online{zhang2023,\n  author = {Zhang, Chi},\n  title = {R Package Website with `Pkgdown`},\n  date = {2023-02-20},\n  url = {https://kundan-kumarr.github.io/blog/talks/technotes_20230220_pkgdown/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nZhang, Chi. 2023. “R Package Website with `Pkgdown`.”\nFebruary 20, 2023. https://kundan-kumarr.github.io/blog/talks/technotes_20230220_pkgdown/."
  },
  {
    "objectID": "blog/talks/technotes_20230225_shinyappsio/index.html",
    "href": "blog/talks/technotes_20230225_shinyappsio/index.html",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "",
    "text": "Useful references:"
  },
  {
    "objectID": "blog/talks/technotes_20230225_shinyappsio/index.html#considerations",
    "href": "blog/talks/technotes_20230225_shinyappsio/index.html#considerations",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "Considerations",
    "text": "Considerations\nA few ways to do it: Shiny Server (free), shinyapps.io (free and premium), and professional Rstudio Connect (paid).\nI choose to test out the second option, since it allows more possibilities compared to the free open-source Shiny Server.\nThe free option should allow me to create 5 apps, which is more than enough for personal use. It also allows 25 active hours per month; a note on that at the end."
  },
  {
    "objectID": "blog/talks/technotes_20230225_shinyappsio/index.html#configuration",
    "href": "blog/talks/technotes_20230225_shinyappsio/index.html#configuration",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "Configuration",
    "text": "Configuration\nSign up with GitHub account; or something else. It is possible to change account name afterwards.\nIn Rstudio,\n\nfirst install.packages('rsconnect')\nthen, configure the account. It can be done with rsconnect::setAccountInfo() with information provided in your own shinyapps.io page.\n\nBefore the last step, it is necessary to have an app to deploy!"
  },
  {
    "objectID": "blog/talks/technotes_20230225_shinyappsio/index.html#create-my-first-shiny-project",
    "href": "blog/talks/technotes_20230225_shinyappsio/index.html#create-my-first-shiny-project",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "Create my first shiny project",
    "text": "Create my first shiny project\nHere I use my usual workflow of creating a new R project:\n\nCreate a new repo on GitHub;\nClone the repo locally, by opening a new R project with version control.\n\nNow copy the two R scripts from the demo example:\n\nserver.R\nui.R\n\nTest locally by running shiny::runApp(). This should render the app."
  },
  {
    "objectID": "blog/talks/technotes_20230225_shinyappsio/index.html#deploy-to-shinyapps.io",
    "href": "blog/talks/technotes_20230225_shinyappsio/index.html#deploy-to-shinyapps.io",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "Deploy to shinyapps.io",
    "text": "Deploy to shinyapps.io\nrsconnect::deployApp() will deploy the app, with an automatically generated url that links to your account.\nThe demo app is deployed here.\n\nNote on active hours\nAfter deployment, the site seems to be active until you shut it down manually; or timeout. The default timeout is 15 minutes, which can be reduced to 5 minutes.\n25 hours per month suggests that I can open the site for 300 times (without manually shuting it down). It might be necessary to start using the paid options, if I have more than one site, or multiple users want to access it …"
  },
  {
    "objectID": "blog/talks/technotes_20250703_research_guide/index.html",
    "href": "blog/talks/technotes_20250703_research_guide/index.html",
    "title": "Research Scientist Interview Guide",
    "section": "",
    "text": "This guide provides a comprehensive framework to prepare for Research Scientist positions in academia, industry research labs (FAANG, OpenAI, DeepMind, etc.), and national labs. It combines insights from my own experience, interviews, and conversations with hiring managers."
  },
  {
    "objectID": "blog/talks/technotes_20250703_research_guide/index.html#overview",
    "href": "blog/talks/technotes_20250703_research_guide/index.html#overview",
    "title": "Research Scientist Interview Guide",
    "section": "",
    "text": "This guide provides a comprehensive framework to prepare for Research Scientist positions in academia, industry research labs (FAANG, OpenAI, DeepMind, etc.), and national labs. It combines insights from my own experience, interviews, and conversations with hiring managers."
  },
  {
    "objectID": "blog/talks/technotes_20250703_research_guide/index.html#key-interview-components",
    "href": "blog/talks/technotes_20250703_research_guide/index.html#key-interview-components",
    "title": "Research Scientist Interview Guide",
    "section": "Key Interview Components",
    "text": "Key Interview Components\n\n1️⃣ Research Portfolio Deep Dive\n\nBe able to explain your core research contributions in detail.\nClearly articulate: problem definition, novelty, methods, results, and real-world impact.\nPrepare multiple levels of technical depth (5-min, 15-min, 30-min versions).\nPractice connecting your work to broader research trends and applications.\n\n\n\n2️⃣ Technical Machine Learning Knowledge\n\nReinforcement Learning: algorithms, policy gradients, actor-critic, safe RL.\nDeep Learning: optimization, architecture design, generalization, transformers.\nProbabilistic Modeling: Bayesian inference, uncertainty estimation, graphical models.\nGenerative Models: GANs, VAEs, diffusion models.\nLarge Language Models: LLM scaling laws, prompting, fine-tuning, RAG architectures.\nVision: object detection, segmentation, multi-modal perception.\n\n\n\n3️⃣ System Design / Applied ML Problems\n\nBe able to discuss:\n\nEnd-to-end ML pipelines\nData challenges (imbalance, noisy labels, drift)\nModel serving and deployment challenges\nScalability, latency, interpretability\n\n\n\n\n4️⃣ Coding and Algorithmic Skills\n\nLeetcode-style DSA for research interviews (moderate level)\nData manipulation (pandas, numpy, SQL)\nModel prototyping (PyTorch, TensorFlow, JAX)\n\n\n\n5️⃣ Behavioral and Collaboration Skills\n\n“Tell me about a time…” questions.\nCollaboration across teams.\nHandling ambiguous open-ended research problems.\nCommunication with product teams or non-research stakeholders."
  },
  {
    "objectID": "blog/talks/technotes_20250703_research_guide/index.html#recommended-preparation-resources",
    "href": "blog/talks/technotes_20250703_research_guide/index.html#recommended-preparation-resources",
    "title": "Research Scientist Interview Guide",
    "section": "Recommended Preparation Resources",
    "text": "Recommended Preparation Resources\n\nPapers: Read papers from top-tier conferences (NeurIPS, ICML, ICLR, CVPR, ACL).\nCoding: Leetcode (medium), ML system design problems.\nSystem Design: Read “Designing Machine Learning Systems” by Chip Huyen.\nMock Interviews: Practice mock sessions with peers or mentors.\nPresentation: Prepare 1-2 strong 20-minute research talks."
  },
  {
    "objectID": "blog/talks/technotes_20250703_research_guide/index.html#example-interview-questions",
    "href": "blog/talks/technotes_20250703_research_guide/index.html#example-interview-questions",
    "title": "Research Scientist Interview Guide",
    "section": "Example Interview Questions",
    "text": "Example Interview Questions\n\nHow does your research contribute to state-of-the-art methods?\nWalk me through one of your recent papers.\nHow would you apply your methods to X domain?\nWhat challenges remain in your area of research?\nHow do you evaluate safety, robustness, or uncertainty in your models?\nHow would you adapt your methods if labeled data was extremely limited?"
  },
  {
    "objectID": "blog/talks/technotes_20250703_research_guide/index.html#my-personal-advice",
    "href": "blog/talks/technotes_20250703_research_guide/index.html#my-personal-advice",
    "title": "Research Scientist Interview Guide",
    "section": "My Personal Advice",
    "text": "My Personal Advice\n\nClarity beats complexity — explain ideas simply.\nBe enthusiastic about your work and its impact.\nConnect your strengths to the job’s mission.\nShow your ability to collaborate and iterate."
  },
  {
    "objectID": "blog/talks/technotes_20250703_research_guide/index.html#mentorship",
    "href": "blog/talks/technotes_20250703_research_guide/index.html#mentorship",
    "title": "Research Scientist Interview Guide",
    "section": "Mentorship",
    "text": "Mentorship\nIf you’re preparing for Research Scientist interviews and would like advice or mentorship, feel free to reach out at cs.kundann@gmail.com."
  },
  {
    "objectID": "blog/talks/blog_20230112_roche_opensource/index.html",
    "href": "blog/talks/blog_20230112_roche_opensource/index.html",
    "title": "Open source reporting with R: clinical, public health, RSE and embrace the change",
    "section": "",
    "text": "Two days ago (Jan 11 2023) I watched a presentation by data scientists at Roche about why they are making their clinical trials in 2023 open source with R. As someone who uses R for most of the time and has done similar works (not in pharma, but in public health surveillance and reporting: watch my talk, slides to find out what we do), I watched the presentation with great interest. Here are my notes, combined with some thoughts on open-source in the industry, public sector and academia.\n\nThree reasons for why I am writing this blog\n\nNote down some of the technology which points towards the future of the field\nRelate to my experience of open-source applied in public health, specifically public health reporting\nShare some thoughts in statistical education of applied students/researchers (e.g. medicine), and training Research Software Engineers\n\n\n\nMy experience with statistical software\nTo put my opinions in perspective,\n\nI do not have experience with SAS or pharma, so I do not have first-hand knowledge on the functionality, ease-of-use or the popularity of commercial softwares in the industry.\nI did my MSc and PhD in statistics/biostatistics/medical informatics and R had always been a default choice.\nI worked in public health for a few years, where Excel is possibly the most common tool, and STATA and R are scarcely used (statisticians, epidemiologists, bioinformaticians).\nIn the past few years, my university has made the switch from SPSS to STATA for intro statistics for medical students (while students at higher level, or doing advanced analyses might use R/python), and a test-run with R might be in motion.\n\n\n\n\nClinical Reporting\nIn drug development at pharmaceutical companies (and/or research institutes and hospitals), these data related tasks are very common:\n\nsummarise safety and efficacy data\nprovide accurate picture of trial outcomes\nmanage data collection across different sites\n\nCompleting these tasks in a correct, efficient and reproducible manner is crucial for patient safety. However, these tasks are also highly resource intensive: highly trained scientist, statisticians and technincians must be involved in the process. Historically, pharma use commercial software such as SAS.\n\nRegulation and exploration needs\nThere are requirements for clinical reporting: both regulartory and exploratory. From the regulatory side, there exist industry standards (CDISC) in the clinical research process, such as SDTM (Model for Tabulation of Study Data) and ADaM (Analysis Data Model). Statistical analyses, tables, listings and graphs (TLGs) also fall into this cateogory.\nFrom the exploratory side, clinical data are highly context dependent, and new formats of data such as imaging are more and more used in prediction modeling and drug development.\nIn addition, it is not hard to imagine that the technical competency of employees differ, especially in large organizations. Enabling people with less experience to analyse trial data in a reproducible manner is helpful for not only the learning and growth of employees, but also the productivity of organizations.\nThe existing commercial tools are not able to adapt to the rapid changes in the field.\n\n\nTransition into Open-Source\nIn this talk, Dr Kieran Martin at Roche introduced that they started using R as their core data science tool, aiming to move their codebase to having a core R. In the future, they plan to have something that is lanugage agnostic: meaning that python, Stan, C++, Julia and beyond can be used for different tasks.\nI only noted down a few of the things they mentioned on the infrastructure side:\n\nOCEAN - a lanugage agnostic computing platform on AWS (docker)\nGit, Gitlab for version control and collaboration\nRstudio connect server\nSnakemake for orchestrate production\n\n\n\nR and Shiny\nThere are obvious benefits of using R. It is convenient to install and use (if you used python and R, you’d probably agree), and the latest development in Shiny made it very easy to develop interactive visualizations, suitable for exploration. Package development is critical for reproducibility and distributing works - which R does it very well. A few packages developed by pharma are Teal and admiral: the ADaM in R, which I intend to check out at one point.\nR has deep roots in academia which means the newest statistical methods are well covered; which also affects the skill sets that talents own - fresh graduates probably already learned it at university. R being open source means that collaboration with external partners is much more efficient, and transparent. Strong community support is another positive thing that encourages beginners to enter the field and learn.\n\n\n\n\nOpen Sourcing Public Health\n\nSurveillance and reporting\nOne key functionality of public health (PH) authorities is stay informed and inform. They collect data from labs, hospitals and clinics across the country, summarize into useful statistics in tables and graphs, make reports, then inform the policy makers to make decisions (such as vaccination campaigns).\nCompared to clinical reporting (in my understanding), there are many similarities - we make TLG (tables, listings and graphs). There are also features that make reporting in public health unique:\n\nPH surveillance and reporting are dynamic and real-time, which can change in a matter of days. That is because the situation of different infectious diseases can evolve rapidly, so PH authorities need to make appropriate adjustments.\nTime and location (spatial-temporal) are important. Different time granularity (daily, weekly) and geographical units (nation, county, municipality, city districts) are typically required for reporting.\n\n\n\nScale up and automate with open source tools\nTraditionally, these reports are made manually - one location, one graph per time on a certain disease. When a global pandemic hits, this is definitely not fast enough. At my team (Sykdomspulsen team at the Norwegian Institute of Public Health), we tried a different approach. Details of what we did can be found in this talk(slides), but to make it brief:\n\nWe developed a fully automated pipeline that connects 15 registries (vaccination, lab, hospital and intensive care and many more). The data is gathered, censored, cleaned and pre-processed for down-stream analysis\nStatistical analysis, tables, graphs and maps are made for all locations in Norway for various outcomes of interest, such as Covid, influenza, respiratory and gastrointestinal infections\nOver 1000 customized reports with over 30 graphs and tables are produced daily and sent to local PH officials, where we also had a shiny website (Kommunehelsetjenester for Kommunelege) for over 300 PH officials to get most up-to-date information about their own municipality\n\nBy automation, every year Sykdomspulsen can save 700 000 NOK (roughly 70k USD) while making 400 times more real-time reports for public health. Even better, with reproducibility and quality control.\n\n\nToolbox\nSykdomspulsen is a small team (8 people, 3 are statisticians and 1 engineer), and our infrastructure was built upon R packages, which we call splverse. Our infrastructure is not fundamentally different from the one Roche introduced, basically:\n\nR does the task planning and project organization. On top of this, the data cleaning, statistical analysis are implemented. Graphs, tables and maps are made with appropriate R packages\nRmarkdown does automated reporting into .docx and .xlsx. Some reports are also in .html tables to be embedded into customized emails\nRstudio Workbench and GitHub help with teamwork\n\nDocker, GoCD and Airflow do the CI/CD and orchestration\n\n\n\n\nEmbrace the transition\n\nCulture change needed\nUnfortunately, not all organizations are eager to abandon the old way. Even at our own institute where researchers are the majority, open source and modern day programming is hardly practiced (by my observation). Even worse, under the budget cuts in 2023-24, a large number of younger employees who have the technical skills have left - which left the public health surveillance even more vulnerable now that Covid is far from over.\nIn my opinion, public health needs open-source and good programming even more than pharmaceutical companies. Both save lifes - and PH has less money to invest in softwares, infrastructures and talents. In this situation, resources should be spent in fields that are critical and most cost-effective; yet in reality this is often not the case.\nThe slow culture change at big organizations can happen, but only if there is a sufficient amount of employees who are willing to embrace the new technology. In the talk by Roche they about about their training strategy. It is not possible to train all users, and not everyone has the same needs at the same time. Therefore, self study with certain study paths is encouraged and supported.\n\n\nTeach programming to students in various fields\nBased on my experience in the UK and Norway, students (myself included) learn R programming in one of the two ways\n\nLearning by Googling (self-taught): a university degree needs to use it: provides a short introduction, then students learn by using. This is how I learned R at my MSc Statistics degree, and this is probably the most common way\nWorkshops at university: organizations such as the Carpentries provide course material and teaching a few times per year, where interested students (usually from subjects such as biology) come and learn. These classes are quite popular, and usually have a long waiting list.\n\nFrom learning by googling to some organized teaching - that is already some good progress. However, if not, can we improve?\nIn my experience with statistical advising with the university hospital, clinical researchers and medical students are enthusastic to get their statistics done, some are also eager to do some analysis themselves. That is good. Yet, there is generally lack of capacity - either knowledge or software skills. Once the statistician who helps with the project stops, the project ends. There is the need to have in-house statistical capacity. To this end, open-source softwares such as R, and good programming practice (reproducibility for example) can help a lot: the license doesn’t end, and everything is documented so that the next person can continue the work.\nI’m glad that my university has made some transitional efforts in this regard: STATA instead of SPSS is being taught to medical students as part of their statistics course. There might be a test-run in R soon, which is very exciting (since I’ll be involved in the teaching)!\n\n\nStatistical engineering and RSEs\nThat was the capacity building to get beginners more independent. On the other side, there is also the need for better programming practice for researchers at more advanced level. Research Software Engineering (RSE) is starting to get more and more attention, because it is not only relevant for research (i.e. getting papers published), but in broader applications.\nFor example, in the talk by Roche, they mentioned that “RSE teams need to accelerate adoption of new statistical methods and biomarker data analysis”, and the implementation with R packages and templates is at its core. In the future more languages would be included such as Python, Stan, C++ and Julia.\nHowever, RSE as a job title or career path is still a new thing. I know two RSEs at my university, and RSE is definitely not your typical academic faculty position: only departments that think it’s important makes positions, often not permanent. To get any new methods actually used in either industry or the public sector outside research, translating methods into tools is must-do. In the future I hope RSE becomes a stable and common career path, and more exciting things can happen.\n\n\n\n\n\nCitationBibTeX citation:@online{zhang2023,\n  author = {Zhang, Chi},\n  title = {Open Source Reporting with {R:} Clinical, Public Health,\n    {RSE} and Embrace the Change},\n  date = {2023-01-13},\n  url = {https://kundan-kumarr.github.io/blog/talks/blog_20230112_roche_opensource/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nZhang, Chi. 2023. “Open Source Reporting with R: Clinical, Public\nHealth, RSE and Embrace the Change.” January 13, 2023. https://kundan-kumarr.github.io/blog/talks/blog_20230112_roche_opensource/."
  },
  {
    "objectID": "blog/talks/technotes_20230519_pkgcran/index.html",
    "href": "blog/talks/technotes_20230519_pkgcran/index.html",
    "title": "R package workflow",
    "section": "",
    "text": "This checklist is being updated over time. Mostly for my own use; but great if it helps you as well!\nFor a complete treatment, please refer to R Packages (2e) by Hadley Wickham and Jennifer Bryan."
  },
  {
    "objectID": "blog/talks/technotes_20230519_pkgcran/index.html#initialize-the-project",
    "href": "blog/talks/technotes_20230519_pkgcran/index.html#initialize-the-project",
    "title": "R package workflow",
    "section": "Initialize the project",
    "text": "Initialize the project\n\nusethis::create_package('path_to_pkg/pkgname') \n\nIt opens a new R project (directory) named pkgname, with the following items:\n\nDESCRIPTION\nNAMESPACE\ndirectory R/\n.Rbuildignore and .gitignore\nand the project icon, pkgname.Rproj.\n\nIf you have an existing R project but wish to build a package there, copy everything but pkgname.Rproj, and modify the files in your existing pkg directory. Pay extra attention to the hidden files like .Rbuildignore.\n\nusethis::use_mit_license() # modify name to yours\nusethis::use_readme_md() # if you do not have this already\nusethis::use_news_md()\nusethis::use_test()\n\n# create a folder for future data documentation\nx &lt;- 1 \nusethis::use_data() \n\nIn addition, URL and bug reports should be added in the DESCRIPTION."
  },
  {
    "objectID": "blog/talks/technotes_20230519_pkgcran/index.html#planning",
    "href": "blog/talks/technotes_20230519_pkgcran/index.html#planning",
    "title": "R package workflow",
    "section": "Planning",
    "text": "Planning\nIt is good practice to start with planning the package, rather than directly start coding.\nCreate a folder called dev. To prevent it from being built, add the following line in .Rbuildignore"
  },
  {
    "objectID": "blog/talks/technotes_20230519_pkgcran/index.html#write-test-and-document",
    "href": "blog/talks/technotes_20230519_pkgcran/index.html#write-test-and-document",
    "title": "R package workflow",
    "section": "Write, test and document",
    "text": "Write, test and document\nCreate exported functions in R/, development code in script/ (or somewhere else, such as dev/).\n\nData: raw and processed\nNeed to be clear in mind where the data files go. There are a few data related folders:\n\nraw data files, in the format of excel sheets or csv. Usually placed as inst/data_name.csv\nR scripts to process the raw data so that we create data object inside the package, put inside data-raw\ndata objects that can be called as pkg::data_name, are placed in data. These files are usually directly generated by executing write.rda().\ndata documentation, usually placed in R/data_documentation.R. These are Roxygen2 documents for the data.\n\n\n\nDocumentation\nYou need to configure the Build tools.\nThese three things should be done:\n\nFunction documentation\nCreate a function f1, and put your cursor on it. Go to Code -&gt; Insert Roxygen Skeleton to create the template.\nAlternatively, use #' to start.\n\n#' A simple placehold function \n#'\n#' @param x a numeric value\n#'\n#' @return a value 3 greater than the input\n#' @export\n#'\n#' @examples \n#' f1(5)\nf1 &lt;- function(x){\n  x+3\n}\n\n\n\nData documentation\nIt can be beneficial to create a separate file to document data only, say data_documentation.R under the R/ directory.\n\n#' Placeholder data x\n#'\n#' This dataset contains one value, x\n#'\n#' @format\n#' \\describe{\n#' \\item{x}{The placeholder data x}\n#' }\n#' @examples\n#' print(x)\n\"x\"\n\n\n\nVignette documentation\n\nusethis::use_vignette('your_vignette')\n\n\n\nDeploy to pkgdown\nCheck this reference here"
  },
  {
    "objectID": "blog/talks/technotes_20230519_pkgcran/index.html#build-package-and-check",
    "href": "blog/talks/technotes_20230519_pkgcran/index.html#build-package-and-check",
    "title": "R package workflow",
    "section": "Build package and check",
    "text": "Build package and check\nIt is possible that your checks don’t pass on the first try.\n\nWhat to ignore when build?\n^.*\\.Rproj$\n^\\.Rproj\\.user$\n^dev$\n^_pkgdown\\.yml$\n^license\\.md$\nMakefile\ndata-raw\ncran-comments.md\n^\\.github$"
  },
  {
    "objectID": "blog/talks/blog_20230103_blogdown2quarto/index.html",
    "href": "blog/talks/blog_20230103_blogdown2quarto/index.html",
    "title": "Website reboot: switching from Blogdown to Quarto",
    "section": "",
    "text": "Since the first time I tried the “academic” template in the popular blogdown package in 2019, three years have passed. Back then, it was THE way to build a personal website using R. The “academic” template was notoriously rich in content, and my solution was to delete components, compile, if it works - great; if not, I put the deleted content back. It worked for a while.\nWhen the distill package came out (probably in 2020?), I rebooted my website since I preferred its clean, minimalistic style. The look was possibly more appropriate for websites for an organisation or tutorials rather than personal blog, yet I appreciated the simplicity.\nThen I stopped updating my website. Between mid 2020 and early 2022, I was too stressed about completing my PhD, and balancing my other two jobs wasn’t the easiest thing. During this period, my mind had been going back to the old site from time to time, but it was hard to find enough time or energy to write about stuff."
  },
  {
    "objectID": "blog/talks/blog_20230103_blogdown2quarto/index.html#from-blogdown-to-distill",
    "href": "blog/talks/blog_20230103_blogdown2quarto/index.html#from-blogdown-to-distill",
    "title": "Website reboot: switching from Blogdown to Quarto",
    "section": "",
    "text": "Since the first time I tried the “academic” template in the popular blogdown package in 2019, three years have passed. Back then, it was THE way to build a personal website using R. The “academic” template was notoriously rich in content, and my solution was to delete components, compile, if it works - great; if not, I put the deleted content back. It worked for a while.\nWhen the distill package came out (probably in 2020?), I rebooted my website since I preferred its clean, minimalistic style. The look was possibly more appropriate for websites for an organisation or tutorials rather than personal blog, yet I appreciated the simplicity.\nThen I stopped updating my website. Between mid 2020 and early 2022, I was too stressed about completing my PhD, and balancing my other two jobs wasn’t the easiest thing. During this period, my mind had been going back to the old site from time to time, but it was hard to find enough time or energy to write about stuff."
  },
  {
    "objectID": "blog/talks/blog_20230103_blogdown2quarto/index.html#time-to-try-quarto",
    "href": "blog/talks/blog_20230103_blogdown2quarto/index.html#time-to-try-quarto",
    "title": "Website reboot: switching from Blogdown to Quarto",
    "section": "Time to try Quarto",
    "text": "Time to try Quarto\nNow that I’ve finally completed the more pressing tasks in October 2022, I can catch up to the cool kids on twitter: create a website with Quarto!\nThere were quite a lot of discussions about Quarto in the summer 2022. I wasn’t following the discussions closely, but I remember there were quite a few talks in the Rstudio conference this year. Then more and more people switched to Quarto on Twitter. Then people I know also switched to Quarto. What’s the fuzz about?\nMy experience with Quarto is focused on websites. I have not tried other forms of publishing. So far I have created:\n\na workshop website for my colleagues\na personal website (the one you are reading right now)\nan R package (qtwAcademic)that wraps three Quarto website templates for beginners\n\nHere are a few things I like about Quarto. Given that I’m not very experienced in front-end development, these comments are going to be about ease-of-use and design, rather than the technicalities.\n\nClean look for both personal and workshop/courses\nWhen I was using “academic” template in blogdown, I liked the structure of the site: projects, talks, blog, softwares and publications sections are clearly displayed at the top. What I didn’t like is that the default homepage was a very long single page; yet its customisation wasn’t the easist. Other templates were either too simple (for blog only), or more suitable for image display (photography projects). I wanted a website that keep the good structure of “academic”, which is quite suitable for academics (hence the name); while keeping each section independent.\nWith distill I could achieve the structure I wanted; but I didn’t enjoy it too much as a personal website (at least it wasn’t as flexible as Quarto). distill is still pretty decent for organisations or documentation site.\nWith Quarto, I can achieve the desired looks for not only a personal website (with or without blogs), but also a workshop, event or even course website. This is fantastic! The top, sidebar or hybrid navigation makes the site structure very clear, especially when there are lots of content. As an aspiring lecturer at university, this is really One Quarto Rules Them All.\n\n\nFlexible yet not overwhelming\nAs I mentioned above, hacking “academic” in blogdown was not that easy - simply because there were too many folders that you are not actually supposed to modify. It was confusing to know what to change in order to achieve the desired output, and multiple folders were having the same names, making it very challenging for beginners. Ironically, this is usually the first template beginners start with!\nThat’s why I immediately fell for Quarto: you only need 4 components to make a decent minimalistic website work:\n\n_quarto.yml to control the overall layout\nindex.qmd at the root folder to control the homepage\nabout.qmd for some basic information about the creator or the website\nproject.qmd for projects or any other content that the creator wants to display\n\nThe way that _quarto.yml clearly specifies the .qmd files really helps beginners to understand where things are. This has been extremely useful for me when I wanted to learn how people made their website by reading the source code - I could understand exactly where to find the information I needed. The clear structure greatly helps the creators themselves, and also those who want to learn.\n\n\nGreat community\nRstats people have a great community. I wouldn’t be able to make my site the way I wanted if people haven’t been sharing their works. I have learned a lot by reading the source code by Dr Emi Tanaka, Dr David Schoch, Bea Milz, Prof Mine Cetinnkaya-Rundel’s STA 210 - Regression Analysis course.\nI also made my own R package that wraps three templates to create Quarto websites that are frequently used by academics, qtwAcademic. In the following days I plan to write up more detailed explanations on how to use the package, along with some new features."
  },
  {
    "objectID": "blog/talks/readnotes_2023010x_preventable_sridhar/index.html",
    "href": "blog/talks/readnotes_2023010x_preventable_sridhar/index.html",
    "title": "Preventable: How a Pandemic Changed the World & How to Stop the Next One - Devi Sridhar",
    "section": "",
    "text": "Advice on some measures to prepare for the next pandemic\n(From Five ways to prepare for the next pandemic by Prof. Devi Sridhar)\n\nMonitor zoonoses. Identify patogens with pandemic potential, regulate better wet markets\nSequence globally. Investment in genetic-sequencing capability\nStrengthen manufacturing. Vaccine inequality, fragility of vaccine production. Private and public sector work together - vaccine research, production and distribution.\nVaccine preparedness. For known diseases (e.g. influenza), invest in vaccines that protect against a wide range of variants. New technology and research for unknown threats\nStop the spread (long enough for the vaccines) to save lives.\n\n\n\n\n\nCitationBibTeX citation:@online{zhang2023,\n  author = {Zhang, Chi},\n  title = {Preventable: {How} a {Pandemic} {Changed} the {World} \\&\n    {How} to {Stop} the {Next} {One} - {Devi} {Sridhar}},\n  date = {2023-03-17},\n  url = {https://kundan-kumarr.github.io/blog/talks/readnotes_2023010x_preventable_sridhar/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nZhang, Chi. 2023. “Preventable: How a Pandemic Changed the World\n& How to Stop the Next One - Devi Sridhar.” March 17, 2023.\nhttps://kundan-kumarr.github.io/blog/talks/readnotes_2023010x_preventable_sridhar/."
  },
  {
    "objectID": "blog/talks/readnotes_2023010x_pandemic_gates/index.html",
    "href": "blog/talks/readnotes_2023010x_pandemic_gates/index.html",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "",
    "text": "Some mistakes\n\nFederal agencies refused to share data\nPersons responsible did not have training inn epidemiology\nNot enough testing, not fast enough\nLacks information sharing\n\n\n\n\nThere should be a global expert team to help preventing the pandemic: they should be responsible for\n\nsurveillance of potential disease outbreaks, sound the alert when necessary;\ncreation and sharing of data system and data on cases\nstandardising policy making and training\nevaluation of the capacity of individual countries\ncoordination of personnels\n\nHowever it is difficult even for all countries to reach an agreement, and secure the funds. There is no one organisation that is able to join forces from all parties. Organisations depend on volunteers. WHO lacks funds, experts who are specialised in pandemic research, and relies on the free global response networks.\nA team, GERM (Global Epidemic Response and Mobilisation) should be established. The main task should be disease surveillance and modeling; rather than treating patients.\n\n\n\n\n\nPassive surveillance: healthcare workers report cases that use healthcare services (e.g. clinic, hospital) to public health authorities.\nActive surveillance: workers go to communities to find potential patients that have not been to clinics or hospitals due to inconvenience, or mild symptoms.\nWhen there are many cases (clusters), the signal might be picked up by a computer algorithm, and alerts sent to healthcare workers so that they pay more attention.\nIn some countries, personnels other than healthcare workers (e.g. teachers, post office staff) might also participate in disease surveillance. In addition, some virus can be detected in the environment such as waste water (e.g. polio, illeagal drug).\n\n\n\nIn LMIC (low and middle income countries), there are higher percentage of unrecorded births ad deaths. Some of them carry out census every other year - no real time data. Some data are also lack information such as cause of death. Without knowing causes of death (such as diarrhea), it is impossible to prevent the disease.\nPost-mortem (autopsy) can be unattainable, especially in LMIC. It could also be undesirable for families who have lost their loved ones - procedures are very invasive. Nevertheless, alternatives exist, such as minimally invasive autopsy technologies like MRI and MRI guided fine-needle biopsy.\n\n\n\n\nThe effect of NPI (non-pharmaceutical intervention, such as masks and lockdown) is difficult to quantify; yet it is a very important measure.\nParadox: NPI is effective -&gt; reduced cases -&gt; people think NPI is not necessary\nLockdown can slow down spread, yet it has a huge impact on economy, especially for LMIC.\nContact tracing\n\nnot a new technology; used for smallpox, Ebola, AIDS.\nnot widely applicable: some countries do it better than others. Need more trust from people towards public health agencies.\nsmartphone apps, not very useful: limited by users\n\nGood ventilation system\n\nviruses survive in air, but for different length of time\n\nSocial distance\n\n6 inch isn’t a magic distance\ndepends on circumstances: indoor/outdoor\n\n\n\n\nInfodemic\n(these two chapters are highly technical, and they deserve a separate note)\n\n\n\nDisaster simulation and drill\nDrill: assume a city is at risk of a disease that has epidemic potential\n\nhow to develop diagnostic tests and large-scale manufacturing and distribution\ngovernment, timely and comprehensive information dissemination\nmanagement of quarantine\nset up system for case reporting\n\n\n\n\nThe impact is different among different groups.\nVaccination distribution is highly imbalanced; yet it is only one of the many aspects where inequality exists, and not even the most unequal.\n\n\n\n\n\nInvest in better vaccine, treatment, diagnosis\nTesting and approval process\nFinding new treatment and vaccine\n\ncreate a large database for anti-virus chemicals, open to all\nAI and software to speed-up development\n\n\n\n\nPublic health agencies are under-funded, this is true for all levels: state/county, country and international organizations (WHO).\n\n\n\nImprove census, birth and death in LMIC; then expand into sequencing pathogens, environmental monitoring.\nAggregate disease surveillance systems internationally, and provide real-time data\n\n\n\nRebuild the system after Covid, invest more in healthcare, more staff\nSpend more on basic prevention for all and early diagnosis, rather than in-hospital treatment for severe cases\nManagement, clear tasks and responsibility"
  },
  {
    "objectID": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#learn-from-covid",
    "href": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#learn-from-covid",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "",
    "text": "Some mistakes\n\nFederal agencies refused to share data\nPersons responsible did not have training inn epidemiology\nNot enough testing, not fast enough\nLacks information sharing"
  },
  {
    "objectID": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#create-a-pandemic-prevention-team",
    "href": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#create-a-pandemic-prevention-team",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "",
    "text": "There should be a global expert team to help preventing the pandemic: they should be responsible for\n\nsurveillance of potential disease outbreaks, sound the alert when necessary;\ncreation and sharing of data system and data on cases\nstandardising policy making and training\nevaluation of the capacity of individual countries\ncoordination of personnels\n\nHowever it is difficult even for all countries to reach an agreement, and secure the funds. There is no one organisation that is able to join forces from all parties. Organisations depend on volunteers. WHO lacks funds, experts who are specialised in pandemic research, and relies on the free global response networks.\nA team, GERM (Global Epidemic Response and Mobilisation) should be established. The main task should be disease surveillance and modeling; rather than treating patients."
  },
  {
    "objectID": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#get-better-at-detecting-outbreaks-early",
    "href": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#get-better-at-detecting-outbreaks-early",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "",
    "text": "Passive surveillance: healthcare workers report cases that use healthcare services (e.g. clinic, hospital) to public health authorities.\nActive surveillance: workers go to communities to find potential patients that have not been to clinics or hospitals due to inconvenience, or mild symptoms.\nWhen there are many cases (clusters), the signal might be picked up by a computer algorithm, and alerts sent to healthcare workers so that they pay more attention.\nIn some countries, personnels other than healthcare workers (e.g. teachers, post office staff) might also participate in disease surveillance. In addition, some virus can be detected in the environment such as waste water (e.g. polio, illeagal drug).\n\n\n\nIn LMIC (low and middle income countries), there are higher percentage of unrecorded births ad deaths. Some of them carry out census every other year - no real time data. Some data are also lack information such as cause of death. Without knowing causes of death (such as diarrhea), it is impossible to prevent the disease.\nPost-mortem (autopsy) can be unattainable, especially in LMIC. It could also be undesirable for families who have lost their loved ones - procedures are very invasive. Nevertheless, alternatives exist, such as minimally invasive autopsy technologies like MRI and MRI guided fine-needle biopsy."
  },
  {
    "objectID": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#help-people-protect-themselves-right-away",
    "href": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#help-people-protect-themselves-right-away",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "",
    "text": "The effect of NPI (non-pharmaceutical intervention, such as masks and lockdown) is difficult to quantify; yet it is a very important measure.\nParadox: NPI is effective -&gt; reduced cases -&gt; people think NPI is not necessary\nLockdown can slow down spread, yet it has a huge impact on economy, especially for LMIC.\nContact tracing\n\nnot a new technology; used for smallpox, Ebola, AIDS.\nnot widely applicable: some countries do it better than others. Need more trust from people towards public health agencies.\nsmartphone apps, not very useful: limited by users\n\nGood ventilation system\n\nviruses survive in air, but for different length of time\n\nSocial distance\n\n6 inch isn’t a magic distance\ndepends on circumstances: indoor/outdoor"
  },
  {
    "objectID": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#find-new-treatment-fast-get-readyy-to-make-vaccines",
    "href": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#find-new-treatment-fast-get-readyy-to-make-vaccines",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "",
    "text": "Infodemic\n(these two chapters are highly technical, and they deserve a separate note)"
  },
  {
    "objectID": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#practice-practice-practice",
    "href": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#practice-practice-practice",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "",
    "text": "Disaster simulation and drill\nDrill: assume a city is at risk of a disease that has epidemic potential\n\nhow to develop diagnostic tests and large-scale manufacturing and distribution\ngovernment, timely and comprehensive information dissemination\nmanagement of quarantine\nset up system for case reporting"
  },
  {
    "objectID": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#close-the-health-gap-between-rich-and-poor-countries",
    "href": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#close-the-health-gap-between-rich-and-poor-countries",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "",
    "text": "The impact is different among different groups.\nVaccination distribution is highly imbalanced; yet it is only one of the many aspects where inequality exists, and not even the most unequal."
  },
  {
    "objectID": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#make-and-fund-a-plan-for-preventing-pandemics",
    "href": "blog/talks/readnotes_2023010x_pandemic_gates/index.html#make-and-fund-a-plan-for-preventing-pandemics",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "",
    "text": "Invest in better vaccine, treatment, diagnosis\nTesting and approval process\nFinding new treatment and vaccine\n\ncreate a large database for anti-virus chemicals, open to all\nAI and software to speed-up development\n\n\n\n\nPublic health agencies are under-funded, this is true for all levels: state/county, country and international organizations (WHO).\n\n\n\nImprove census, birth and death in LMIC; then expand into sequencing pathogens, environmental monitoring.\nAggregate disease surveillance systems internationally, and provide real-time data\n\n\n\nRebuild the system after Covid, invest more in healthcare, more staff\nSpend more on basic prevention for all and early diagnosis, rather than in-hospital treatment for severe cases\nManagement, clear tasks and responsibility"
  },
  {
    "objectID": "blog/talks/technotes_20230111_deployqt/index.html",
    "href": "blog/talks/technotes_20230111_deployqt/index.html",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "",
    "text": "After you have the public repo, clone it to your local repo."
  },
  {
    "objectID": "blog/talks/technotes_20230111_deployqt/index.html#create-a-public-repository-on-github",
    "href": "blog/talks/technotes_20230111_deployqt/index.html#create-a-public-repository-on-github",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "",
    "text": "After you have the public repo, clone it to your local repo."
  },
  {
    "objectID": "blog/talks/technotes_20230111_deployqt/index.html#create-quarto-project",
    "href": "blog/talks/technotes_20230111_deployqt/index.html#create-quarto-project",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "2. Create Quarto project",
    "text": "2. Create Quarto project\nThis can be a website, a book (a specific type of website) or something else.\nTest compilation by quarto render, or click the Render button."
  },
  {
    "objectID": "blog/talks/technotes_20230111_deployqt/index.html#configure-quarto-project",
    "href": "blog/talks/technotes_20230111_deployqt/index.html#configure-quarto-project",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "3. Configure Quarto project",
    "text": "3. Configure Quarto project\nIn _quarto.yml, change the project configuration to use docs as the output-dir:\nproject:\n  type: website\n  output-dir: docs\n\n\n\n\n\nThen add .nojekyll to the root of the repository. Can do this by (in terminal)\ntouch .nojekyll\nPush everything to your repository."
  },
  {
    "objectID": "blog/talks/technotes_20230111_deployqt/index.html#configure-github-pages",
    "href": "blog/talks/technotes_20230111_deployqt/index.html#configure-github-pages",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "4. Configure GitHub Pages",
    "text": "4. Configure GitHub Pages\nGo to Settings &gt; Pages, publish from docs of the main branch.\n\n\n\n\n\nCan check GitHub Action and deployment status.\n\n\n\n\n\n\n\n\n\n\nAfter the deployment is successful, go to view deployment, and a successful website should be published."
  },
  {
    "objectID": "blog/talks/readnotes_20240606_bad_pharma/index.html",
    "href": "blog/talks/readnotes_20240606_bad_pharma/index.html",
    "title": "Bad Pharma: How medicine is broken, and how we can fix it - Ben Goldacre",
    "section": "",
    "text": "Industry funded trials were twenty times more likely to give results that are favoring the test drug\n\nOn the need for meta-analysis\n\nPeople would write long review articles surveying the literature - in which they would cite the trial data they come across in a completely unsystematic fashion, often reflecting their own prejudice and values.\n\nOn trials\n\n… mild torture economy: you’re not being paid to do a job, you’re being paid to endure.\n\nOn regulators\n\nfree movement of staff between regulators and drug companies… a fifth of those surveyed said they had been pressured to approve a drug despite reservation about efficacy and safety\n\n\napplication from large companies, which have greater experience with the regulatory process, pass through to approval faster than those from smaller companies\n\nOn surrogate outcomes\n\nthey are approved for showing a benefit on surrogate outcomes, such as blood test, that is only weakly or theoretically associated with the real suffering and death we’re trying to avoid. Sometimes drugs which work well to change surrogate outcomes simply don’t make any difference to the real outcome.\n\nOn comparative effectiveness research\n\nit is a vitally important filed, in many cases the value of finding out what works best among the drugs we already have would hugely exceed the value of developing entirely new ones.\n\nOn safety and efficacy for approved drugs\n\n39 percent patients believe that FDA only approves ‘extremely effective’ drugs, and 25 percent that it only approves drugs without serious side effect. However regulators frequently approve drugs that are only vaguely effective, with serious side effects, on the off-chance that they might be useful to someone, somewhere, when other interventions aren’t an option.\n\nOn drug reviewing\n\nRegulators that have approved a drug are often reluctant to take it off the market, in case it is seen as an admissio of their failure to spot problems in the first place\n\nOn trial patients\n\nthe ‘ideal’ patients are likely to get better, they exaggerate the benefits of drugs, and help expensive new medicines appear to be more cost effective than they really are. ‘External validity’: trial patient being unrepresentative\n\nOn comparison drugs\n\nit is common to see trials where a new drug is compared to a competitor that is known to be useless; or with a good competitor at a stupidly low (or high) dose\n\nOn random variation in the data\n\nearly stopping because you peeked in the results. should set up stopping rules, specified before the trial begins\nneed a large trial to detect a small difference between two treatments, and a very large trial to be confident that two drugs are equally effective\n(multiple testing, sub-group analysis): measuring lots of things, some will be statistically significant, simply from the natural random variation in all trial data.\n\nOn presenting the results\n\npercent reduction in the risk of heart attack (risk difference)\nrelative risk reduction\npresenting the results as relative risk reduction overstates the benefits\n\nWays trials go wrong\n\nunrepresentative patients\ntoo brief\nmeasure the wrong outcomes\ngo missing, if the result is unflattering\nanalysed wrongly\n\nOn ‘simple trial’ using EHR\n\nat present trials are very expensive. Many struggle to recruit enough patients, many struggle to recruit everyday doctors who don’t want to get involved in the mess of filing out patient report forms, calling patients back for extra appointments, doing extra measurements and so on\nsimple trials have disadvantage of being not blinded - patients know what drug they’ve received\npragmatic trials are cheap\nthese trials run forever and follow-up data are easy to get\n\nOn marketing\n\nSome have estimated that the pharmaceutical industry overall spends twice as much on marketing and promotion as it does on research and developments\n\n\nDrugs are advertised more when the number of potential patients, rather than the current patients, is large."
  },
  {
    "objectID": "blog/talks/readnotes_20240606_bad_pharma/index.html#notes-from-the-book",
    "href": "blog/talks/readnotes_20240606_bad_pharma/index.html#notes-from-the-book",
    "title": "Bad Pharma: How medicine is broken, and how we can fix it - Ben Goldacre",
    "section": "",
    "text": "Industry funded trials were twenty times more likely to give results that are favoring the test drug\n\nOn the need for meta-analysis\n\nPeople would write long review articles surveying the literature - in which they would cite the trial data they come across in a completely unsystematic fashion, often reflecting their own prejudice and values.\n\nOn trials\n\n… mild torture economy: you’re not being paid to do a job, you’re being paid to endure.\n\nOn regulators\n\nfree movement of staff between regulators and drug companies… a fifth of those surveyed said they had been pressured to approve a drug despite reservation about efficacy and safety\n\n\napplication from large companies, which have greater experience with the regulatory process, pass through to approval faster than those from smaller companies\n\nOn surrogate outcomes\n\nthey are approved for showing a benefit on surrogate outcomes, such as blood test, that is only weakly or theoretically associated with the real suffering and death we’re trying to avoid. Sometimes drugs which work well to change surrogate outcomes simply don’t make any difference to the real outcome.\n\nOn comparative effectiveness research\n\nit is a vitally important filed, in many cases the value of finding out what works best among the drugs we already have would hugely exceed the value of developing entirely new ones.\n\nOn safety and efficacy for approved drugs\n\n39 percent patients believe that FDA only approves ‘extremely effective’ drugs, and 25 percent that it only approves drugs without serious side effect. However regulators frequently approve drugs that are only vaguely effective, with serious side effects, on the off-chance that they might be useful to someone, somewhere, when other interventions aren’t an option.\n\nOn drug reviewing\n\nRegulators that have approved a drug are often reluctant to take it off the market, in case it is seen as an admissio of their failure to spot problems in the first place\n\nOn trial patients\n\nthe ‘ideal’ patients are likely to get better, they exaggerate the benefits of drugs, and help expensive new medicines appear to be more cost effective than they really are. ‘External validity’: trial patient being unrepresentative\n\nOn comparison drugs\n\nit is common to see trials where a new drug is compared to a competitor that is known to be useless; or with a good competitor at a stupidly low (or high) dose\n\nOn random variation in the data\n\nearly stopping because you peeked in the results. should set up stopping rules, specified before the trial begins\nneed a large trial to detect a small difference between two treatments, and a very large trial to be confident that two drugs are equally effective\n(multiple testing, sub-group analysis): measuring lots of things, some will be statistically significant, simply from the natural random variation in all trial data.\n\nOn presenting the results\n\npercent reduction in the risk of heart attack (risk difference)\nrelative risk reduction\npresenting the results as relative risk reduction overstates the benefits\n\nWays trials go wrong\n\nunrepresentative patients\ntoo brief\nmeasure the wrong outcomes\ngo missing, if the result is unflattering\nanalysed wrongly\n\nOn ‘simple trial’ using EHR\n\nat present trials are very expensive. Many struggle to recruit enough patients, many struggle to recruit everyday doctors who don’t want to get involved in the mess of filing out patient report forms, calling patients back for extra appointments, doing extra measurements and so on\nsimple trials have disadvantage of being not blinded - patients know what drug they’ve received\npragmatic trials are cheap\nthese trials run forever and follow-up data are easy to get\n\nOn marketing\n\nSome have estimated that the pharmaceutical industry overall spends twice as much on marketing and promotion as it does on research and developments\n\n\nDrugs are advertised more when the number of potential patients, rather than the current patients, is large."
  },
  {
    "objectID": "blog/talks/technotes_20230228_clinreport_part3/index.html",
    "href": "blog/talks/technotes_20230228_clinreport_part3/index.html",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 3",
    "section": "",
    "text": "This is a course provided by Genentech (part of Roche) on Coursera.\nCourse link"
  },
  {
    "objectID": "blog/talks/technotes_20230228_clinreport_part3/index.html#principles-and-tools",
    "href": "blog/talks/technotes_20230228_clinreport_part3/index.html#principles-and-tools",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 3",
    "section": "Principles and tools",
    "text": "Principles and tools\nReproducibility: Git (code versioning), dependencies (renv for r package dependencies, Docker for system dependencies)\n\nClean code\nCode comments: not recommended! Better to write code in a way that does not need additional comments.\nDRY: don’t repeat yourself (principle of software development), avoid copy and paste everywhere.\nSRP: single-responsibility prinicple, a function should do one thing: either plot a chart, saves a file, changes variables etc, but not all.\nNaming conventions\n\nReserve dots (.) for S3 methods (print.patient)\nReserve CamelCase for R6 classes or package names (OurPatients)\nUse snake cases (all_patients) for function names and arguments, use verb noun pattern (plot_this())\n\n\n\nCode smells\nA function might be too large: break into smaller ones (e.g. could fit in one screen)\nA function violates SRP: break into smaller ones, and be explicit in what result it is expected to return\nA function with multiple arguments: the scenarios to be tested increase rapidly. Recommended to minimize number of critical function arguments, and break the function into smaller ones.\nBad comments in the code: drop the unnecessary, unclear, outdated comments, write code that are self-explanatory.\n\n\nDevelopment workflow\nCode refactoring: change existing code without its functionality\nTDD: Test-Driven Development\n\nstart with writing a new (failing) test\nwrite code thtat passes the nenw tetst\nrefactor the code\nand repeat\n\nBenefits: your code is covered by tests; you think of testing scenarios first; “fail fast” - can immediately repair the code; more freedom to refactor (improve) the code.\nHow to test\n\nautomatically: CI/CD, after pushing Git commits\nmanually:\n\nrun all unit tests in the package (Build / Test package)\nrun tests in a selected test file (Run Tests)\nrun a single test in Rstudio console\n\n\nHow to check\n\nR CMD CHECK"
  },
  {
    "objectID": "blog/talks/technotes_20230228_clinreport_part3/index.html#writing-robust-statistical-software",
    "href": "blog/talks/technotes_20230228_clinreport_part3/index.html#writing-robust-statistical-software",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 3",
    "section": "Writing robust statistical software",
    "text": "Writing robust statistical software\nImplement complext statistical methods such that the software is reliable, and includes appropriate testing to ensure high quality and validity and ultimately credibility of statistical analysis results.\n\nchoose the right method and understand them\nsolve the core implementation problem with prototype code\n\nNeed to try a few different solutions, compare and select the best one. Might also need to involve domain experts.\n\nspend enough time on planning the design of the R package\n\nDon’t write the package right away; instead define the scope, discuss with users, and design the package.\nStart to draw a flow diagram, align names, arguments and classes; write prototype code.\n\nassume the package will evolve over time\n\nPackages you depend on will change; users will require new features\nWrite tests\n\nunit tests\nintegration tests\n\nMake the package extensible\n\nconsider object oriented package designs\ncombine functions in pipelines\n\nKeep it manageable\n\navoid too many arguments\navoid too large functions"
  },
  {
    "objectID": "blog/talks/technotes_20230228_clinreport_part3/index.html#key-components",
    "href": "blog/talks/technotes_20230228_clinreport_part3/index.html#key-components",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 3",
    "section": "Key components",
    "text": "Key components\n\nDependency management\nInstall dependencies (system/OS level; R packages)\n\nSet repos (can be specified in options()) to e.g. CRAN, BioConductor\nrenv\ncontainer with dependencies pre-installed\n\n\n\nStatic code analysis\n\nLinting (for programmatic and syntax errors) via lintr package\nCode style enforcement via styler package\nSpell checks identifies misspelled words in vignettes, docs and R code via spelling package\n\n\n\nTesting\n\nR CMD build builds R packages as a installable artifact\nR CMD check runs 20+ checks including unit tests, reports errors, warnigns and notes\nTest coverage reports with covr, checks how many lines of code are covered with tests\nR CMD INSTALL tests R package installation\n\n\n\nDocumentation\nAuto-generated docs via Roxygen and pkgdown\n\n\nRelease and deployments\nRelease artifacts and deployments to target systems\n\nChangelog (features, bug fixes) in the NEWS.md\nRelease: create the package with R CMD build. Validation report with thevalidatoR\nPublishing: CRAN, BioConductor"
  },
  {
    "objectID": "blog/talks/technotes_20240506_ohdsi_part1/index.html",
    "href": "blog/talks/technotes_20240506_ohdsi_part1/index.html",
    "title": "Notes: The Book of OHDSI - Data Analytics",
    "section": "",
    "text": "The Book of OHDSI written by the OHDSI community.\nWhat is required to go from origin (source data) to destination (evidence):\nOMOP: Observational Medical Outcomes Partnership, aims to identify true drug safety association.\nOMOP CDM: common data model, a mechanism to standardize the structure, content and semantics to make it possible to write statistical code that can be reused at every data site.\nOHDSI community (2014) has created libraries of open-source analytics tools atop OMOP CDM to support:"
  },
  {
    "objectID": "blog/talks/technotes_20240506_ohdsi_part1/index.html#characterization",
    "href": "blog/talks/technotes_20240506_ohdsi_part1/index.html#characterization",
    "title": "Notes: The Book of OHDSI - Data Analytics",
    "section": "Characterization",
    "text": "Characterization\n\nWhat happened to the patients.\n\nChapter 11 Characterization\nTypical characterization questions:\n\nHow many patients…?\nHow often does…? What proportion of patients …?\nWhat is the distribution of values for …?\nWhat is the median length of exposure for patients on …?\nOther drugs the patient is using?\n\nDesired output:\n\ncount, percentage\naverages and other descriptive statistics\nprevalence, incidence rate\nrule-based phenotype\ndrug utilization, adherence, treatment pathways, line of therapy\ndisease natural history, co-morbidity profile"
  },
  {
    "objectID": "blog/talks/technotes_20240506_ohdsi_part1/index.html#population-level-estimation",
    "href": "blog/talks/technotes_20240506_ohdsi_part1/index.html#population-level-estimation",
    "title": "Notes: The Book of OHDSI - Data Analytics",
    "section": "Population-level estimation",
    "text": "Population-level estimation\n\nWhat are the causal effects\n\nChapter 12 Population-level Estimation\nTypical questions:\n\nWhat is the effect of …?\nWhich treatment works better?\nWhat is the risk of X on Y?\nWhat is the time-to-event of …?\n\nDesired output:\n\nRR, HR, OR\nAssociation, correlation\nATE, causal effect"
  },
  {
    "objectID": "blog/talks/technotes_20240506_ohdsi_part1/index.html#patient-level-prediction",
    "href": "blog/talks/technotes_20240506_ohdsi_part1/index.html#patient-level-prediction",
    "title": "Notes: The Book of OHDSI - Data Analytics",
    "section": "Patient-level prediction",
    "text": "Patient-level prediction\n\nWhat will happen to A?\n\nChapter 13 Patient-level Prediction\nTypical questions:\n\nWhat is the chance that this patient will…?\nWho are the candidate for…?\n\nDesired output:\n\nprobability for an individual\nprediction model\nhigh/low risk groups\nprobabilistic phenotype"
  },
  {
    "objectID": "blog/talks/technotes_20231001_qt_webr/index.html",
    "href": "blog/talks/technotes_20231001_qt_webr/index.html",
    "title": "Use WebR in your existing quarto website",
    "section": "",
    "text": "WebR is the new hot topic in the R community. Coupled with Quarto, you can run R code interactively in a web browser. This is achieved with the great quarto extension, quarto-webr developed by James J Balamuta.\nIn the positconf 2023 talk, documentation and YouTube, James introduced how to make a webR empowered quarto document. It is simple enough, and you can make it work quite smoothly."
  },
  {
    "objectID": "blog/talks/technotes_20231001_qt_webr/index.html#when-your-render-gets-stuck",
    "href": "blog/talks/technotes_20231001_qt_webr/index.html#when-your-render-gets-stuck",
    "title": "Use WebR in your existing quarto website",
    "section": "When your render gets stuck",
    "text": "When your render gets stuck\nBut there is a twist. This works perfectly fine with a new quarto project, where no output-dir is specified yet. When I tried to replicate the same thing for my existing quarto website (with output-dir: docs so that I could deploy it with GitHub Pages), my rendered html file got stuck:\n\nIf you read the troubleshooting documentation, you’ll see that it’s a problem with the two js files. This agrees with what Rstudio Background Jobs tells us.\n\nI moved the two files (manually..) around, then render again, nothing changed.\n\nSolution: set channel-type option\nThis is a solution provided by the authors, although I don’t quite understand what it did, but it did the magic. (Thanks to Linh’s help!)\nThis is where you specify this option.\n\nRender again, now it works! WebR status turns green, and I can run code interactively in the browser."
  },
  {
    "objectID": "blog/talks/blog_20230104_qtwAcademic/index.html",
    "href": "blog/talks/blog_20230104_qtwAcademic/index.html",
    "title": "qtwAcademic: a quick and easy way to start your Quarto website",
    "section": "",
    "text": "qtwAcademic stands for Quarto Websites for Academics, which provides a few Quarto templates for Quarto website that are commonly used by academics.\nThe templates are designed to make it quick and easy for users with little or no Quarto experience to create a website for their personal portfolio or courses. Each template is fully customizable once the user is more familiar with Quarto.\nRead more about the package here.\nMore details about the package is being written … \n\n\n\nCitationBibTeX citation:@online{zhang2023,\n  author = {Zhang, Chi},\n  title = {qtwAcademic: A Quick and Easy Way to Start Your {Quarto}\n    Website},\n  date = {2023-01-05},\n  url = {https://kundan-kumarr.github.io/blog/talks/blog_20230104_qtwAcademic/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nZhang, Chi. 2023. “qtwAcademic: A Quick and Easy Way to Start Your\nQuarto Website.” January 5, 2023. https://kundan-kumarr.github.io/blog/talks/blog_20230104_qtwAcademic/."
  },
  {
    "objectID": "blog/talks/technotes_20231018_qt_styling/index.html",
    "href": "blog/talks/technotes_20231018_qt_styling/index.html",
    "title": "Styling your quarto project",
    "section": "",
    "text": "Useful references:\n\nTalk by Emil Hvitfeldt on Styling and Templating Quarto Documents\n\n\n\n\nCitationBibTeX citation:@online{zhang2023,\n  author = {Zhang, Chi},\n  title = {Styling Your Quarto Project},\n  date = {2023-10-18},\n  url = {https://kundan-kumarr.github.io/blog/talks/technotes_20231018_qt_styling/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nZhang, Chi. 2023. “Styling Your Quarto Project.” October\n18, 2023. https://kundan-kumarr.github.io/blog/talks/technotes_20231018_qt_styling/."
  },
  {
    "objectID": "blog/talks/technotes_20230222_clinreport_part2/index.html",
    "href": "blog/talks/technotes_20230222_clinreport_part2/index.html",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 2",
    "section": "",
    "text": "This is a course provided by Genentech (part of Roche) on Coursera.\nCourse link"
  },
  {
    "objectID": "blog/talks/technotes_20230222_clinreport_part2/index.html#agile-mindset-and-devops-practices",
    "href": "blog/talks/technotes_20230222_clinreport_part2/index.html#agile-mindset-and-devops-practices",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 2",
    "section": "Agile mindset and DevOps practices",
    "text": "Agile mindset and DevOps practices\n\nData science as a new way of thinking\nNew way of working means\n\nleverage standards and automation (CI/CD)\nadopt new data types quickly, reusing data for multiple purposes, pooling data, data marts\nopen-sourcing and collaborating cross pharma (small, readable, self-tested code)\ncoding for reusability, moving away from single-use programs\nrapidly re-arranginng re-usable components to meet analytical need at hand\n\nData scientist need to have hard skills, such as\n\nSAS, R, Python, JS, bash\ncloud, containers\nCI/CD tools\nvisualisation\nknowledge of various data types\n\nand also soft skills:\n\ncollaborative and inclusive\ntransparent and practical\ncreative and proactive\nasking the right questions\nable to wear many hats, be more flexible and resilient\n\n\n\nAgile\nProject management; a mindset: uncover better ways of working, by doing and helping others do it.\n1st principle: highest priority is to satisfy the customer through early and continuous delivery of valuable software.\nImplementations: Kanban, Scrum, Lean, Extreme programming\nTools:\n\nbacklog\nkanban board (not started, in progress, done)\nWIP (work in progress limit)\nprogress measures: e.g. team velocity\n\n\n\nDevOps\nIncrease efficiency by improving the connection between Dev (software development) and Ops (IT operations).\nThe goal is continuous delivery and continuous improvement.\nPractices:\n\nmodular architecture\nversion control\nmerge into trunk daily\nautomated and continuous testing, continuous integration\nautomated deployment\n\n\nDevOps in clinical reporting\nRisks around production run:\n\nare all dependencies in production?\nwas all quality control completed and successful?\nis all documentation complete?\nwas the transfer to eDMS correct and successful?"
  },
  {
    "objectID": "blog/talks/technotes_20230222_clinreport_part2/index.html#version-control",
    "href": "blog/talks/technotes_20230222_clinreport_part2/index.html#version-control",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 2",
    "section": "Version control",
    "text": "Version control\nFeature branch (as opposed to master branch): one task per branch\nname feature branch: issue number and description\nEach issue should have a clear description, short and specific; instead of being long and overarching.\n\nWorkflow for clinical reporting\nRestraints of clinical deliveries: timing annd multiple deliveries; resourcing challenges\nMight need to choose between feature and GitFlow."
  },
  {
    "objectID": "blog/talks/technotes_20230222_clinreport_part2/index.html#reproducible-projects-in-r",
    "href": "blog/talks/technotes_20230222_clinreport_part2/index.html#reproducible-projects-in-r",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 2",
    "section": "Reproducible projects in R",
    "text": "Reproducible projects in R\nTo reproduce your work:\n\nGit (version control)\nR libraries\nWell structured projects\nUnderlying dependencies (e.g. operating systems, C++/C)\n\n\nWell structured projects\nClear names\nGood documentation\n\n\nR libraries and versions\nCheck session info; but not the most practical way.\nUse global libraries, .libPaths(), this gives you the path where all the packages are installed. Global libraries is useful when using a server for multiple R sessions, where they look for the packages in the same place.\nSolutions\n\nrenv package: makes each project in R self-contained.\nCheckpoint: project level library paths based on snapshots of CRAN\n\nUse Docker images! Saves R version, operating system, underlying dependencies"
  },
  {
    "objectID": "blog/talks/blog_20230717_teaching/index.html",
    "href": "blog/talks/blog_20230717_teaching/index.html",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "",
    "text": "Earlier this year (2023) I wrote a blog about my thoughts on the role of open source software in statisical education. Naturally, I advocate for more use of open source tools such as R/python in teaching introductory statistics to applied scientists. Nonetheless, how the material is taught will make a huge difference in the understanding and interest in the material.\nI was taught statistics in the classic way: lectures with tons of mathematical formulae and proofs, while programming and data analyses were left for students themselves to figure out. Those who were the fastest learners were the ones who already had a degree in computer science, which probably doesn’t sound surprising. I, for one, definitely struggled."
  },
  {
    "objectID": "blog/talks/blog_20230717_teaching/index.html#does-statistics-have-to-be-daunting",
    "href": "blog/talks/blog_20230717_teaching/index.html#does-statistics-have-to-be-daunting",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "Does statistics have to be daunting?",
    "text": "Does statistics have to be daunting?\nFor applied scientists in various fields, data analysis is a core task, and also a challenging one. You must have met clinicians or biologists who would love their data to be analysed yet don’t know how to. Yes, statistics and data skills can take some time to learn; but with the right method, they don’t have to be daunting. It is up to the educator to find a way that benefits the most students. An observation is that many researchers do not know or remember advanced math; yet do they need advanced math to grasp many fundamental statistical concepts?\nI believe that it is far more important and useful to teach basic IT skills and exploratory data analysis so that students can develop an understanding of their own data; rather than using a test blindly."
  },
  {
    "objectID": "blog/talks/blog_20230717_teaching/index.html#rebooting-mf9130e-classroom",
    "href": "blog/talks/blog_20230717_teaching/index.html#rebooting-mf9130e-classroom",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "Rebooting MF9130E classroom",
    "text": "Rebooting MF9130E classroom\nWhen I heard that the teaching team at Biostatistics Department, Faculty of Medicine was thinking about trying a novel pedagogical method on the MF9130E (2023 spring) class, I was more than excited to contribute. This is a PhD level course of 8 days long, offered three times a year (twice in Norwegian language). Students come from a variey of backgrounds in health and life sciences. Since this is an introductory course, the topics are broad rather than specialised.\nA few years ago, statistical software for the course made the transition from SPSS to Stata. To be more precise, students were introduced to, but not really explained to, or elaborated on how to use Stata proficiently. Why? The course is about statistics so only statistics is taught. Data skills such as manipulation are not part of statistics. \nWell, we will change that by starting to use R.\n\nThree open source musketeers\nR, quarto and GitHub the three musketeers in facilitating the transformation. We build a quarto course website where all the material are public, hosted with GitHub Pages. Having a course website is beneficial for students to have an overview of the course, in contrast to many scattered lecture notes and exercises to be downloaded.\nThe biggest advantage of using quarto is the rendered output from code. From a student’s perspective, it is reassuring to see the same result and plots using the data and code provided by the instructor. For the instructor, it is also convenient to see whether the code functions as expected. When we do not want to show the output, it is also very easy to suppress. We have created one copy with and one withtout rendered output as exercises, and are glad to see some students challenging themselves by attempting to solve the problems without solution.\nUsing Github and quarto together to build a course website is rather straightforward. I think the site structure is simple yet flexible enough to navigate. Collaboration across a small teaching team is also manageable. Github Pages was easy to set up, and changes made on the main branch is deployed within the minute. This proved to be useful in quite a few moments (where we had to replace some datasets or add some notice).\n\n\nThe Carpentries pedagogical model\nThe Carpentries is an organisation that teaches foundational coding and data science skills to researchers. I myself benefited from their workshop on version control and git taught at University of Oslo, and I think the traditional classroom could use some of the methods at these data science workshops.\nTo put simply, there are two things I tried with the course setup for MF9130E:\n\nLive coding demonstration, plenty of it\nSticky-notes flag and helper (teaching assistant) in class\n\nIn the live coding demonstration (which I was responsible for), I made sure that students were taught the most commonly used R commands for data manipulation and exploration. Quarto webpages on introduction to R, basic EDA, intermediate EDA have been created and guided through in class, mixed with statistical concepts and visualizations. Without knowing how your data looks like, blindly using statistical tests is dangerous - that is the motivation for doing so.\nWhether students feel supported can make a huge difference in their willingness to learn. Taking it slow at the beginning, and solve the problems on an individual basis can prevent early drop-outs, especially when programming and IT systems are involved. Naturally, when we don’t have helpers we can not help everyone; this is a limitation for this model. Students should be encouraged to help each other.\n\n\nLet them explore\nThe last important change in the class was to give time to students themselves. We reduced the lecturing on theory and computation, and added time for practice and discussion. The guided practice with live demo also came with solution and comments, so students could explore at their own pace. We left plenty of time for them to ask questions, and made sure most people can follow the exercises."
  },
  {
    "objectID": "blog/talks/blog_20230717_teaching/index.html#how-did-it-go",
    "href": "blog/talks/blog_20230717_teaching/index.html#how-did-it-go",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "How did it go?",
    "text": "How did it go?\nAfter the 8 day course we carried out a small survey among the ~50 students in the spring 2023 class. Student backgrounds are diverse, they work on lab data, clinical data or observational/epidemiological data:\n\nobservational study on humans 36%\nRCT 18%\nin vitro research 15%\nothers are in animal research, meta analysis or something else\n\nStatistical competency (method, software) among students are generally on the basic end. Over 75% of the cohort report themselves to have basic to very basic knowledge of statistics; 33% do not use any statistical software, around 45% have used SPSS or Stata. On the other hand, some students (7%) report to have advanced knowledge and have some R experience.\n\nSome feedback\nThis is the first time we do the course with R, live demo and put an emphasis on basic data manipulation and exploration - which means we do not have enough data, it is just an initial impression.\nHere’s what we have received. On the positive side, 86% find the course useful for their own PhD research. 75% felt they are able to use the correct methods for their analyses, which is quite encouraging. Most felt the examples and exercises were able to demonstrate the theory. Students have generally positive experience with the live demo, and find the instructors supportive. This is good!\nIn the meantime, it is only natural that some are dissatisfied (21%) in some ways. Common complaints are: R is not user friendly to absolute beginners; the leap from no software to a programming language is too big for some.\nAs for whether students have really mastered the knowledge intended, we do not have enough data to draw a conclusion. We do observe that the take home project show somewhat better understanding, but can not say for sure just yet.\nThis is a class with very diverse backgrounds, hence it is challenging to cater to everyone’s needs. Yet, we are satisfied with the trial-transformation with our introductory statistics class, and we plan to gradually implement more classes with R, and possibly hands-on practice (depending on capacity)."
  }
]